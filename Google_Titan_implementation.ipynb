{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3mXS8GikFxZsMElQrnYS1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9ab5f624ec93472fb8fdb2f560381ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_dc3482aecf654c428e01320f8ece9e33"
          }
        },
        "4dbf431042de43538e5c67a79eec4a81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c626291d255434488cdd3372523f04f",
            "placeholder": "​",
            "style": "IPY_MODEL_097d6e343082453db24108b04d857387",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "a9626f8fcf594adf98a5eed8b0695c78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_99e5d2fe0e2248af88d5973b12d695c4",
            "placeholder": "​",
            "style": "IPY_MODEL_cdb5052851ca46aeb25c6e9ed46cebca",
            "value": ""
          }
        },
        "164c2a0d13e24f3b990ebacce8276549": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_acc16a83034c4f6285269a59afc7a79d",
            "style": "IPY_MODEL_702f750ebd404e67a975c989ac0f9df9",
            "value": true
          }
        },
        "fb72f917b86e424ea3748b3702ec7726": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_e638fcca1f0441e0bbc3c0e87a59655a",
            "style": "IPY_MODEL_4e1a90f9224a49078b97c7fb10b989ab",
            "tooltip": ""
          }
        },
        "99882c7702ae4c9b8b02dd5ab7435191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b16c121a933f456e8270208eabd54e84",
            "placeholder": "​",
            "style": "IPY_MODEL_b53e3f89844f4fb1b86ba309dda3853d",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "dc3482aecf654c428e01320f8ece9e33": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "0c626291d255434488cdd3372523f04f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "097d6e343082453db24108b04d857387": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "99e5d2fe0e2248af88d5973b12d695c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdb5052851ca46aeb25c6e9ed46cebca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acc16a83034c4f6285269a59afc7a79d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "702f750ebd404e67a975c989ac0f9df9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e638fcca1f0441e0bbc3c0e87a59655a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e1a90f9224a49078b97c7fb10b989ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "b16c121a933f456e8270208eabd54e84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b53e3f89844f4fb1b86ba309dda3853d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "78b74bd69d3a4e8e9ddc9268a4d43592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a346707f210249b8be3ab7effa7fe494",
            "placeholder": "​",
            "style": "IPY_MODEL_abe9b4ea839f4777be95e2c80620fefd",
            "value": "Connecting..."
          }
        },
        "a346707f210249b8be3ab7effa7fe494": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "abe9b4ea839f4777be95e2c80620fefd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8cab55edab94623b62d9d83ecdd4693": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d69af1f063d4d26920b71bf9b53429b",
              "IPY_MODEL_eecfd1392a904ba18f25a510dfc51592",
              "IPY_MODEL_ca92c85376df4ecc9252376f5945a14b"
            ],
            "layout": "IPY_MODEL_1d0f2cf5cf8f4e47b268cc0e181192ad"
          }
        },
        "8d69af1f063d4d26920b71bf9b53429b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_05047602ca9a4257b845638c5a3e5819",
            "placeholder": "​",
            "style": "IPY_MODEL_64cae146d5fe4126b893846d561cc9e7",
            "value": "model.pt: 100%"
          }
        },
        "eecfd1392a904ba18f25a510dfc51592": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdeb793e0ce84ddb89a6907dbbb39025",
            "max": 1783630714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_de91c71c74054ac886310772bce970a9",
            "value": 1783630714
          }
        },
        "ca92c85376df4ecc9252376f5945a14b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ec69ae97e6447eb8a7e9315b6fb8b60",
            "placeholder": "​",
            "style": "IPY_MODEL_9f0ee65dce804722b6ccb2605111bc80",
            "value": " 1.78G/1.78G [00:38&lt;00:00, 54.0MB/s]"
          }
        },
        "1d0f2cf5cf8f4e47b268cc0e181192ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "05047602ca9a4257b845638c5a3e5819": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "64cae146d5fe4126b893846d561cc9e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdeb793e0ce84ddb89a6907dbbb39025": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de91c71c74054ac886310772bce970a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1ec69ae97e6447eb8a7e9315b6fb8b60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9f0ee65dce804722b6ccb2605111bc80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "74a1e7d70dc64cf1b5c867640a90826a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e92dfd6ceb34218a5f0cbc2d65815ea",
              "IPY_MODEL_6ce19335871a44c99ff00a47a68362d2",
              "IPY_MODEL_c0485b3c0d9e42f0a3d3b16097dc6062"
            ],
            "layout": "IPY_MODEL_2eda86bda86d4d3b99b470345123749d"
          }
        },
        "8e92dfd6ceb34218a5f0cbc2d65815ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8df4330ed2f40ee83771173e828d34c",
            "placeholder": "​",
            "style": "IPY_MODEL_ec414407f0af4c489b9292bc5b2c6fa4",
            "value": "best.pt: 100%"
          }
        },
        "6ce19335871a44c99ff00a47a68362d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5f6b806b8394c348e91bb9ecb6302b8",
            "max": 1783604126,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79ba9b68a56f416e8bb8a0608bac5ece",
            "value": 1783604126
          }
        },
        "c0485b3c0d9e42f0a3d3b16097dc6062": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ae46ff9b20e43758119583c5a2c2274",
            "placeholder": "​",
            "style": "IPY_MODEL_89f828bb2873494e881baeaad65ca2c7",
            "value": " 1.78G/1.78G [00:34&lt;00:00, 53.4MB/s]"
          }
        },
        "2eda86bda86d4d3b99b470345123749d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8df4330ed2f40ee83771173e828d34c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ec414407f0af4c489b9292bc5b2c6fa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e5f6b806b8394c348e91bb9ecb6302b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "79ba9b68a56f416e8bb8a0608bac5ece": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8ae46ff9b20e43758119583c5a2c2274": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "89f828bb2873494e881baeaad65ca2c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7ae463d97f98446b8243c2b28a03cd4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53b1c635be354fb380acdb005d769163",
              "IPY_MODEL_7809b095612c484fb5005cb7f65a090f",
              "IPY_MODEL_843c59996777449b8c4566edcd15a06c"
            ],
            "layout": "IPY_MODEL_9330e5c196734a2382d533bb27f5edd9"
          }
        },
        "53b1c635be354fb380acdb005d769163": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01d5adbc69c449068d5d753c04092cc8",
            "placeholder": "​",
            "style": "IPY_MODEL_866c4db0e08f48e2981cc9be3ec44caf",
            "value": "latest.pt: 100%"
          }
        },
        "7809b095612c484fb5005cb7f65a090f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ed919882e6249d683582f88f6d5269c",
            "max": 1783677014,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_84921a03a9ba41c18d8496b4485a540a",
            "value": 1783677014
          }
        },
        "843c59996777449b8c4566edcd15a06c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b95e3025b99f4376a5aa4f12a442552e",
            "placeholder": "​",
            "style": "IPY_MODEL_72403acc9b2544e3b6194bfb5c341828",
            "value": " 1.78G/1.78G [00:36&lt;00:00, 55.5MB/s]"
          }
        },
        "9330e5c196734a2382d533bb27f5edd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01d5adbc69c449068d5d753c04092cc8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "866c4db0e08f48e2981cc9be3ec44caf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2ed919882e6249d683582f88f6d5269c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "84921a03a9ba41c18d8496b4485a540a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b95e3025b99f4376a5aa4f12a442552e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72403acc9b2544e3b6194bfb5c341828": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajveer43/titan_transformer/blob/master/Google_Titan_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Titan Model 1"
      ],
      "metadata": {
        "id": "HXcaaUQsPreE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMoxp2Q9GJu5",
        "outputId": "25ea2a1d-d956-4c23-b0fc-c029c7287e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 256])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DepthwiseSeparableConv1D(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size):\n",
        "        super().__init__()\n",
        "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size, groups=in_channels, padding=kernel_size // 2)\n",
        "        self.pointwise = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class GatingMechanism(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate = torch.sigmoid(self.linear(x))\n",
        "        return gate * x\n",
        "\n",
        "class TitansMemoryModule(nn.Module):\n",
        "    def __init__(self, input_dim, memory_dim):\n",
        "        super().__init__()\n",
        "        self.long_term_memory = nn.Linear(input_dim, memory_dim)\n",
        "        self.persistent_memory = nn.Parameter(torch.zeros(memory_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        long_term_output = self.long_term_memory(x)\n",
        "        combined_memory = long_term_output + self.persistent_memory.unsqueeze(0)\n",
        "        return combined_memory\n",
        "\n",
        "class TitansAttention(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(input_dim, input_dim)\n",
        "        self.key = nn.Linear(input_dim, input_dim)\n",
        "        self.value = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "        self.conv = DepthwiseSeparableConv1D(input_dim, kernel_size=3)\n",
        "        self.gating = GatingMechanism(input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = F.normalize(self.query(x), p=2, dim=-1)\n",
        "        k = F.normalize(self.key(x), p=2, dim=-1)\n",
        "        v = self.value(x)\n",
        "\n",
        "        attention_weights = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(x.size(-1), dtype=torch.float32))\n",
        "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, v)\n",
        "\n",
        "        attention_output = attention_output.transpose(1, 2)  # Prepare for Conv1D\n",
        "        conv_output = self.conv(attention_output)\n",
        "        conv_output = conv_output.transpose(1, 2)  # Back to original shape\n",
        "\n",
        "        gated_output = self.gating(conv_output)\n",
        "        return gated_output\n",
        "\n",
        "class TitansModel(nn.Module):\n",
        "    def __init__(self, input_dim, memory_dim):\n",
        "        super().__init__()\n",
        "        self.memory_module = TitansMemoryModule(input_dim, memory_dim)\n",
        "        self.attention = TitansAttention(memory_dim)\n",
        "        self.residual = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        memory_output = self.memory_module(x)\n",
        "        attention_output = self.attention(memory_output)\n",
        "        return self.residual(attention_output + memory_output)\n",
        "\n",
        "# Example usage\n",
        "input_dim = 128\n",
        "memory_dim = 256\n",
        "sequence_length = 32\n",
        "batch_size = 8\n",
        "\n",
        "model = TitansModel(input_dim, memory_dim)\n",
        "inputs = torch.randn(batch_size, sequence_length, input_dim)\n",
        "outputs = model(inputs)\n",
        "print(outputs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Titan implementation"
      ],
      "metadata": {
        "id": "bW8uphnsthvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Titan Model Implementation"
      ],
      "metadata": {
        "id": "N3dpyqGzPx8C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNfDK8wOPxa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class DepthwiseSeparableConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
        "        super().__init__()\n",
        "        self.depthwise = nn.Conv1d(\n",
        "            in_channels, in_channels, kernel_size,\n",
        "            padding=padding, groups=in_channels\n",
        "        )\n",
        "        self.pointwise = nn.Conv1d(in_channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class GatingMechanism(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.gate_proj = nn.Linear(dim, dim)\n",
        "        self.transform_proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate = torch.sigmoid(self.gate_proj(x))\n",
        "        transformed = self.transform_proj(x)\n",
        "        return gate * transformed\n",
        "\n",
        "class TitanAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, head_dim=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        # Projections for Q, K, V\n",
        "        self.q_proj = nn.Linear(dim, num_heads * head_dim)\n",
        "        self.k_proj = nn.Linear(dim, num_heads * head_dim)\n",
        "        self.v_proj = nn.Linear(dim, num_heads * head_dim)\n",
        "\n",
        "        # Depthwise separable convolutions\n",
        "        self.q_conv = DepthwiseSeparableConv1d(\n",
        "            num_heads * head_dim,\n",
        "            num_heads * head_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.k_conv = DepthwiseSeparableConv1d(\n",
        "            num_heads * head_dim,\n",
        "            num_heads * head_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.v_conv = DepthwiseSeparableConv1d(\n",
        "            num_heads * head_dim,\n",
        "            num_heads * head_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "        self.out_proj = nn.Linear(num_heads * head_dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, dim = x.shape\n",
        "\n",
        "        # Project and apply SiLU activation\n",
        "        q = F.silu(self.q_proj(x))\n",
        "        k = F.silu(self.k_proj(x))\n",
        "        v = F.silu(self.v_proj(x))\n",
        "\n",
        "        # Reshape for depthwise conv\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Apply depthwise separable convolutions\n",
        "        q = self.q_conv(q).transpose(1, 2)\n",
        "        k = self.k_conv(k).transpose(1, 2)\n",
        "        v = self.v_conv(v).transpose(1, 2)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # L2 normalize queries and keys\n",
        "        q = F.normalize(q, p=2, dim=-1)\n",
        "        k = F.normalize(k, p=2, dim=-1)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            # Properly reshape mask for broadcasting\n",
        "            # mask shape: [batch_size, seq_len] -> [batch_size, 1, 1, seq_len]\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        # Reshape and project output\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class TitanBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, head_dim=64, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = TitanAttention(dim, num_heads, head_dim, dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        mlp_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.SiLU(),\n",
        "            GatingMechanism(mlp_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Residual connection for attention\n",
        "        x = x + self.attn(self.norm1(x), mask)\n",
        "        # Residual connection for MLP\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class TitanTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        num_heads=8,\n",
        "        head_dim=64,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TitanBlock(\n",
        "                dim=dim,\n",
        "                num_heads=num_heads,\n",
        "                head_dim=head_dim,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Example usage\n",
        "def create_titan_model(\n",
        "    vocab_size=50000,\n",
        "    max_seq_length=1024,\n",
        "    dim=512,\n",
        "    depth=12,\n",
        "    num_heads=8,\n",
        "    head_dim=64,\n",
        "    mlp_ratio=4,\n",
        "    dropout=0.1\n",
        "):\n",
        "    class TitanModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding(vocab_size, dim)\n",
        "            self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_length, dim))\n",
        "            self.transformer = TitanTransformer(\n",
        "                dim=dim,\n",
        "                depth=depth,\n",
        "                num_heads=num_heads,\n",
        "                head_dim=head_dim,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout\n",
        "            )\n",
        "\n",
        "        def forward(self, x, mask=None):\n",
        "            # Add positional embeddings\n",
        "            x = self.embedding(x)\n",
        "            x = x + self.pos_embedding[:, :x.size(1), :]\n",
        "\n",
        "            # Apply transformer\n",
        "            x = self.transformer(x, mask)\n",
        "            return x\n",
        "\n",
        "    return TitanModel()"
      ],
      "metadata": {
        "id": "SAIzaOtzGKo2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model\n",
        "model = create_titan_model(\n",
        "    vocab_size=50000,\n",
        "    max_seq_length=1024,\n",
        "    dim=512,\n",
        "    depth=12\n",
        ")\n",
        "\n",
        "# Example input (batch_size=2, seq_length=128)\n",
        "x = torch.randint(0, 50000, (2, 128))\n",
        "mask = torch.ones(2, 128)\n",
        "\n",
        "# Forward pass\n",
        "output = model(x, mask)"
      ],
      "metadata": {
        "id": "3CVC-r4LHheJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc1YJrHfHiVb",
        "outputId": "cf9b7fe1-bc2e-4812-9b25-469cf3f3803e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-2.5337e-01, -6.0905e-01,  2.3120e+00,  ..., -2.9865e-01,\n",
            "          -1.7289e-01,  1.9123e+00],\n",
            "         [-7.7001e-01,  1.0869e+00,  3.5012e+00,  ...,  1.3517e+00,\n",
            "          -6.5708e-02, -4.5865e-02],\n",
            "         [-1.3881e+00,  8.3592e-02,  1.2183e-01,  ..., -1.8826e+00,\n",
            "          -1.4912e-01, -9.6813e-01],\n",
            "         ...,\n",
            "         [-1.6125e+00,  2.4097e-01,  1.0992e+00,  ..., -2.0965e-01,\n",
            "           1.4801e-01,  1.5081e-02],\n",
            "         [ 4.3039e-01,  7.8977e-01,  1.8156e+00,  ..., -6.9191e-02,\n",
            "          -2.0223e+00, -4.6822e-01],\n",
            "         [-1.2896e+00, -7.1673e-01,  6.4670e-01,  ..., -4.1999e-01,\n",
            "          -9.2779e-01, -2.3601e-01]],\n",
            "\n",
            "        [[-8.0844e-02,  6.1648e-01,  2.4976e+00,  ...,  7.4161e-01,\n",
            "          -1.3234e+00, -2.1923e-01],\n",
            "         [-9.1315e-01,  2.2983e-01,  9.5114e-01,  ...,  1.0183e+00,\n",
            "          -1.2915e+00, -5.7810e-04],\n",
            "         [-5.8095e-01, -4.8502e-02,  1.2166e+00,  ..., -7.5621e-01,\n",
            "          -2.1875e-01,  2.9964e-01],\n",
            "         ...,\n",
            "         [-1.7111e+00,  1.6506e+00,  5.7960e-01,  ...,  7.1421e-01,\n",
            "           4.6483e-01,  8.7765e-01],\n",
            "         [-1.4993e+00,  7.3645e-01,  2.3547e-01,  ...,  2.0394e-01,\n",
            "          -9.0674e-01, -9.8030e-01],\n",
            "         [ 4.5026e-01,  1.6866e-01, -1.4808e+00,  ..., -3.5278e-01,\n",
            "          -1.5645e+00, -1.1945e+00]]], grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using the huggingface tokenizer"
      ],
      "metadata": {
        "id": "SxRiNuPKQwR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Optional, Tuple\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "class TitanTextGenerator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: 'TitanModel',\n",
        "        tokenizer: Optional[PreTrainedTokenizerFast] = None,\n",
        "        max_length: int = 1024,\n",
        "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    ):\n",
        "        self.model = model.to(device)\n",
        "        self.model.eval()  # Set to evaluation mode\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Initialize tokenizer if not provided\n",
        "        if tokenizer is None:\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        else:\n",
        "            self.tokenizer = tokenizer\n",
        "\n",
        "    def preprocess_text(self, text: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Tokenize input text\n",
        "        encodings = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encodings['input_ids'].to(self.device)\n",
        "        attention_mask = encodings['attention_mask'].to(self.device)\n",
        "\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_next_token(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 50,\n",
        "        top_p: float = 0.9\n",
        "    ) -> torch.Tensor:\n",
        "        # Get model output\n",
        "        outputs = self.model(input_ids, attention_mask)\n",
        "\n",
        "        # Get logits for the next token (last position)\n",
        "        next_token_logits = outputs[:, -1, :]\n",
        "\n",
        "        # Apply temperature\n",
        "        next_token_logits = next_token_logits / temperature\n",
        "\n",
        "        # Apply top-k filtering\n",
        "        if top_k > 0:\n",
        "            values, indices = torch.topk(next_token_logits, top_k)\n",
        "            min_values = values[:, -1].unsqueeze(-1).expand_as(next_token_logits)\n",
        "            next_token_logits = torch.where(\n",
        "                next_token_logits < min_values,\n",
        "                torch.ones_like(next_token_logits) * float('-inf'),\n",
        "                next_token_logits\n",
        "            )\n",
        "\n",
        "        # # Apply top-p (nucleus) filtering\n",
        "        # if top_p < 1.0:\n",
        "        #     # Sort logits in descending order\n",
        "        #     sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n",
        "        #     cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        #     # Create a mask for tokens to remove\n",
        "        #     sorted_indices_to_remove = cumulative_probs > top_p\n",
        "\n",
        "        #     # Shift the indices to the right to keep also the first token above the threshold\n",
        "        #     sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        #     sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        #     # Scatter sorted indices to original positions\n",
        "        #     indices_to_remove = sorted_indices_to_remove.scatter(\n",
        "        #         dim=1,\n",
        "        #         index=sorted_indices,\n",
        "        #         src=sorted_indices_to_remove\n",
        "        #     )\n",
        "        #     next_token_logits = next_token_logits.masked_fill(indices_to_remove, float('-inf'))\n",
        "\n",
        "        # Apply top-p (nucleus) filtering\n",
        "        if top_p < 1.0:\n",
        "            # Sort logits in descending order\n",
        "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n",
        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "            # Mask tokens with cumulative probability above top_p\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0  # Keep at least one token\n",
        "\n",
        "            # Scatter mask back to logits\n",
        "            mask = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "            next_token_logits = next_token_logits.masked_fill(mask, float('-inf'))\n",
        "\n",
        "\n",
        "        # Sample from the filtered distribution\n",
        "        probs = F.softmax(next_token_logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        return next_token\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        text: str,\n",
        "        max_new_tokens: int = 50,\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 50,\n",
        "        top_p: float = 0.9,\n",
        "        early_stopping: bool = True\n",
        "    ) -> str:\n",
        "        # Preprocess input text\n",
        "        input_ids, attention_mask = self.preprocess_text(text)\n",
        "\n",
        "        generated_tokens = []\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Generate next token\n",
        "            next_token = self.generate_next_token(\n",
        "                input_ids,\n",
        "                attention_mask,\n",
        "                temperature,\n",
        "                top_k,\n",
        "                top_p\n",
        "            )\n",
        "\n",
        "            # Append the new token\n",
        "            generated_tokens.append(next_token.item())\n",
        "\n",
        "            # Update input_ids and attention_mask\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "            attention_mask = torch.cat([\n",
        "                attention_mask,\n",
        "                torch.ones((1, 1), device=self.device)\n",
        "            ], dim=1)\n",
        "\n",
        "            # Check for EOS token\n",
        "            if early_stopping and next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            # Check if we've exceeded max_length\n",
        "            if input_ids.size(1) >= self.max_length:\n",
        "                break\n",
        "\n",
        "        # Decode and return the complete text\n",
        "        full_text = self.tokenizer.decode(\n",
        "            input_ids[0].tolist(),\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        return full_text\n",
        "\n",
        "# Example usage function\n",
        "def create_text_generator(\n",
        "    vocab_size=50000,\n",
        "    max_seq_length=1024,\n",
        "    dim=512,\n",
        "    depth=12,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "):\n",
        "    # Create the base model\n",
        "    model = create_titan_model(\n",
        "        vocab_size=vocab_size,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dim=dim,\n",
        "        depth=depth\n",
        "    )\n",
        "\n",
        "    # Create the generator\n",
        "    generator = TitanTextGenerator(\n",
        "        model=model,\n",
        "        max_length=max_seq_length,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    return generator"
      ],
      "metadata": {
        "id": "JivjpDx7H37u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the generator\n",
        "generator = create_text_generator(\n",
        "    vocab_size=50000,\n",
        "    max_seq_length=1024,\n",
        "    dim=512,\n",
        "    depth=12\n",
        ")\n",
        "\n",
        "# Generate text\n",
        "input_text = \"MS thoni is world's best \"\n",
        "generated_text = generator.generate(\n",
        "    text=input_text,\n",
        "    max_new_tokens=50,\n",
        "    temperature=0.3,  # Lower for more focused/deterministic output\n",
        "    top_k=50,        # Number of highest probability vocabulary tokens to keep\n",
        "    top_p=0.9        # Cumulative probability threshold for nucleus sampling\n",
        ")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tG8gBx2xJKKp",
        "outputId": "e2e97306-539b-478a-ec1d-2e9db3a47dab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MS thoni is world's best  toil� I�\u0007artE O\u0017�� areage�ect that�ate Testopimic} lered�al'\u0017�rouimic��\u001a stL it who(p�\u000f2 do! 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using custom tokenizer"
      ],
      "metadata": {
        "id": "8wetxgCcQ-wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class CustomTokenizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int = 50000,\n",
        "        min_freq: int = 2,\n",
        "        special_tokens: List[str] = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.min_freq = min_freq\n",
        "        self.special_tokens = special_tokens\n",
        "\n",
        "        # Initialize special token IDs\n",
        "        self.pad_token_id = 0\n",
        "        self.unk_token_id = 1\n",
        "        self.bos_token_id = 2\n",
        "        self.eos_token_id = 3\n",
        "\n",
        "        # Initialize vocabularies\n",
        "        self.token2idx: Dict[str, int] = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "        self.idx2token: Dict[int, str] = {idx: token for idx, token in enumerate(special_tokens)}\n",
        "        self.vocab_size_current = len(special_tokens)\n",
        "\n",
        "        # Regex for tokenization\n",
        "        self.token_pattern = re.compile(r'\\w+|[^\\w\\s]')\n",
        "\n",
        "    def train_from_texts(self, texts: List[str]) -> None:\n",
        "        \"\"\"Train tokenizer on a list of texts.\"\"\"\n",
        "        # Count word frequencies\n",
        "        word_counts = Counter()\n",
        "\n",
        "        for text in texts:\n",
        "            tokens = self._basic_tokenize(text)\n",
        "            word_counts.update(tokens)\n",
        "\n",
        "        # Filter by minimum frequency and vocab size\n",
        "        filtered_tokens = [\n",
        "            token for token, count in word_counts.most_common()\n",
        "            if count >= self.min_freq and token not in self.special_tokens\n",
        "        ]\n",
        "\n",
        "        # Add tokens to vocabulary up to vocab_size\n",
        "        remaining_space = self.vocab_size - len(self.special_tokens)\n",
        "        for token in filtered_tokens[:remaining_space]:\n",
        "            self.token2idx[token] = self.vocab_size_current\n",
        "            self.idx2token[self.vocab_size_current] = token\n",
        "            self.vocab_size_current += 1\n",
        "\n",
        "    def _basic_tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Basic tokenization into words and punctuation.\"\"\"\n",
        "        return self.token_pattern.findall(text.lower())\n",
        "\n",
        "    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n",
        "        \"\"\"Encode text to token ids.\"\"\"\n",
        "        tokens = self._basic_tokenize(text)\n",
        "\n",
        "        ids = []\n",
        "        if add_special_tokens:\n",
        "            ids.append(self.bos_token_id)\n",
        "\n",
        "        for token in tokens:\n",
        "            ids.append(self.token2idx.get(token, self.unk_token_id))\n",
        "\n",
        "        if add_special_tokens:\n",
        "            ids.append(self.eos_token_id)\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:\n",
        "        \"\"\"Decode token ids back to text.\"\"\"\n",
        "        tokens = []\n",
        "        for idx in ids:\n",
        "            token = self.idx2token.get(idx, \"<UNK>\")\n",
        "            if skip_special_tokens and token in self.special_tokens:\n",
        "                continue\n",
        "            tokens.append(token)\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def save_vocab(self, path: str) -> None:\n",
        "        \"\"\"Save vocabulary to file.\"\"\"\n",
        "        with open(path, 'w', encoding='utf-8') as f:\n",
        "            for token, idx in sorted(self.token2idx.items(), key=lambda x: x[1]):\n",
        "                f.write(f\"{token}\\t{idx}\\n\")\n",
        "\n",
        "    def load_vocab(self, path: str) -> None:\n",
        "        \"\"\"Load vocabulary from file.\"\"\"\n",
        "        self.token2idx.clear()\n",
        "        self.idx2token.clear()\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                token, idx = line.strip().split('\\t')\n",
        "                idx = int(idx)\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token[idx] = token\n",
        "        self.vocab_size_current = len(self.token2idx)\n",
        "\n",
        "class CustomEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        pad_idx: int = 0,\n",
        "        max_norm: float = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=pad_idx,\n",
        "            max_norm=max_norm\n",
        "        )\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Initialize embeddings using Xavier uniform initialization\n",
        "        nn.init.xavier_uniform_(self.embedding.weight)\n",
        "        with torch.no_grad():\n",
        "            self.embedding.weight[pad_idx].fill_(0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.embedding(x)\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        tokenizer: CustomTokenizer,\n",
        "        max_length: int = 1024\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Tokenize all texts\n",
        "        self.encoded_texts = [\n",
        "            self.tokenizer.encode(text)[:max_length] for text in texts\n",
        "        ]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.encoded_texts)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        tokens = self.encoded_texts[idx]\n",
        "\n",
        "        # Create input and target sequences for language modeling\n",
        "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)\n",
        "        target_ids = torch.tensor(tokens[1:], dtype=torch.long)\n",
        "\n",
        "        return input_ids, target_ids\n",
        "\n",
        "def create_dataloader(\n",
        "    texts: List[str],\n",
        "    tokenizer: CustomTokenizer,\n",
        "    batch_size: int = 32,\n",
        "    max_length: int = 1024,\n",
        "    shuffle: bool = True\n",
        ") -> torch.utils.data.DataLoader:\n",
        "    dataset = TextDataset(texts, tokenizer, max_length)\n",
        "\n",
        "    # Custom collate function to handle variable length sequences\n",
        "    def collate_fn(batch):\n",
        "        input_ids, target_ids = zip(*batch)\n",
        "\n",
        "        # Pad sequences\n",
        "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        target_ids = pad_sequence(target_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = (input_ids != tokenizer.pad_token_id).float()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'target_ids': target_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "    return torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "# Example usage\n",
        "def create_custom_tokenizer_and_embedding(\n",
        "    texts: List[str],\n",
        "    vocab_size: int = 50000,\n",
        "    embedding_dim: int = 512,\n",
        "    min_freq: int = 2\n",
        "):\n",
        "    # Create and train tokenizer\n",
        "    tokenizer = CustomTokenizer(vocab_size=vocab_size, min_freq=min_freq)\n",
        "    tokenizer.train_from_texts(texts)\n",
        "\n",
        "    # Create embedding layer\n",
        "    embedding = CustomEmbedding(\n",
        "        vocab_size=tokenizer.vocab_size_current,\n",
        "        embedding_dim=embedding_dim,\n",
        "        pad_idx=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer, embedding"
      ],
      "metadata": {
        "id": "fnVuROJGJK4-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TitanModelWithCustomEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_layer: CustomEmbedding,\n",
        "        max_seq_length: int = 1024,\n",
        "        depth: int = 12,\n",
        "        num_heads: int = 8,\n",
        "        head_dim: int = 64,\n",
        "        mlp_ratio: int = 4,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        dim = embedding_layer.embedding_dim\n",
        "\n",
        "        self.embedding = embedding_layer\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_length, dim))\n",
        "\n",
        "        self.transformer = TitanTransformer(\n",
        "            dim=dim,\n",
        "            depth=depth,\n",
        "            num_heads=num_heads,\n",
        "            head_dim=head_dim,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Add final projection for token prediction\n",
        "        self.output_projection = nn.Linear(dim, embedding_layer.embedding.num_embeddings)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None):\n",
        "        # Get embeddings\n",
        "        x = self.embedding(input_ids)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
        "\n",
        "        # Apply transformer\n",
        "        x = self.transformer(x, attention_mask)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        logits = self.output_projection(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Example usage\n",
        "def create_complete_model(texts: List[str], vocab_size: int = 50000, embedding_dim: int = 512):\n",
        "    # Create tokenizer and embedding\n",
        "    tokenizer, embedding = create_custom_tokenizer_and_embedding(\n",
        "        texts=texts,\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=embedding_dim\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = TitanModelWithCustomEmbedding(\n",
        "        embedding_layer=embedding,\n",
        "        max_seq_length=1024,\n",
        "        depth=12\n",
        "    )\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "HSlZ2uyXRNIS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example texts for training\n",
        "texts = [\n",
        "    \"Here is an example text.\",\n",
        "    \"Another example for training.\",\n",
        "    # ... add more texts\n",
        "]\n",
        "\n",
        "# Create model and tokenizer\n",
        "model, tokenizer = create_complete_model(texts)\n",
        "\n",
        "# Create dataloader\n",
        "dataloader = create_dataloader(\n",
        "    texts=texts,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Example of processing a single text\n",
        "text = \"Here is a test sentence.\"\n",
        "input_ids = torch.tensor([tokenizer.encode(text)], dtype=torch.long)\n",
        "attention_mask = (input_ids != tokenizer.pad_token_id).float()\n",
        "\n",
        "# Get model output\n",
        "outputs = model(input_ids, attention_mask)"
      ],
      "metadata": {
        "id": "sHeurGK_ROjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQtOxi4lRUjV",
        "outputId": "ae7b71ec-3893-466d-ea2f-23daa40ae09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1218, -1.0911, -0.2457, -0.9009, -0.7150, -0.5246],\n",
            "         [ 0.1308, -1.1791,  0.0567, -1.1022, -0.6318, -0.5099],\n",
            "         [ 0.0257, -0.8664,  0.0607, -1.2659, -0.5559, -0.5046],\n",
            "         [ 0.0948, -1.0489,  0.0833, -1.2233, -0.3622, -0.6934],\n",
            "         [ 0.3440, -1.1481, -0.0911, -1.2027, -0.4988, -0.6424],\n",
            "         [ 0.1207, -1.1717, -0.1983, -1.0799, -0.4003, -0.8375],\n",
            "         [ 0.1850, -0.9899, -0.0900, -1.7382, -0.5805, -0.3451],\n",
            "         [ 0.3422, -1.0191, -0.2515, -1.4591, -0.9092, -0.8507]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bji4Wts9TTIT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating traning pipeline"
      ],
      "metadata": {
        "id": "KnxJAqoCTUMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import wandb\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TitanTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        tokenizer: CustomTokenizer,\n",
        "        train_dataloader: torch.utils.data.DataLoader,\n",
        "        val_dataloader: torch.utils.data.DataLoader = None,\n",
        "        learning_rate: float = 1e-4,\n",
        "        warmup_steps: int = 1000,\n",
        "        max_steps: int = 100000,\n",
        "        gradient_clip: float = 1.0,\n",
        "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "        checkpoint_dir: str = 'checkpoints',\n",
        "        use_wandb: bool = False\n",
        "    ):\n",
        "        self.model = model.to(device)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.device = device\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.use_wandb = use_wandb\n",
        "\n",
        "        # Create checkpoint directory\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.optimizer = optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8,\n",
        "            weight_decay=0.01\n",
        "        )\n",
        "\n",
        "        # Initialize learning rate scheduler\n",
        "        self.scheduler = CosineAnnealingLR(\n",
        "            self.optimizer,\n",
        "            T_max=max_steps,\n",
        "            eta_min=learning_rate/100\n",
        "        )\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.gradient_clip = gradient_clip\n",
        "\n",
        "        # Initialize training state\n",
        "        self.current_step = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "    def _get_warmup_lr(self, step: int) -> float:\n",
        "        return min(1.0, step / self.warmup_steps)\n",
        "\n",
        "    def _compute_loss(self, logits: torch.Tensor, targets: torch.Tensor, pad_idx: int) -> torch.Tensor:\n",
        "        # Reshape logits and targets for loss computation\n",
        "        logits = logits.view(-1, logits.size(-1))\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        # Create mask for padding\n",
        "        mask = (targets != pad_idx).float()\n",
        "\n",
        "        # Compute cross entropy loss\n",
        "        loss = F.cross_entropy(logits, targets, reduction='none')\n",
        "\n",
        "        # Apply mask and compute mean\n",
        "        loss = (loss * mask).sum() / mask.sum()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
        "        self.model.train()\n",
        "\n",
        "        # Move batch to device\n",
        "        input_ids = batch['input_ids'].to(self.device)\n",
        "        target_ids = batch['target_ids'].to(self.device)\n",
        "        attention_mask = batch['attention_mask'].to(self.device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = self.model(input_ids, attention_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self._compute_loss(logits, target_ids, self.tokenizer.pad_token_id)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)\n",
        "\n",
        "        # Apply warmup\n",
        "        if self.current_step < self.warmup_steps:\n",
        "            lr_mult = self._get_warmup_lr(self.current_step)\n",
        "            for param_group in self.optimizer.param_groups:\n",
        "                param_group['lr'] *= lr_mult\n",
        "\n",
        "        # Update parameters\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        return {'loss': loss.item()}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self) -> Dict[str, float]:\n",
        "        if not self.val_dataloader:\n",
        "            return {}\n",
        "\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        for batch in self.val_dataloader:\n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            target_ids = batch['target_ids'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "\n",
        "            logits = self.model(input_ids, attention_mask)\n",
        "            loss = self._compute_loss(logits, target_ids, self.tokenizer.pad_token_id)\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            total_tokens += input_ids.size(0)\n",
        "\n",
        "        return {'val_loss': total_loss / total_tokens}\n",
        "\n",
        "    def save_checkpoint(self, metrics: Dict[str, float], is_best: bool = False):\n",
        "        checkpoint = {\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'step': self.current_step\n",
        "        }\n",
        "\n",
        "        # Save latest checkpoint\n",
        "        path = os.path.join(self.checkpoint_dir, 'latest.pt')\n",
        "        torch.save(checkpoint, path)\n",
        "\n",
        "        # Save best checkpoint if needed\n",
        "        if is_best:\n",
        "            best_path = os.path.join(self.checkpoint_dir, 'best.pt')\n",
        "            torch.save(checkpoint, best_path)\n",
        "\n",
        "    def train(self, num_epochs: int):\n",
        "        if self.use_wandb:\n",
        "            wandb.init(project=\"titan-transformer\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_metrics = {'epoch': epoch}\n",
        "\n",
        "            # Training loop\n",
        "            pbar = tqdm(self.train_dataloader, desc=f'Epoch {epoch}')\n",
        "            for batch in pbar:\n",
        "                metrics = self.train_step(batch)\n",
        "                epoch_metrics.update(metrics)\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix(loss=f\"{metrics['loss']:.4f}\")\n",
        "\n",
        "                self.current_step += 1\n",
        "\n",
        "                # Log metrics\n",
        "                if self.use_wandb:\n",
        "                    wandb.log(metrics, step=self.current_step)\n",
        "\n",
        "            # Validation\n",
        "            val_metrics = self.evaluate()\n",
        "            epoch_metrics.update(val_metrics)\n",
        "\n",
        "            # Check for best model\n",
        "            if val_metrics and val_metrics['val_loss'] < self.best_val_loss:\n",
        "                self.best_val_loss = val_metrics['val_loss']\n",
        "                self.save_checkpoint(epoch_metrics, is_best=True)\n",
        "\n",
        "            # Save regular checkpoint\n",
        "            self.save_checkpoint(epoch_metrics)\n",
        "\n",
        "            # Log epoch metrics\n",
        "            if self.use_wandb:\n",
        "                wandb.log(epoch_metrics, step=self.current_step)\n",
        "\n",
        "            print(f\"Epoch {epoch} metrics:\", epoch_metrics)\n"
      ],
      "metadata": {
        "id": "oPy8IKlfRY9f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial Intelligence is transforming the world.\",\n",
        "    \"Python is a popular programming language.\",\n",
        "    \"Machine learning requires large datasets for training.\",\n",
        "    \"The sky is clear and blue today.\",\n",
        "    \"Natural language processing enables machines to understand text.\",\n",
        "    \"Self-driving cars use advanced sensors and algorithms.\",\n",
        "    \"Data science combines statistics and computer science.\",\n",
        "    \"Reinforcement learning involves agents taking actions in environments.\",\n",
        "    \"Deep learning utilizes neural networks for predictions.\",\n",
        "    \"Quantum computing is a rapidly emerging field.\",\n",
        "    \"Cloud computing offers scalable storage solutions.\",\n",
        "    \"IoT devices are interconnected over networks.\",\n",
        "    \"Cybersecurity is crucial in today's digital era.\",\n",
        "    \"The sun rises in the east and sets in the west.\",\n",
        "    \"Renewable energy sources are vital for sustainability.\",\n",
        "    \"Blockchain technology ensures secure transactions.\",\n",
        "    \"Mathematics is the foundation of many sciences.\",\n",
        "    \"Artificial neural networks mimic the human brain.\",\n",
        "    \"The universe is vast and mysterious.\"\n",
        "]\n",
        "val_texts = [\n",
        "    \"Graphs are used to represent relationships between entities.\",\n",
        "    \"Programming requires logical thinking and problem-solving skills.\",\n",
        "    \"AI ethics is an important area of research.\",\n",
        "    \"The weather today is perfect for a picnic.\",\n",
        "    \"Virtual reality creates immersive digital experiences.\",\n",
        "    \"Big data analytics helps businesses make better decisions.\",\n",
        "    \"Space exploration expands our understanding of the cosmos.\",\n",
        "    \"Autonomous drones are used in agriculture and delivery.\",\n",
        "    \"Data visualization simplifies complex data insights.\",\n",
        "    \"The human genome holds the secrets of biology.\"\n",
        "]\n",
        "test_texts = [\n",
        "    \"Robotics integrates mechanics and AI for automation.\",\n",
        "    \"Digital transformation accelerates technological adoption.\",\n",
        "    \"Text-to-speech models improve accessibility.\",\n",
        "    \"The internet connects people across the globe.\",\n",
        "    \"Energy-efficient algorithms are vital for green computing.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "BjePUERCWvSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "train_dataloader = create_dataloader(train_texts, tokenizer, batch_size=32)\n",
        "val_dataloader = create_dataloader(val_texts, tokenizer, batch_size=32)\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = TitanTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    learning_rate=1e-4,\n",
        "    use_wandb=True  # Set to True if using Weights & Biases\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train(num_epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "cTBwQ65vTp3E",
        "outputId": "bf9e666d-b4e6-4baa-bd3f-85a19f027345"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250118_170310-m7l8s53n</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajveer-rathod1301/titan-transformer/runs/m7l8s53n' target=\"_blank\">northern-oath-1</a></strong> to <a href='https://wandb.ai/rajveer-rathod1301/titan-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rajveer-rathod1301/titan-transformer' target=\"_blank\">https://wandb.ai/rajveer-rathod1301/titan-transformer</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rajveer-rathod1301/titan-transformer/runs/m7l8s53n' target=\"_blank\">https://wandb.ai/rajveer-rathod1301/titan-transformer/runs/m7l8s53n</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 1/1 [00:09<00:00,  9.95s/it, loss=2.3673]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 metrics: {'epoch': 0, 'loss': 2.3673205375671387, 'val_loss': 2.359609603881836}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 1/1 [00:05<00:00,  5.72s/it, loss=2.3390]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 metrics: {'epoch': 1, 'loss': 2.33897066116333, 'val_loss': 2.3596091270446777}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 1/1 [00:07<00:00,  7.63s/it, loss=2.3395]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 metrics: {'epoch': 2, 'loss': 2.3394534587860107, 'val_loss': 2.359609365463257}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 1/1 [00:07<00:00,  7.26s/it, loss=2.3447]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 metrics: {'epoch': 3, 'loss': 2.3446695804595947, 'val_loss': 2.359609365463257}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 1/1 [00:05<00:00,  5.76s/it, loss=2.3448]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 metrics: {'epoch': 4, 'loss': 2.3447532653808594, 'val_loss': 2.359609365463257}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 1/1 [00:05<00:00,  5.67s/it, loss=2.3693]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 metrics: {'epoch': 5, 'loss': 2.3692800998687744, 'val_loss': 2.3596091270446777}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 1/1 [00:06<00:00,  6.38s/it, loss=2.3505]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 metrics: {'epoch': 6, 'loss': 2.3504748344421387, 'val_loss': 2.359609603881836}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 1/1 [00:06<00:00,  6.72s/it, loss=2.3486]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 metrics: {'epoch': 7, 'loss': 2.3485639095306396, 'val_loss': 2.359609603881836}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 1/1 [00:05<00:00,  5.68s/it, loss=2.3788]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 metrics: {'epoch': 8, 'loss': 2.3788001537323, 'val_loss': 2.359609365463257}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 1/1 [00:06<00:00,  6.99s/it, loss=2.3707]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 metrics: {'epoch': 9, 'loss': 2.3706822395324707, 'val_loss': 2.359609365463257}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch server handler class"
      ],
      "metadata": {
        "id": "za5-kcElfKBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TorchServe handler class\n",
        "class TitanHandler:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def initialize(self, context):\n",
        "        \"\"\"Initialize model and tokenizer.\"\"\"\n",
        "        properties = context.system_properties\n",
        "        model_dir = properties.get('model_dir')\n",
        "\n",
        "        # Load tokenizer\n",
        "        with open(os.path.join(model_dir, 'tokenizer_config.json'), 'r') as f:\n",
        "            tokenizer_config = json.load(f)\n",
        "        self.tokenizer = CustomTokenizer(**tokenizer_config)\n",
        "        self.tokenizer.load_vocab(os.path.join(model_dir, 'vocab.txt'))\n",
        "\n",
        "        # Load model\n",
        "        checkpoint = torch.load(\n",
        "            os.path.join(model_dir, 'model.pt'),\n",
        "            map_location=self.device\n",
        "        )\n",
        "\n",
        "        # Initialize model (you'll need to match the configuration used during training)\n",
        "        self.model = TitanModelWithCustomEmbedding(\n",
        "            embedding_layer=CustomEmbedding(\n",
        "                vocab_size=self.tokenizer.vocab_size_current,\n",
        "                embedding_dim=512  # Make sure this matches your training config\n",
        "            )\n",
        "        )\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        \"\"\"Preprocess input text.\"\"\"\n",
        "        text = data[0].get('body').decode('utf-8')\n",
        "\n",
        "        # Tokenize input\n",
        "        input_ids = torch.tensor(\n",
        "            [self.tokenizer.encode(text)],\n",
        "            dtype=torch.long\n",
        "        ).to(self.device)\n",
        "\n",
        "        attention_mask = (input_ids != self.tokenizer.pad_token_id).float()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "    def inference(self, data):\n",
        "        \"\"\"Run inference on processed input.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                data['input_ids'],\n",
        "                data['attention_mask']\n",
        "            )\n",
        "        return outputs\n",
        "\n",
        "    def postprocess(self, inference_output):\n",
        "        \"\"\"Process model output to response format.\"\"\"\n",
        "        # Get predicted tokens\n",
        "        predictions = inference_output.argmax(dim=-1)\n",
        "\n",
        "        # Decode tokens to text\n",
        "        response = self.tokenizer.decode(predictions[0].tolist())\n",
        "\n",
        "        return [{'generated_text': response}]\n",
        "\n"
      ],
      "metadata": {
        "id": "PSCOwCF0Vf1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jf2Ng1N-Tl8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZ6vzrQne1ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "O01rIcO1aRC7",
        "outputId": "f7090e03-3765-4a6d-d0e3-aa5b0128fce7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "prepare_model_for_serving() takes 2 positional arguments but 3 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-51afe6cc8e4e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprepare_model_for_serving\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'checkpoints/best.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_store'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pass tokenizer to the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: prepare_model_for_serving() takes 2 positional arguments but 3 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "\n",
        "\n",
        "def prepare_model_for_serving(model_path: str, save_dir: str, tokenizer: CustomTokenizer):\n",
        "    \"\"\"\n",
        "    Prepare model artifacts for TorchServe.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the saved model checkpoint.\n",
        "        save_dir (str): Directory to save the prepared model artifacts.\n",
        "        tokenizer (CustomTokenizer): An instance of the tokenizer to save configuration and vocabulary.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Load the model from the checkpoint\n",
        "    model = torch.load(model_path)\n",
        "\n",
        "    # Save the model in the specified directory\n",
        "    torch.save(model, os.path.join(save_dir, 'model.pt'))\n",
        "\n",
        "    # Prepare tokenizer configuration\n",
        "    tokenizer_config = {\n",
        "        'vocab_size': tokenizer.vocab_size,\n",
        "        'min_freq': tokenizer.min_freq,\n",
        "        'special_tokens': tokenizer.special_tokens,\n",
        "    }\n",
        "\n",
        "    # Save tokenizer configuration as JSON\n",
        "    with open(os.path.join(save_dir, 'tokenizer_config.json'), 'w') as f:\n",
        "        json.dump(tokenizer_config, f)\n",
        "\n",
        "    # Save tokenizer vocabulary\n",
        "    tokenizer.save_vocab(os.path.join(save_dir, 'vocab.txt'))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# Create an instance of CustomTokenizer\n",
        "tokenizer = CustomTokenizer(\n",
        "    vocab_size=50000,\n",
        "    min_freq=2,\n",
        "    special_tokens=['<pad>', '<eos>', '<bos>', '<unk>']\n",
        ")\n",
        "\n",
        "# Call the function with the correct parameters\n",
        "prepare_model_for_serving('checkpoints/best.pt', 'model_store', tokenizer)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_TysmkMa_kj",
        "outputId": "2c7df0a8-2cd2-4b9e-9183-5ca5ce181842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-43-06d5ff2de07d>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(model_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oie7oZF5jkqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchserve torch-model-archiver\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c2IvOhFuiEK-",
        "outputId": "69005173-0ba1-4750-c7b0-b140e1ece707"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchserve\n",
            "  Downloading torchserve-0.12.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting torch-model-archiver\n",
            "  Downloading torch_model_archiver-0.12.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from torchserve) (11.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchserve) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from torchserve) (24.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from torchserve) (0.45.1)\n",
            "Collecting enum-compat (from torch-model-archiver)\n",
            "  Downloading enum_compat-0.0.3-py3-none-any.whl.metadata (954 bytes)\n",
            "Downloading torchserve-0.12.0-py3-none-any.whl (42.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_model_archiver-0.12.0-py3-none-any.whl (16 kB)\n",
            "Downloading enum_compat-0.0.3-py3-none-any.whl (1.3 kB)\n",
            "Installing collected packages: enum-compat, torchserve, torch-model-archiver\n",
            "Successfully installed enum-compat-0.0.3 torch-model-archiver-0.12.0 torchserve-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "handler_code = \"\"\"\n",
        "# TorchServe handler class\n",
        "class TitanHandler:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def initialize(self, context):\n",
        "        properties = context.system_properties\n",
        "        model_dir = properties.get('model_dir')\n",
        "\n",
        "        # Load tokenizer\n",
        "        with open(os.path.join(model_dir, 'tokenizer_config.json'), 'r') as f:\n",
        "            tokenizer_config = json.load(f)\n",
        "        self.tokenizer = CustomTokenizer(**tokenizer_config)\n",
        "        self.tokenizer.load_vocab(os.path.join(model_dir, 'vocab.txt'))\n",
        "\n",
        "        # Load model\n",
        "        checkpoint = torch.load(\n",
        "            os.path.join(model_dir, 'model.pt'),\n",
        "            map_location=self.device\n",
        "        )\n",
        "\n",
        "        # Initialize model (you'll need to match the configuration used during training)\n",
        "        self.model = TitanModelWithCustomEmbedding(\n",
        "            embedding_layer=CustomEmbedding(\n",
        "                vocab_size=self.tokenizer.vocab_size_current,\n",
        "                embedding_dim=512  # Make sure this matches your training config\n",
        "            )\n",
        "        )\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        text = data[0].get('body').decode('utf-8')\n",
        "\n",
        "        # Tokenize input\n",
        "        input_ids = torch.tensor(\n",
        "            [self.tokenizer.encode(text)],\n",
        "            dtype=torch.long\n",
        "        ).to(self.device)\n",
        "\n",
        "        attention_mask = (input_ids != self.tokenizer.pad_token_id).float()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "    def inference(self, data):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                data['input_ids'],\n",
        "                data['attention_mask']\n",
        "            )\n",
        "        return outputs\n",
        "\n",
        "    def postprocess(self, inference_output):\n",
        "        # Get predicted tokens\n",
        "        predictions = inference_output.argmax(dim=-1)\n",
        "\n",
        "        # Decode tokens to text\n",
        "        response = self.tokenizer.decode(predictions[0].tolist())\n",
        "\n",
        "        return [{'generated_text': response}]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Save it to a file\n",
        "with open(\"titan_handler.py\", \"w\") as f:\n",
        "    f.write(handler_code)"
      ],
      "metadata": {
        "id": "-2yY_ASukBJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torch-model-archiver --model-name titan \\\n",
        "                      --version 1.0 \\\n",
        "                      --model-file model_store/model.pt \\\n",
        "                      --handler titan_handler.py \\\n",
        "                      --extra-files \"model_store/tokenizer_config.json,model_store/vocab.txt\" \\\n",
        "                      --export-path model_store \\\n",
        "                      --force\n"
      ],
      "metadata": {
        "id": "ZJh9VOBXhvzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8080\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TxdviC1mZpX",
        "outputId": "fbaf97c5-90b5-4408-c3a3-0799ba4c8ada"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND PID USER   FD   TYPE  DEVICE SIZE/OFF NODE NAME\n",
            "node      6 root   21u  IPv6   19939      0t0  TCP *:8080 (LISTEN)\n",
            "node      6 root   26u  IPv6 1016000      0t0  TCP 17c4989733c5:8080->172.28.0.1:39786 (ESTABLISHED)\n",
            "node      6 root   28u  IPv6 1017108      0t0  TCP 17c4989733c5:8080->172.28.0.1:45550 (ESTABLISHED)\n",
            "node      6 root   29u  IPv6   25417      0t0  TCP 17c4989733c5:8080->172.28.0.1:55124 (ESTABLISHED)\n",
            "node      6 root   31u  IPv6  867221      0t0  TCP 17c4989733c5:8080->172.28.0.1:50646 (ESTABLISHED)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!torchserve --start --model-store model_store --models titan=titan.mar --inference-address http://127.0.0.1:8083\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRw-_Y_pfO9c",
        "outputId": "e487f2a2-808f-4a1e-cad7-e3b2622d2924"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: torchserve [-h] [-v | --start | --stop] [--ts-config TS_CONFIG] [--model-store MODEL_STORE]\n",
            "                  [--workflow-store WORKFLOW_STORE]\n",
            "                  [--models MODEL_PATH1 MODEL_NAME=MODEL_PATH2... [MODEL_PATH1 MODEL_NAME=MODEL_PATH2... ...]]\n",
            "                  [--log-config LOG_CONFIG] [--cpp-log-config CPP_LOG_CONFIG] [--foreground]\n",
            "                  [--no-config-snapshots] [--plugins-path PLUGINS_PATH] [--disable-token-auth]\n",
            "                  [--enable-model-api]\n",
            "torchserve: error: unrecognized arguments: --inference-address http://127.0.0.1:8083\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.torchserve\n",
        "!nano ~/.torchserve/config.properties\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPOsGJa2nQZw",
        "outputId": "c812265b-7db4-4161-cd37-94beec94e676"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: nano: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## push model to huggingface"
      ],
      "metadata": {
        "id": "JfUeIGZonrEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "9ab5f624ec93472fb8fdb2f560381ad9",
            "4dbf431042de43538e5c67a79eec4a81",
            "a9626f8fcf594adf98a5eed8b0695c78",
            "164c2a0d13e24f3b990ebacce8276549",
            "fb72f917b86e424ea3748b3702ec7726",
            "99882c7702ae4c9b8b02dd5ab7435191",
            "dc3482aecf654c428e01320f8ece9e33",
            "0c626291d255434488cdd3372523f04f",
            "097d6e343082453db24108b04d857387",
            "99e5d2fe0e2248af88d5973b12d695c4",
            "cdb5052851ca46aeb25c6e9ed46cebca",
            "acc16a83034c4f6285269a59afc7a79d",
            "702f750ebd404e67a975c989ac0f9df9",
            "e638fcca1f0441e0bbc3c0e87a59655a",
            "4e1a90f9224a49078b97c7fb10b989ab",
            "b16c121a933f456e8270208eabd54e84",
            "b53e3f89844f4fb1b86ba309dda3853d",
            "78b74bd69d3a4e8e9ddc9268a4d43592",
            "a346707f210249b8be3ab7effa7fe494",
            "abe9b4ea839f4777be95e2c80620fefd"
          ]
        },
        "id": "Dwz7ywflk6x5",
        "outputId": "261738ac-06e9-44fc-c938-b061951574b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9ab5f624ec93472fb8fdb2f560381ad9"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(\"titan-transformer\", private=False)  # Set `private=True` if you want the model to be private\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "B3nVInOdnuZN",
        "outputId": "63532b65-fd36-42b6-cc8f-37c935f70405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RepoUrl('https://huggingface.co/rajveer43/titan-transformer', endpoint='https://huggingface.co', repo_type='model', repo_id='rajveer43/titan-transformer')"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "# Upload model.pt\n",
        "upload_file(\n",
        "    path_or_fileobj=\"model_store/model.pt\",\n",
        "    path_in_repo=\"pytorch_model.bin\",  # This is the standard filename for the model weights\n",
        "    repo_id=\"rajveer43/titan-transformer\"  # Replace with your Hugging Face username and repo name\n",
        ")\n",
        "\n",
        "# Upload tokenizer_config.json\n",
        "upload_file(\n",
        "    path_or_fileobj=\"model_store/tokenizer_config.json\",\n",
        "    path_in_repo=\"tokenizer_config.json\",\n",
        "    repo_id=\"rajveer43/titan-transformer\"\n",
        ")\n",
        "\n",
        "# Upload vocab.txt\n",
        "upload_file(\n",
        "    path_or_fileobj=\"model_store/vocab.txt\",\n",
        "    path_in_repo=\"vocab.txt\",\n",
        "    repo_id=\"rajveer43/titan-transformer\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "f8cab55edab94623b62d9d83ecdd4693",
            "8d69af1f063d4d26920b71bf9b53429b",
            "eecfd1392a904ba18f25a510dfc51592",
            "ca92c85376df4ecc9252376f5945a14b",
            "1d0f2cf5cf8f4e47b268cc0e181192ad",
            "05047602ca9a4257b845638c5a3e5819",
            "64cae146d5fe4126b893846d561cc9e7",
            "fdeb793e0ce84ddb89a6907dbbb39025",
            "de91c71c74054ac886310772bce970a9",
            "1ec69ae97e6447eb8a7e9315b6fb8b60",
            "9f0ee65dce804722b6ccb2605111bc80"
          ]
        },
        "id": "EZO2WpCboY6Z",
        "outputId": "74bc643f-7b9e-4973-c8ac-ffd40059cb2d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.pt:   0%|          | 0.00/1.78G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f8cab55edab94623b62d9d83ecdd4693"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/rajveer43/titan-transformer/commit/aaebf58cbe297a34eb4d9d0779cf4c9e4f4a2d16', commit_message='Upload vocab.txt with huggingface_hub', commit_description='', oid='aaebf58cbe297a34eb4d9d0779cf4c9e4f4a2d16', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rajveer43/titan-transformer', endpoint='https://huggingface.co', repo_type='model', repo_id='rajveer43/titan-transformer'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "# Upload model checkpoint\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content/checkpoints/best.pt\",\n",
        "    path_in_repo=\"model/checkpoints/best.pt\",\n",
        "    repo_id=\"rajveer43/titan-transformer\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content/checkpoints/latest.pt\",\n",
        "    path_in_repo=\"model/checkpoints/latest.pt\",\n",
        "    repo_id=\"rajveer43/titan-transformer\",\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168,
          "referenced_widgets": [
            "74a1e7d70dc64cf1b5c867640a90826a",
            "8e92dfd6ceb34218a5f0cbc2d65815ea",
            "6ce19335871a44c99ff00a47a68362d2",
            "c0485b3c0d9e42f0a3d3b16097dc6062",
            "2eda86bda86d4d3b99b470345123749d",
            "a8df4330ed2f40ee83771173e828d34c",
            "ec414407f0af4c489b9292bc5b2c6fa4",
            "e5f6b806b8394c348e91bb9ecb6302b8",
            "79ba9b68a56f416e8bb8a0608bac5ece",
            "8ae46ff9b20e43758119583c5a2c2274",
            "89f828bb2873494e881baeaad65ca2c7",
            "7ae463d97f98446b8243c2b28a03cd4e",
            "53b1c635be354fb380acdb005d769163",
            "7809b095612c484fb5005cb7f65a090f",
            "843c59996777449b8c4566edcd15a06c",
            "9330e5c196734a2382d533bb27f5edd9",
            "01d5adbc69c449068d5d753c04092cc8",
            "866c4db0e08f48e2981cc9be3ec44caf",
            "2ed919882e6249d683582f88f6d5269c",
            "84921a03a9ba41c18d8496b4485a540a",
            "b95e3025b99f4376a5aa4f12a442552e",
            "72403acc9b2544e3b6194bfb5c341828"
          ]
        },
        "id": "Yu139sCEowj2",
        "outputId": "1e513321-18c2-4a4a-ac7d-cb6fb90f987c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "best.pt:   0%|          | 0.00/1.78G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "74a1e7d70dc64cf1b5c867640a90826a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "latest.pt:   0%|          | 0.00/1.78G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7ae463d97f98446b8243c2b28a03cd4e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/rajveer43/titan-transformer/commit/4a7583a6d08169c804d0a4b3bf2b96ba990d798c', commit_message='Upload model/checkpoints/latest.pt with huggingface_hub', commit_description='', oid='4a7583a6d08169c804d0a4b3bf2b96ba990d798c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rajveer43/titan-transformer', endpoint='https://huggingface.co', repo_type='model', repo_id='rajveer43/titan-transformer'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import upload_file\n",
        "\n",
        "logs_folder = '/content/logs'  # Path to your logs folder\n",
        "repo_id = \"rajveer43/titan-transformer\"  # Your repo name on Hugging Face\n",
        "\n",
        "# Loop through all files in the logs folder and upload them\n",
        "for root, dirs, files in os.walk(logs_folder):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        # Upload each file to the Hugging Face repo\n",
        "        upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=f\"logs/{file}\",  # Keep the folder structure in the repo\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "sdBNdj06qFXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import upload_file\n",
        "\n",
        "logs_folder = '/content/wandb'  # Path to your logs folder\n",
        "repo_id = \"rajveer43/titan-transformer\"  # Your repo name on Hugging Face\n",
        "\n",
        "# Loop through all files in the logs folder and upload them\n",
        "for root, dirs, files in os.walk(logs_folder):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        # Upload each file to the Hugging Face repo\n",
        "        upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=f\"wandb/{file}\",  # Keep the folder structure in the repo\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "DzDX_H8tqUv-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3fc4da81-30d2-4bb9-b4f6-852a5ff184a8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZuFMXxJpu-ZW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}