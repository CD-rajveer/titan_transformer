{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6a76xtPduuU8etaPbTmWo",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0a86feb762bf472488c3ac01c7ecbcc3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_50b5efaaeb654beb9841a30f4bc1a8fb",
              "IPY_MODEL_6c1424f60bc9434eb48795cdc3d46905",
              "IPY_MODEL_9a2c140f839d492aa8b14c09fdc295d9"
            ],
            "layout": "IPY_MODEL_f1d8d199c1164661af55cb9c15b69232"
          }
        },
        "50b5efaaeb654beb9841a30f4bc1a8fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d305642a64e481eae75979677764946",
            "placeholder": "​",
            "style": "IPY_MODEL_4424c5868cd3405cbbd29c85bebc7613",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "6c1424f60bc9434eb48795cdc3d46905": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_11c152285f6444ce8e52be73c60a491f",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c48106ba87f54f908503f35bf4ba3308",
            "value": 26
          }
        },
        "9a2c140f839d492aa8b14c09fdc295d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_407068b164624228b9ca549419aa2144",
            "placeholder": "​",
            "style": "IPY_MODEL_f0baeeab55ec4585921c55698b18ddc9",
            "value": " 26.0/26.0 [00:00&lt;00:00, 2.16kB/s]"
          }
        },
        "f1d8d199c1164661af55cb9c15b69232": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d305642a64e481eae75979677764946": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4424c5868cd3405cbbd29c85bebc7613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "11c152285f6444ce8e52be73c60a491f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c48106ba87f54f908503f35bf4ba3308": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "407068b164624228b9ca549419aa2144": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0baeeab55ec4585921c55698b18ddc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7cf32d0f603547e699de946969342ab6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8e70fbe6980e4ca7a1695c7f93b2ee5d",
              "IPY_MODEL_8794d0fa1bf94a319a9c728d11eb7e28",
              "IPY_MODEL_01b76c679ca4462788d650e072ef8530"
            ],
            "layout": "IPY_MODEL_1e57af09163b409ca0335e8703673282"
          }
        },
        "8e70fbe6980e4ca7a1695c7f93b2ee5d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_556e28e6e9cf4a0daaa9971da55a60fd",
            "placeholder": "​",
            "style": "IPY_MODEL_26ed749f0fcb44c695eed2d9f495d3f1",
            "value": "vocab.json: 100%"
          }
        },
        "8794d0fa1bf94a319a9c728d11eb7e28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_65f158276a164505a9430746d608ddf2",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b40193f7b48f4ed189e9dd64eb65006d",
            "value": 1042301
          }
        },
        "01b76c679ca4462788d650e072ef8530": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54d8df881e8f4999aabc3efb19b4e029",
            "placeholder": "​",
            "style": "IPY_MODEL_fcd4f8eb32ff4cb3955383e22aea838e",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 4.71MB/s]"
          }
        },
        "1e57af09163b409ca0335e8703673282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "556e28e6e9cf4a0daaa9971da55a60fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26ed749f0fcb44c695eed2d9f495d3f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "65f158276a164505a9430746d608ddf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b40193f7b48f4ed189e9dd64eb65006d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "54d8df881e8f4999aabc3efb19b4e029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fcd4f8eb32ff4cb3955383e22aea838e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6051e478b3f34d8899bf35637a4f3aa4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_43feb17de73f4793b6393c7eff5bb781",
              "IPY_MODEL_b0af4659b65b4690a1fa83479d679e92",
              "IPY_MODEL_9aece0f2fe1d442a88e08b5c4aef3845"
            ],
            "layout": "IPY_MODEL_3a807ec605ce4973a97d0ce02b9b56f9"
          }
        },
        "43feb17de73f4793b6393c7eff5bb781": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ba325fba371a4d729e6900d89b6358b2",
            "placeholder": "​",
            "style": "IPY_MODEL_1b781f20ccb349228ce89db61371cfec",
            "value": "merges.txt: 100%"
          }
        },
        "b0af4659b65b4690a1fa83479d679e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c820519e6a14e6391f6328c338bfaa4",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e509d62cc4ec498e9cf67fd85114837a",
            "value": 456318
          }
        },
        "9aece0f2fe1d442a88e08b5c4aef3845": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1098058f8fdb4b4e9e61e8e0351256fb",
            "placeholder": "​",
            "style": "IPY_MODEL_b6139d4b41a14c3697368483227aa049",
            "value": " 456k/456k [00:00&lt;00:00, 3.25MB/s]"
          }
        },
        "3a807ec605ce4973a97d0ce02b9b56f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba325fba371a4d729e6900d89b6358b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b781f20ccb349228ce89db61371cfec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2c820519e6a14e6391f6328c338bfaa4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e509d62cc4ec498e9cf67fd85114837a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1098058f8fdb4b4e9e61e8e0351256fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6139d4b41a14c3697368483227aa049": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cde31a5fe3a0452886cf8d30aae2befa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6123f46891bd43e294e399e992a72b2c",
              "IPY_MODEL_a1d946ea20624bc4a6ae477756377584",
              "IPY_MODEL_8186ee2fa4de4c9db0baf112525d9ec8"
            ],
            "layout": "IPY_MODEL_37d250116e90406398e1135f28b180e9"
          }
        },
        "6123f46891bd43e294e399e992a72b2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_efdfb715d474443bbc43bcc9e6b098a7",
            "placeholder": "​",
            "style": "IPY_MODEL_f9811ceed01a4195aec193372fd87bb5",
            "value": "tokenizer.json: 100%"
          }
        },
        "a1d946ea20624bc4a6ae477756377584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a76602aee51b4c58b8349b57588a35b8",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_149966e0262e45658637853bdd42e855",
            "value": 1355256
          }
        },
        "8186ee2fa4de4c9db0baf112525d9ec8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c81be8dae644643a2558b7f8564ea65",
            "placeholder": "​",
            "style": "IPY_MODEL_391c7ae1f02447d6a673e9987f5ec218",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 8.55MB/s]"
          }
        },
        "37d250116e90406398e1135f28b180e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "efdfb715d474443bbc43bcc9e6b098a7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9811ceed01a4195aec193372fd87bb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a76602aee51b4c58b8349b57588a35b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "149966e0262e45658637853bdd42e855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c81be8dae644643a2558b7f8564ea65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "391c7ae1f02447d6a673e9987f5ec218": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91e3d04e915747bc90d382b9fe199db4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4918aa7e89084eb4968a6159ea147f28",
              "IPY_MODEL_6ef06409c5a94f1697eec44347cc9ef9",
              "IPY_MODEL_13b2a950cb624952af4c8655ad4fe69b"
            ],
            "layout": "IPY_MODEL_d9bc59b8ac0744a5bc6f632d2a2cbb1e"
          }
        },
        "4918aa7e89084eb4968a6159ea147f28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f136d27fab1a48afa06182e8ef2a7833",
            "placeholder": "​",
            "style": "IPY_MODEL_dfaa4b9f19164fa7b3231ee286018e08",
            "value": "config.json: 100%"
          }
        },
        "6ef06409c5a94f1697eec44347cc9ef9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2118fa2954b747758e71cfa1a6d9b1c4",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_021f66ecc73c46738c28234529027c83",
            "value": 665
          }
        },
        "13b2a950cb624952af4c8655ad4fe69b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cbd39ee8caa448a08fadd94fe2e34f3e",
            "placeholder": "​",
            "style": "IPY_MODEL_878daebc61f841d29609882bfcc49fdb",
            "value": " 665/665 [00:00&lt;00:00, 45.4kB/s]"
          }
        },
        "d9bc59b8ac0744a5bc6f632d2a2cbb1e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f136d27fab1a48afa06182e8ef2a7833": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfaa4b9f19164fa7b3231ee286018e08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2118fa2954b747758e71cfa1a6d9b1c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "021f66ecc73c46738c28234529027c83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cbd39ee8caa448a08fadd94fe2e34f3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "878daebc61f841d29609882bfcc49fdb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b13156840a134523b7e75349c7f0a7c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [],
            "layout": "IPY_MODEL_93c55ba22f4b44a9a743a12f9e42e11b"
          }
        },
        "5696dcc327904b278858d660ad3f8f7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c503930f1c9b420793d0534d3f0c7f1b",
            "placeholder": "​",
            "style": "IPY_MODEL_39d90b14d7a54e1a9605363a7f4b47c6",
            "value": "<center> <img\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svg\nalt='Hugging Face'> <br> Copy a token from <a\nhref=\"https://huggingface.co/settings/tokens\" target=\"_blank\">your Hugging Face\ntokens page</a> and paste it below. <br> Immediately click login after copying\nyour token or it might be stored in plain text in this notebook file. </center>"
          }
        },
        "f6766bbbdcb647e6a22041e628d8b2d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "PasswordModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "PasswordModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "PasswordView",
            "continuous_update": true,
            "description": "Token:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_2be16a521e31493c8e8fe458eaa058ca",
            "placeholder": "​",
            "style": "IPY_MODEL_721d9a62d50c4bffbb6daf78eb80c272",
            "value": ""
          }
        },
        "95d85b4bab814c30b3b89bf5636a9f8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "CheckboxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "CheckboxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "CheckboxView",
            "description": "Add token as git credential?",
            "description_tooltip": null,
            "disabled": false,
            "indent": true,
            "layout": "IPY_MODEL_33b7c997711042cc9c523dd876643538",
            "style": "IPY_MODEL_b8580ef455e24285a7a11821ebd51c97",
            "value": true
          }
        },
        "f2d231bd699541949e5a073730f4d625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Login",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_5fca24af31b24d868acd5f74a7b2c616",
            "style": "IPY_MODEL_7f888bf4839b4b1790b25bdb94c9a5e3",
            "tooltip": ""
          }
        },
        "dcc7b21db20e4a7c96d2c09e32befa36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59e5fd00d9d542b08ddc8b93fb100991",
            "placeholder": "​",
            "style": "IPY_MODEL_60cca3e60fd74c2bb6e4db0e2fb2c5a2",
            "value": "\n<b>Pro Tip:</b> If you don't already have one, you can create a dedicated\n'notebooks' token with 'write' access, that you can then easily reuse for all\nnotebooks. </center>"
          }
        },
        "93c55ba22f4b44a9a743a12f9e42e11b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": "center",
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "flex",
            "flex": null,
            "flex_flow": "column",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "50%"
          }
        },
        "c503930f1c9b420793d0534d3f0c7f1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "39d90b14d7a54e1a9605363a7f4b47c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2be16a521e31493c8e8fe458eaa058ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "721d9a62d50c4bffbb6daf78eb80c272": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "33b7c997711042cc9c523dd876643538": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8580ef455e24285a7a11821ebd51c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5fca24af31b24d868acd5f74a7b2c616": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f888bf4839b4b1790b25bdb94c9a5e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "59e5fd00d9d542b08ddc8b93fb100991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60cca3e60fd74c2bb6e4db0e2fb2c5a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c557620bc9394123a90575ff981cbfa8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb8b3fbefe9247408522a3df181bd2f2",
            "placeholder": "​",
            "style": "IPY_MODEL_d799cb39bb0446358210f05d934b7136",
            "value": "Connecting..."
          }
        },
        "eb8b3fbefe9247408522a3df181bd2f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d799cb39bb0446358210f05d934b7136": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e307d4f896844ad789dd60c7d84e206e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7113ac964e9b4f3d9aaa9377e777e6d8",
              "IPY_MODEL_969cf83fff904a2abaa2aeef67a2941e",
              "IPY_MODEL_d55e1fd354f740e9ae331340a30b593a"
            ],
            "layout": "IPY_MODEL_de95fcc37c53468bb39785c94f54abf2"
          }
        },
        "7113ac964e9b4f3d9aaa9377e777e6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe196903723344f3a42800522f2b3af3",
            "placeholder": "​",
            "style": "IPY_MODEL_78d5ea11905d42bcaaf257f307a75594",
            "value": "model.pt: 100%"
          }
        },
        "969cf83fff904a2abaa2aeef67a2941e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9550f280af6d49b0b13cfba60b62c97e",
            "max": 1783648250,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_869c9dbe70614c3b84f9024dfd21c9b0",
            "value": 1783648250
          }
        },
        "d55e1fd354f740e9ae331340a30b593a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d86f68c76bcb424fada8b788ebb94746",
            "placeholder": "​",
            "style": "IPY_MODEL_b5c1a8f0bef3459a9876b478e7b2199b",
            "value": " 1.78G/1.78G [00:41&lt;00:00, 43.5MB/s]"
          }
        },
        "de95fcc37c53468bb39785c94f54abf2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fe196903723344f3a42800522f2b3af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "78d5ea11905d42bcaaf257f307a75594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9550f280af6d49b0b13cfba60b62c97e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "869c9dbe70614c3b84f9024dfd21c9b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d86f68c76bcb424fada8b788ebb94746": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5c1a8f0bef3459a9876b478e7b2199b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5aa21eb9621c43f7b8ed6a88bca8f73e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd0f021734a34171b8e16d8feca7730e",
              "IPY_MODEL_46700dbebc714d04bb0937c20e3d6b32",
              "IPY_MODEL_7774423d40be4a45a70a8e65a3ba9938"
            ],
            "layout": "IPY_MODEL_52e1e326c3de4afab83bb3de7f672a30"
          }
        },
        "dd0f021734a34171b8e16d8feca7730e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e19b95a5f9d7465da13d41a5be9c87e4",
            "placeholder": "​",
            "style": "IPY_MODEL_7dddfc478fdc4e74aaa8ab01ff904e84",
            "value": "best.pt: 100%"
          }
        },
        "46700dbebc714d04bb0937c20e3d6b32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_eb61d11ff02d4c9eb6de6b0de63a61bb",
            "max": 1783621662,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_536fd26457564d608ba41b81171da328",
            "value": 1783621662
          }
        },
        "7774423d40be4a45a70a8e65a3ba9938": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c349571575fd44c7b338e314becbe0b7",
            "placeholder": "​",
            "style": "IPY_MODEL_5ebdad84b0c440249ad5b108ba522ee6",
            "value": " 1.78G/1.78G [00:35&lt;00:00, 31.4MB/s]"
          }
        },
        "52e1e326c3de4afab83bb3de7f672a30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e19b95a5f9d7465da13d41a5be9c87e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7dddfc478fdc4e74aaa8ab01ff904e84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "eb61d11ff02d4c9eb6de6b0de63a61bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "536fd26457564d608ba41b81171da328": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c349571575fd44c7b338e314becbe0b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ebdad84b0c440249ad5b108ba522ee6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "050fdc221f704e5cb12846d2ef4a5487": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_baf32155ec8f4a30b26cc07a609bd25a",
              "IPY_MODEL_6319f503d9a74baea291ec164e6fcebc",
              "IPY_MODEL_cb71539f140048b0965f8cba47086123"
            ],
            "layout": "IPY_MODEL_414a3f21b9864ad29963a13afe83b170"
          }
        },
        "baf32155ec8f4a30b26cc07a609bd25a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f4e7e158eac4ad6b8054cf6cd66cef1",
            "placeholder": "​",
            "style": "IPY_MODEL_ace8961ca8fb4c4a99742e323cc6e936",
            "value": "latest.pt: 100%"
          }
        },
        "6319f503d9a74baea291ec164e6fcebc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b8ab03626f2849be893b8dfd3d5cf4cb",
            "max": 1783694614,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d941eee846354cf89da6bb3da4899b01",
            "value": 1783694614
          }
        },
        "cb71539f140048b0965f8cba47086123": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_44b534b49f2a45e1945bd1395121af60",
            "placeholder": "​",
            "style": "IPY_MODEL_13f7ede4ab1342739d5b0e6d3cab88a1",
            "value": " 1.78G/1.78G [00:38&lt;00:00, 31.7MB/s]"
          }
        },
        "414a3f21b9864ad29963a13afe83b170": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f4e7e158eac4ad6b8054cf6cd66cef1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ace8961ca8fb4c4a99742e323cc6e936": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b8ab03626f2849be893b8dfd3d5cf4cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d941eee846354cf89da6bb3da4899b01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "44b534b49f2a45e1945bd1395121af60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13f7ede4ab1342739d5b0e6d3cab88a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajveer43/titan_transformer/blob/master/Google_Titan_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Titan implementation"
      ],
      "metadata": {
        "id": "bW8uphnsthvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Titan Model Implementation"
      ],
      "metadata": {
        "id": "N3dpyqGzPx8C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNfDK8wOPxa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class DepthwiseSeparableConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
        "        super().__init__()\n",
        "        self.depthwise = nn.Conv1d(\n",
        "            in_channels, in_channels, kernel_size,\n",
        "            padding=padding, groups=in_channels\n",
        "        )\n",
        "        self.pointwise = nn.Conv1d(in_channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class GatingMechanism(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.gate_proj = nn.Linear(dim, dim)\n",
        "        self.transform_proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate = torch.sigmoid(self.gate_proj(x))\n",
        "        transformed = self.transform_proj(x)\n",
        "        return gate * transformed\n",
        "\n",
        "class TitanAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, head_dim=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        # Projections for Q, K, V\n",
        "        self.q_proj = nn.Linear(dim, num_heads * head_dim)\n",
        "        self.k_proj = nn.Linear(dim, num_heads * head_dim)\n",
        "        self.v_proj = nn.Linear(dim, num_heads * head_dim)\n",
        "\n",
        "        # Depthwise separable convolutions\n",
        "        self.q_conv = DepthwiseSeparableConv1d(\n",
        "            num_heads * head_dim,\n",
        "            num_heads * head_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.k_conv = DepthwiseSeparableConv1d(\n",
        "            num_heads * head_dim,\n",
        "            num_heads * head_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.v_conv = DepthwiseSeparableConv1d(\n",
        "            num_heads * head_dim,\n",
        "            num_heads * head_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "        self.out_proj = nn.Linear(num_heads * head_dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, dim = x.shape\n",
        "\n",
        "        # Project and apply SiLU activation\n",
        "        q = F.silu(self.q_proj(x))\n",
        "        k = F.silu(self.k_proj(x))\n",
        "        v = F.silu(self.v_proj(x))\n",
        "\n",
        "        # Reshape for depthwise conv\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Apply depthwise separable convolutions\n",
        "        q = self.q_conv(q).transpose(1, 2)\n",
        "        k = self.k_conv(k).transpose(1, 2)\n",
        "        v = self.v_conv(v).transpose(1, 2)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # L2 normalize queries and keys\n",
        "        q = F.normalize(q, p=2, dim=-1)\n",
        "        k = F.normalize(k, p=2, dim=-1)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            # Properly reshape mask for broadcasting\n",
        "            # mask shape: [batch_size, seq_len] -> [batch_size, 1, 1, seq_len]\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        # Reshape and project output\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class TitanBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, head_dim=64, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = TitanAttention(dim, num_heads, head_dim, dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        mlp_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.SiLU(),\n",
        "            GatingMechanism(mlp_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Residual connection for attention\n",
        "        x = x + self.attn(self.norm1(x), mask)\n",
        "        # Residual connection for MLP\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class TitanTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        num_heads=8,\n",
        "        head_dim=64,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TitanBlock(\n",
        "                dim=dim,\n",
        "                num_heads=num_heads,\n",
        "                head_dim=head_dim,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Example usage\n",
        "def create_titan_model(\n",
        "    vocab_size=50000,\n",
        "    max_seq_length=1024,\n",
        "    dim=512,\n",
        "    depth=12,\n",
        "    num_heads=8,\n",
        "    head_dim=64,\n",
        "    mlp_ratio=4,\n",
        "    dropout=0.1\n",
        "):\n",
        "    class TitanModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding(vocab_size, dim)\n",
        "            self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_length, dim))\n",
        "            self.transformer = TitanTransformer(\n",
        "                dim=dim,\n",
        "                depth=depth,\n",
        "                num_heads=num_heads,\n",
        "                head_dim=head_dim,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout\n",
        "            )\n",
        "\n",
        "        def forward(self, x, mask=None):\n",
        "            # Add positional embeddings\n",
        "            x = self.embedding(x)\n",
        "            x = x + self.pos_embedding[:, :x.size(1), :]\n",
        "\n",
        "            # Apply transformer\n",
        "            x = self.transformer(x, mask)\n",
        "            return x\n",
        "\n",
        "    return TitanModel()"
      ],
      "metadata": {
        "id": "SAIzaOtzGKo2"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model\n",
        "model = create_titan_model(\n",
        "    vocab_size=50000,\n",
        "    max_seq_length=1024,\n",
        "    dim=512,\n",
        "    depth=12\n",
        ")\n",
        "\n",
        "# Example input (batch_size=2, seq_length=128)\n",
        "x = torch.randint(0, 50000, (2, 128))\n",
        "mask = torch.ones(2, 128)\n",
        "\n",
        "# Forward pass\n",
        "output = model(x, mask)"
      ],
      "metadata": {
        "id": "3CVC-r4LHheJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc1YJrHfHiVb",
        "outputId": "a978b7cd-c4e7-48f9-a5ad-712ece5270f6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.0967, -1.5880,  0.4721,  ..., -2.2226,  1.4096, -1.2016],\n",
            "         [ 0.3354, -0.0147,  1.5741,  ..., -1.1080,  1.0628,  0.5618],\n",
            "         [ 0.6820,  1.6194,  1.6092,  ..., -1.7524, -0.3709,  0.3478],\n",
            "         ...,\n",
            "         [ 0.4796, -0.3505,  0.9744,  ..., -0.2081,  0.1899,  0.2010],\n",
            "         [ 1.0270, -0.3352,  1.5500,  ..., -1.2115, -0.6697, -0.1870],\n",
            "         [-0.7622, -1.3119,  1.2983,  ..., -0.9509, -0.5621,  0.1794]],\n",
            "\n",
            "        [[ 1.4440, -0.6268, -0.6059,  ...,  0.3159, -1.1959, -0.0720],\n",
            "         [ 0.1174,  0.2564,  0.0666,  ...,  0.2098, -0.0316,  0.4069],\n",
            "         [-0.3333,  0.6420,  1.4212,  ..., -0.6555, -0.0132, -0.0676],\n",
            "         ...,\n",
            "         [ 2.0356,  0.2566, -0.3156,  ...,  0.4360,  0.4951, -2.2695],\n",
            "         [ 0.5867, -0.8586,  0.8307,  ...,  0.3732,  0.9553,  0.5013],\n",
            "         [-0.5009, -1.7895,  2.6095,  ...,  1.9907, -1.1462,  0.2110]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using the huggingface tokenizer"
      ],
      "metadata": {
        "id": "SxRiNuPKQwR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Optional, Tuple\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "class TitanTextGenerator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: 'TitanModel',\n",
        "        tokenizer: Optional[PreTrainedTokenizerFast] = None,\n",
        "        max_length: int = 1024,\n",
        "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    ):\n",
        "        self.model = model.to(device)\n",
        "        self.model.eval()  # Set to evaluation mode\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Initialize tokenizer if not provided\n",
        "        if tokenizer is None:\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        else:\n",
        "            self.tokenizer = tokenizer\n",
        "\n",
        "    def preprocess_text(self, text: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Tokenize input text\n",
        "        encodings = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encodings['input_ids'].to(self.device)\n",
        "        attention_mask = encodings['attention_mask'].to(self.device)\n",
        "\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_next_token(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 50,\n",
        "        top_p: float = 0.9\n",
        "    ) -> torch.Tensor:\n",
        "        # Get model output\n",
        "        outputs = self.model(input_ids, attention_mask)\n",
        "\n",
        "        # Get logits for the next token (last position)\n",
        "        next_token_logits = outputs[:, -1, :]\n",
        "\n",
        "        # Apply temperature\n",
        "        next_token_logits = next_token_logits / temperature\n",
        "\n",
        "        # Apply top-k filtering\n",
        "        if top_k > 0:\n",
        "            values, indices = torch.topk(next_token_logits, top_k)\n",
        "            min_values = values[:, -1].unsqueeze(-1).expand_as(next_token_logits)\n",
        "            next_token_logits = torch.where(\n",
        "                next_token_logits < min_values,\n",
        "                torch.ones_like(next_token_logits) * float('-inf'),\n",
        "                next_token_logits\n",
        "            )\n",
        "\n",
        "        # # Apply top-p (nucleus) filtering\n",
        "        # if top_p < 1.0:\n",
        "        #     # Sort logits in descending order\n",
        "        #     sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n",
        "        #     cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        #     # Create a mask for tokens to remove\n",
        "        #     sorted_indices_to_remove = cumulative_probs > top_p\n",
        "\n",
        "        #     # Shift the indices to the right to keep also the first token above the threshold\n",
        "        #     sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        #     sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        #     # Scatter sorted indices to original positions\n",
        "        #     indices_to_remove = sorted_indices_to_remove.scatter(\n",
        "        #         dim=1,\n",
        "        #         index=sorted_indices,\n",
        "        #         src=sorted_indices_to_remove\n",
        "        #     )\n",
        "        #     next_token_logits = next_token_logits.masked_fill(indices_to_remove, float('-inf'))\n",
        "\n",
        "        # Apply top-p (nucleus) filtering\n",
        "        if top_p < 1.0:\n",
        "            # Sort logits in descending order\n",
        "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n",
        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "            # Mask tokens with cumulative probability above top_p\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0  # Keep at least one token\n",
        "\n",
        "            # Scatter mask back to logits\n",
        "            mask = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "            next_token_logits = next_token_logits.masked_fill(mask, float('-inf'))\n",
        "\n",
        "\n",
        "        # Sample from the filtered distribution\n",
        "        probs = F.softmax(next_token_logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        return next_token\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        text: str,\n",
        "        max_new_tokens: int = 50,\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 50,\n",
        "        top_p: float = 0.9,\n",
        "        early_stopping: bool = True\n",
        "    ) -> str:\n",
        "        # Preprocess input text\n",
        "        input_ids, attention_mask = self.preprocess_text(text)\n",
        "\n",
        "        generated_tokens = []\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Generate next token\n",
        "            next_token = self.generate_next_token(\n",
        "                input_ids,\n",
        "                attention_mask,\n",
        "                temperature,\n",
        "                top_k,\n",
        "                top_p\n",
        "            )\n",
        "\n",
        "            # Append the new token\n",
        "            generated_tokens.append(next_token.item())\n",
        "\n",
        "            # Update input_ids and attention_mask\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "            attention_mask = torch.cat([\n",
        "                attention_mask,\n",
        "                torch.ones((1, 1), device=self.device)\n",
        "            ], dim=1)\n",
        "\n",
        "            # Check for EOS token\n",
        "            if early_stopping and next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            # Check if we've exceeded max_length\n",
        "            if input_ids.size(1) >= self.max_length:\n",
        "                break\n",
        "\n",
        "        # Decode and return the complete text\n",
        "        full_text = self.tokenizer.decode(\n",
        "            input_ids[0].tolist(),\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        return full_text\n",
        "\n",
        "# Example usage function\n",
        "def create_text_generator(\n",
        "    vocab_size=50000,\n",
        "    max_seq_length=1024,\n",
        "    dim=512,\n",
        "    depth=12,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "):\n",
        "    # Create the base model\n",
        "    model = create_titan_model(\n",
        "        vocab_size=vocab_size,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dim=dim,\n",
        "        depth=depth\n",
        "    )\n",
        "\n",
        "    # Create the generator\n",
        "    generator = TitanTextGenerator(\n",
        "        model=model,\n",
        "        max_length=max_seq_length,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    return generator"
      ],
      "metadata": {
        "id": "JivjpDx7H37u"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the generator\n",
        "generator = create_text_generator(\n",
        "    vocab_size=50000,\n",
        "    max_seq_length=1024,\n",
        "    dim=512,\n",
        "    depth=12\n",
        ")\n",
        "\n",
        "# Generate text\n",
        "input_text = \"MS thoni is world's best \"\n",
        "generated_text = generator.generate(\n",
        "    text=input_text,\n",
        "    max_new_tokens=50,\n",
        "    temperature=0.3,  # Lower for more focused/deterministic output\n",
        "    top_k=50,        # Number of highest probability vocabulary tokens to keep\n",
        "    top_p=0.9        # Cumulative probability threshold for nucleus sampling\n",
        ")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194,
          "referenced_widgets": [
            "0a86feb762bf472488c3ac01c7ecbcc3",
            "50b5efaaeb654beb9841a30f4bc1a8fb",
            "6c1424f60bc9434eb48795cdc3d46905",
            "9a2c140f839d492aa8b14c09fdc295d9",
            "f1d8d199c1164661af55cb9c15b69232",
            "4d305642a64e481eae75979677764946",
            "4424c5868cd3405cbbd29c85bebc7613",
            "11c152285f6444ce8e52be73c60a491f",
            "c48106ba87f54f908503f35bf4ba3308",
            "407068b164624228b9ca549419aa2144",
            "f0baeeab55ec4585921c55698b18ddc9",
            "7cf32d0f603547e699de946969342ab6",
            "8e70fbe6980e4ca7a1695c7f93b2ee5d",
            "8794d0fa1bf94a319a9c728d11eb7e28",
            "01b76c679ca4462788d650e072ef8530",
            "1e57af09163b409ca0335e8703673282",
            "556e28e6e9cf4a0daaa9971da55a60fd",
            "26ed749f0fcb44c695eed2d9f495d3f1",
            "65f158276a164505a9430746d608ddf2",
            "b40193f7b48f4ed189e9dd64eb65006d",
            "54d8df881e8f4999aabc3efb19b4e029",
            "fcd4f8eb32ff4cb3955383e22aea838e",
            "6051e478b3f34d8899bf35637a4f3aa4",
            "43feb17de73f4793b6393c7eff5bb781",
            "b0af4659b65b4690a1fa83479d679e92",
            "9aece0f2fe1d442a88e08b5c4aef3845",
            "3a807ec605ce4973a97d0ce02b9b56f9",
            "ba325fba371a4d729e6900d89b6358b2",
            "1b781f20ccb349228ce89db61371cfec",
            "2c820519e6a14e6391f6328c338bfaa4",
            "e509d62cc4ec498e9cf67fd85114837a",
            "1098058f8fdb4b4e9e61e8e0351256fb",
            "b6139d4b41a14c3697368483227aa049",
            "cde31a5fe3a0452886cf8d30aae2befa",
            "6123f46891bd43e294e399e992a72b2c",
            "a1d946ea20624bc4a6ae477756377584",
            "8186ee2fa4de4c9db0baf112525d9ec8",
            "37d250116e90406398e1135f28b180e9",
            "efdfb715d474443bbc43bcc9e6b098a7",
            "f9811ceed01a4195aec193372fd87bb5",
            "a76602aee51b4c58b8349b57588a35b8",
            "149966e0262e45658637853bdd42e855",
            "4c81be8dae644643a2558b7f8564ea65",
            "391c7ae1f02447d6a673e9987f5ec218",
            "91e3d04e915747bc90d382b9fe199db4",
            "4918aa7e89084eb4968a6159ea147f28",
            "6ef06409c5a94f1697eec44347cc9ef9",
            "13b2a950cb624952af4c8655ad4fe69b",
            "d9bc59b8ac0744a5bc6f632d2a2cbb1e",
            "f136d27fab1a48afa06182e8ef2a7833",
            "dfaa4b9f19164fa7b3231ee286018e08",
            "2118fa2954b747758e71cfa1a6d9b1c4",
            "021f66ecc73c46738c28234529027c83",
            "cbd39ee8caa448a08fadd94fe2e34f3e",
            "878daebc61f841d29609882bfcc49fdb"
          ]
        },
        "id": "tG8gBx2xJKKp",
        "outputId": "cfca9327-8200-4762-b0d4-2bebc6fece60"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a86feb762bf472488c3ac01c7ecbcc3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7cf32d0f603547e699de946969342ab6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6051e478b3f34d8899bf35637a4f3aa4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cde31a5fe3a0452886cf8d30aae2befa"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91e3d04e915747bc90d382b9fe199db4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MS thoni is world's best D not..) canial� W bJ f�\u001eivend�A&A& we�A& weokar00olon int�A& e*\"\u001e�ide�\frell W b their p%re\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using custom tokenizer"
      ],
      "metadata": {
        "id": "8wetxgCcQ-wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class CustomTokenizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int = 50000,\n",
        "        min_freq: int = 2,\n",
        "        special_tokens: List[str] = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.min_freq = min_freq\n",
        "        self.special_tokens = special_tokens\n",
        "\n",
        "        # Initialize special token IDs\n",
        "        self.pad_token_id = 0\n",
        "        self.unk_token_id = 1\n",
        "        self.bos_token_id = 2\n",
        "        self.eos_token_id = 3\n",
        "\n",
        "        # Initialize vocabularies\n",
        "        self.token2idx: Dict[str, int] = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "        self.idx2token: Dict[int, str] = {idx: token for idx, token in enumerate(special_tokens)}\n",
        "        self.vocab_size_current = len(special_tokens)\n",
        "\n",
        "        # Regex for tokenization\n",
        "        self.token_pattern = re.compile(r'\\w+|[^\\w\\s]')\n",
        "\n",
        "    def train_from_texts(self, texts: List[str]) -> None:\n",
        "        \"\"\"Train tokenizer on a list of texts.\"\"\"\n",
        "        # Count word frequencies\n",
        "        word_counts = Counter()\n",
        "\n",
        "        for text in texts:\n",
        "            tokens = self._basic_tokenize(text)\n",
        "            word_counts.update(tokens)\n",
        "\n",
        "        # Filter by minimum frequency and vocab size\n",
        "        filtered_tokens = [\n",
        "            token for token, count in word_counts.most_common()\n",
        "            if count >= self.min_freq and token not in self.special_tokens\n",
        "        ]\n",
        "\n",
        "        # Add tokens to vocabulary up to vocab_size\n",
        "        remaining_space = self.vocab_size - len(self.special_tokens)\n",
        "        for token in filtered_tokens[:remaining_space]:\n",
        "            self.token2idx[token] = self.vocab_size_current\n",
        "            self.idx2token[self.vocab_size_current] = token\n",
        "            self.vocab_size_current += 1\n",
        "\n",
        "    def _basic_tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Basic tokenization into words and punctuation.\"\"\"\n",
        "        return self.token_pattern.findall(text.lower())\n",
        "\n",
        "    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n",
        "        \"\"\"Encode text to token ids.\"\"\"\n",
        "        tokens = self._basic_tokenize(text)\n",
        "\n",
        "        ids = []\n",
        "        if add_special_tokens:\n",
        "            ids.append(self.bos_token_id)\n",
        "\n",
        "        for token in tokens:\n",
        "            ids.append(self.token2idx.get(token, self.unk_token_id))\n",
        "\n",
        "        if add_special_tokens:\n",
        "            ids.append(self.eos_token_id)\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:\n",
        "        \"\"\"Decode token ids back to text.\"\"\"\n",
        "        tokens = []\n",
        "        for idx in ids:\n",
        "            token = self.idx2token.get(idx, \"<UNK>\")\n",
        "            if skip_special_tokens and token in self.special_tokens:\n",
        "                continue\n",
        "            tokens.append(token)\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def save_vocab(self, path: str) -> None:\n",
        "        \"\"\"Save vocabulary to file.\"\"\"\n",
        "        with open(path, 'w', encoding='utf-8') as f:\n",
        "            for token, idx in sorted(self.token2idx.items(), key=lambda x: x[1]):\n",
        "                f.write(f\"{token}\\t{idx}\\n\")\n",
        "\n",
        "    def load_vocab(self, path: str) -> None:\n",
        "        \"\"\"Load vocabulary from file.\"\"\"\n",
        "        self.token2idx.clear()\n",
        "        self.idx2token.clear()\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                token, idx = line.strip().split('\\t')\n",
        "                idx = int(idx)\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token[idx] = token\n",
        "        self.vocab_size_current = len(self.token2idx)\n",
        "\n",
        "class CustomEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        pad_idx: int = 0,\n",
        "        max_norm: float = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=pad_idx,\n",
        "            max_norm=max_norm\n",
        "        )\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Initialize embeddings using Xavier uniform initialization\n",
        "        nn.init.xavier_uniform_(self.embedding.weight)\n",
        "        with torch.no_grad():\n",
        "            self.embedding.weight[pad_idx].fill_(0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.embedding(x)\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        tokenizer: CustomTokenizer,\n",
        "        max_length: int = 1024\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Tokenize all texts\n",
        "        self.encoded_texts = [\n",
        "            self.tokenizer.encode(text)[:max_length] for text in texts\n",
        "        ]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.encoded_texts)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        tokens = self.encoded_texts[idx]\n",
        "\n",
        "        # Create input and target sequences for language modeling\n",
        "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)\n",
        "        target_ids = torch.tensor(tokens[1:], dtype=torch.long)\n",
        "\n",
        "        return input_ids, target_ids\n",
        "\n",
        "def create_dataloader(\n",
        "    texts: List[str],\n",
        "    tokenizer: CustomTokenizer,\n",
        "    batch_size: int = 32,\n",
        "    max_length: int = 1024,\n",
        "    shuffle: bool = True\n",
        ") -> torch.utils.data.DataLoader:\n",
        "    dataset = TextDataset(texts, tokenizer, max_length)\n",
        "\n",
        "    # Custom collate function to handle variable length sequences\n",
        "    def collate_fn(batch):\n",
        "        input_ids, target_ids = zip(*batch)\n",
        "\n",
        "        # Pad sequences\n",
        "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        target_ids = pad_sequence(target_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = (input_ids != tokenizer.pad_token_id).float()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'target_ids': target_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "    return torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "# Example usage\n",
        "def create_custom_tokenizer_and_embedding(\n",
        "    texts: List[str],\n",
        "    vocab_size: int = 50000,\n",
        "    embedding_dim: int = 512,\n",
        "    min_freq: int = 2\n",
        "):\n",
        "    # Create and train tokenizer\n",
        "    tokenizer = CustomTokenizer(vocab_size=vocab_size, min_freq=min_freq)\n",
        "    tokenizer.train_from_texts(texts)\n",
        "\n",
        "    # Create embedding layer\n",
        "    embedding = CustomEmbedding(\n",
        "        vocab_size=tokenizer.vocab_size_current,\n",
        "        embedding_dim=embedding_dim,\n",
        "        pad_idx=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer, embedding"
      ],
      "metadata": {
        "id": "fnVuROJGJK4-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TitanModelWithCustomEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_layer: CustomEmbedding,\n",
        "        max_seq_length: int = 1024,\n",
        "        depth: int = 12,\n",
        "        num_heads: int = 8,\n",
        "        head_dim: int = 64,\n",
        "        mlp_ratio: int = 4,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        dim = embedding_layer.embedding_dim\n",
        "\n",
        "        self.embedding = embedding_layer\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_length, dim))\n",
        "\n",
        "        self.transformer = TitanTransformer(\n",
        "            dim=dim,\n",
        "            depth=depth,\n",
        "            num_heads=num_heads,\n",
        "            head_dim=head_dim,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Add final projection for token prediction\n",
        "        self.output_projection = nn.Linear(dim, embedding_layer.embedding.num_embeddings)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None):\n",
        "        # Get embeddings\n",
        "        x = self.embedding(input_ids)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
        "\n",
        "        # Apply transformer\n",
        "        x = self.transformer(x, attention_mask)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        logits = self.output_projection(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Example usage\n",
        "def create_complete_model(texts: List[str], vocab_size: int = 50000, embedding_dim: int = 512):\n",
        "    # Create tokenizer and embedding\n",
        "    tokenizer, embedding = create_custom_tokenizer_and_embedding(\n",
        "        texts=texts,\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=embedding_dim\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = TitanModelWithCustomEmbedding(\n",
        "        embedding_layer=embedding,\n",
        "        max_seq_length=1024,\n",
        "        depth=12\n",
        "    )\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "HSlZ2uyXRNIS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example texts for training\n",
        "texts = [\n",
        "    \"Here is an example text.\",\n",
        "    \"Another example for training.\",\n",
        "    # ... add more texts\n",
        "]\n",
        "\n",
        "# Create model and tokenizer\n",
        "model, tokenizer = create_complete_model(texts)\n",
        "\n",
        "# Create dataloader\n",
        "dataloader = create_dataloader(\n",
        "    texts=texts,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Example of processing a single text\n",
        "text = \"Here is a test sentence.\"\n",
        "input_ids = torch.tensor([tokenizer.encode(text)], dtype=torch.long)\n",
        "attention_mask = (input_ids != tokenizer.pad_token_id).float()\n",
        "\n",
        "# Get model output\n",
        "outputs = model(input_ids, attention_mask)"
      ],
      "metadata": {
        "id": "sHeurGK_ROjd"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQtOxi4lRUjV",
        "outputId": "7ddcab0b-6436-418e-c7c8-d7e4619a1f27"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[-0.8365,  0.1644, -0.6665, -0.4761, -0.0949,  0.0239],\n",
            "         [-0.7550,  0.0202, -0.7357, -0.5227, -0.6834, -0.0340],\n",
            "         [-0.9247,  0.0017, -0.5474, -0.7903, -0.6522,  0.1734],\n",
            "         [-0.7042, -0.1944, -0.3851, -0.4764, -0.4044,  0.4866],\n",
            "         [-1.1108, -0.0616, -0.6679, -0.6358, -0.3751,  0.4013],\n",
            "         [-0.7235, -0.0043, -0.3396, -0.5388, -0.7040,  0.1414],\n",
            "         [-1.0377, -0.1011, -0.3936, -0.5497, -0.3725,  0.3084],\n",
            "         [-0.7377,  0.1324, -0.0662, -0.2708, -0.4715,  0.3859]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bji4Wts9TTIT"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating traning pipeline"
      ],
      "metadata": {
        "id": "KnxJAqoCTUMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import wandb\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TitanTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        tokenizer: CustomTokenizer,\n",
        "        train_dataloader: torch.utils.data.DataLoader,\n",
        "        val_dataloader: torch.utils.data.DataLoader = None,\n",
        "        learning_rate: float = 1e-4,\n",
        "        warmup_steps: int = 1000,\n",
        "        max_steps: int = 100000,\n",
        "        gradient_clip: float = 1.0,\n",
        "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "        checkpoint_dir: str = 'checkpoints',\n",
        "        use_wandb: bool = False\n",
        "    ):\n",
        "        self.model = model.to(device)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.device = device\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.use_wandb = use_wandb\n",
        "\n",
        "        # Create checkpoint directory\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.optimizer = optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8,\n",
        "            weight_decay=0.01\n",
        "        )\n",
        "\n",
        "        # Initialize learning rate scheduler\n",
        "        self.scheduler = CosineAnnealingLR(\n",
        "            self.optimizer,\n",
        "            T_max=max_steps,\n",
        "            eta_min=learning_rate/100\n",
        "        )\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.gradient_clip = gradient_clip\n",
        "\n",
        "        # Initialize training state\n",
        "        self.current_step = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "    def _get_warmup_lr(self, step: int) -> float:\n",
        "        return min(1.0, step / self.warmup_steps)\n",
        "\n",
        "    def _compute_loss(self, logits: torch.Tensor, targets: torch.Tensor, pad_idx: int) -> torch.Tensor:\n",
        "        # Reshape logits and targets for loss computation\n",
        "        logits = logits.view(-1, logits.size(-1))\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        # Create mask for padding\n",
        "        mask = (targets != pad_idx).float()\n",
        "\n",
        "        # Compute cross entropy loss\n",
        "        loss = F.cross_entropy(logits, targets, reduction='none')\n",
        "\n",
        "        # Apply mask and compute mean\n",
        "        loss = (loss * mask).sum() / mask.sum()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
        "        self.model.train()\n",
        "\n",
        "        # Move batch to device\n",
        "        input_ids = batch['input_ids'].to(self.device)\n",
        "        target_ids = batch['target_ids'].to(self.device)\n",
        "        attention_mask = batch['attention_mask'].to(self.device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = self.model(input_ids, attention_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self._compute_loss(logits, target_ids, self.tokenizer.pad_token_id)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)\n",
        "\n",
        "        # Apply warmup\n",
        "        if self.current_step < self.warmup_steps:\n",
        "            lr_mult = self._get_warmup_lr(self.current_step)\n",
        "            for param_group in self.optimizer.param_groups:\n",
        "                param_group['lr'] *= lr_mult\n",
        "\n",
        "        # Update parameters\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        return {'loss': loss.item()}\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    # def evaluate(self) -> Dict[str, float]:\n",
        "    #     if not self.val_dataloader:\n",
        "    #         return {}\n",
        "\n",
        "    #     self.model.eval()\n",
        "    #     total_loss = 0\n",
        "    #     total_tokens = 0\n",
        "\n",
        "    #     for batch in self.val_dataloader:\n",
        "    #         input_ids = batch['input_ids'].to(self.device)\n",
        "    #         target_ids = batch['target_ids'].to(self.device)\n",
        "    #         attention_mask = batch['attention_mask'].to(self.device)\n",
        "\n",
        "    #         logits = self.model(input_ids, attention_mask)\n",
        "    #         loss = self._compute_loss(logits, target_ids, self.tokenizer.pad_token_id)\n",
        "\n",
        "    #         total_loss += loss.item() * input_ids.size(0)\n",
        "    #         total_tokens += input_ids.size(0)\n",
        "\n",
        "    #     return {'val_loss': total_loss / total_tokens}\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, dataloader: torch.utils.data.DataLoader) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluates the model on the provided dataloader.\n",
        "\n",
        "        Args:\n",
        "            dataloader: The dataloader to use for evaluation.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the evaluation metrics.\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        for batch in dataloader:  # Use the provided dataloader\n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            target_ids = batch['target_ids'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "\n",
        "            logits = self.model(input_ids, attention_mask)\n",
        "            loss = self._compute_loss(logits, target_ids, self.tokenizer.pad_token_id)\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            total_tokens += input_ids.size(0)\n",
        "\n",
        "        return {'val_loss': total_loss / total_tokens}\n",
        "\n",
        "    def save_checkpoint(self, metrics: Dict[str, float], is_best: bool = False):\n",
        "        checkpoint = {\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'step': self.current_step\n",
        "        }\n",
        "\n",
        "        # Save latest checkpoint\n",
        "        path = os.path.join(self.checkpoint_dir, 'latest.pt')\n",
        "        torch.save(checkpoint, path)\n",
        "\n",
        "        # Save best checkpoint if needed\n",
        "        if is_best:\n",
        "            best_path = os.path.join(self.checkpoint_dir, 'best.pt')\n",
        "            torch.save(checkpoint, best_path)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def train(self, num_epochs: int):\n",
        "        if self.use_wandb:\n",
        "            wandb.init(project=\"titan-transformer\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_metrics = {'epoch': epoch}\n",
        "\n",
        "            # Training loop\n",
        "            pbar = tqdm(self.train_dataloader, desc=f'Epoch {epoch}')\n",
        "            for batch in pbar:\n",
        "                metrics = self.train_step(batch)\n",
        "                epoch_metrics.update(metrics)\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix(loss=f\"{metrics['loss']:.4f}\")\n",
        "\n",
        "                self.current_step += 1\n",
        "\n",
        "                # Log metrics\n",
        "                if self.use_wandb:\n",
        "                    wandb.log(metrics, step=self.current_step)\n",
        "\n",
        "            # Validation\n",
        "            if self.val_dataloader:  # Check if a validation dataloader exists\n",
        "                val_metrics = self.evaluate(self.val_dataloader)  # Pass the dataloader here\n",
        "                epoch_metrics.update(val_metrics)\n",
        "\n",
        "                # Check for best model\n",
        "                if val_metrics and val_metrics['val_loss'] < self.best_val_loss:\n",
        "                    self.best_val_loss = val_metrics['val_loss']\n",
        "                    self.save_checkpoint(epoch_metrics, is_best=True)\n",
        "\n",
        "            # Save regular checkpoint\n",
        "            self.save_checkpoint(epoch_metrics)\n",
        "\n",
        "            # Log epoch metrics\n",
        "            if self.use_wandb:\n",
        "                wandb.log(epoch_metrics, step=self.current_step)\n",
        "\n",
        "            print(f\"Epoch {epoch} metrics:\", epoch_metrics)\n",
        "\n",
        "\n",
        "\n",
        "    # def train(self, num_epochs: int):\n",
        "    #     if self.use_wandb:\n",
        "    #         wandb.init(project=\"titan-transformer\")\n",
        "\n",
        "    #     for epoch in range(num_epochs):\n",
        "    #         epoch_metrics = {'epoch': epoch}\n",
        "\n",
        "    #         # Training loop\n",
        "    #         pbar = tqdm(self.train_dataloader, desc=f'Epoch {epoch}')\n",
        "    #         for batch in pbar:\n",
        "    #             metrics = self.train_step(batch)\n",
        "    #             epoch_metrics.update(metrics)\n",
        "\n",
        "    #             # Update progress bar\n",
        "    #             pbar.set_postfix(loss=f\"{metrics['loss']:.4f}\")\n",
        "\n",
        "    #             self.current_step += 1\n",
        "\n",
        "    #             # Log metrics\n",
        "    #             if self.use_wandb:\n",
        "    #                 wandb.log(metrics, step=self.current_step)\n",
        "\n",
        "    #         # Validation\n",
        "    #         val_metrics = self.evaluate()\n",
        "    #         epoch_metrics.update(val_metrics)\n",
        "\n",
        "    #         # Check for best model\n",
        "    #         if val_metrics and val_metrics['val_loss'] < self.best_val_loss:\n",
        "    #             self.best_val_loss = val_metrics['val_loss']\n",
        "    #             self.save_checkpoint(epoch_metrics, is_best=True)\n",
        "\n",
        "    #         # Save regular checkpoint\n",
        "    #         self.save_checkpoint(epoch_metrics)\n",
        "\n",
        "    #         # Log epoch metrics\n",
        "    #         if self.use_wandb:\n",
        "    #             wandb.log(epoch_metrics, step=self.current_step)\n",
        "\n",
        "    #         print(f\"Epoch {epoch} metrics:\", epoch_metrics)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def test(self, test_dataloader: torch.utils.data.DataLoader) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluates the model on the test dataset.\n",
        "\n",
        "        Args:\n",
        "            test_dataloader: The dataloader for the test dataset.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the evaluation metrics for the test dataset.\n",
        "        \"\"\"\n",
        "        print(\"Evaluating on test dataset...\")\n",
        "        metrics = self.evaluate(test_dataloader)\n",
        "        print(f\"Test Results: {metrics}\")\n",
        "        return metrics\n",
        "\n"
      ],
      "metadata": {
        "id": "oPy8IKlfRY9f"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_texts = [\n",
        "#     \"The quick brown fox jumps over the lazy dog.\",\n",
        "#     \"Artificial Intelligence is transforming the world.\",\n",
        "#     \"Python is a popular programming language.\",\n",
        "#     \"Machine learning requires large datasets for training.\",\n",
        "#     \"The sky is clear and blue today.\",\n",
        "#     \"Natural language processing enables machines to understand text.\",\n",
        "#     \"Self-driving cars use advanced sensors and algorithms.\",\n",
        "#     \"Data science combines statistics and computer science.\",\n",
        "#     \"Reinforcement learning involves agents taking actions in environments.\",\n",
        "#     \"Deep learning utilizes neural networks for predictions.\",\n",
        "#     \"Quantum computing is a rapidly emerging field.\",\n",
        "#     \"Cloud computing offers scalable storage solutions.\",\n",
        "#     \"IoT devices are interconnected over networks.\",\n",
        "#     \"Cybersecurity is crucial in today's digital era.\",\n",
        "#     \"The sun rises in the east and sets in the west.\",\n",
        "#     \"Renewable energy sources are vital for sustainability.\",\n",
        "#     \"Blockchain technology ensures secure transactions.\",\n",
        "#     \"Mathematics is the foundation of many sciences.\",\n",
        "#     \"Artificial neural networks mimic the human brain.\",\n",
        "#     \"The universe is vast and mysterious.\"\n",
        "# ]\n",
        "# val_texts = [\n",
        "#     \"Graphs are used to represent relationships between entities.\",\n",
        "#     \"Programming requires logical thinking and problem-solving skills.\",\n",
        "#     \"AI ethics is an important area of research.\",\n",
        "#     \"The weather today is perfect for a picnic.\",\n",
        "#     \"Virtual reality creates immersive digital experiences.\",\n",
        "#     \"Big data analytics helps businesses make better decisions.\",\n",
        "#     \"Space exploration expands our understanding of the cosmos.\",\n",
        "#     \"Autonomous drones are used in agriculture and delivery.\",\n",
        "#     \"Data visualization simplifies complex data insights.\",\n",
        "#     \"The human genome holds the secrets of biology.\"\n",
        "# ]\n",
        "# test_texts = [\n",
        "#     \"Robotics integrates mechanics and AI for automation.\",\n",
        "#     \"Digital transformation accelerates technological adoption.\",\n",
        "#     \"Text-to-speech models improve accessibility.\",\n",
        "#     \"The internet connects people across the globe.\",\n",
        "#     \"Energy-efficient algorithms are vital for green computing.\"\n",
        "# ]\n"
      ],
      "metadata": {
        "id": "BjePUERCWvSa"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datapreprocessing\n"
      ],
      "metadata": {
        "id": "W3zuLjOOwJd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load dataset"
      ],
      "metadata": {
        "id": "HGlVnenOUTMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install kagglehub"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1HrT3FdqUeCK",
        "outputId": "9298ff0b-cabb-4b88-fa70-732e21846aa6"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kagglehub in /usr/local/lib/python3.11/dist-packages (0.3.6)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kagglehub) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from kagglehub) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from kagglehub) (4.67.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->kagglehub) (2024.12.14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"jsonali2003/linguogen-text-generation-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7rv8qn9USsQ",
        "outputId": "a3b4010a-90b4-4606-f717-e5a1fc337291"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/jsonali2003/linguogen-text-generation-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10.0k/10.0k [00:00<00:00, 20.6MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n",
            "Path to dataset files: /root/.cache/kagglehub/datasets/jsonali2003/linguogen-text-generation-dataset/versions/1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Download the latest version of the dataset\n",
        "path = kagglehub.dataset_download(\"jsonali2003/linguogen-text-generation-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)\n",
        "\n",
        "# List all CSV files in the downloaded dataset folder\n",
        "import os\n",
        "csv_files = [os.path.join(path, file) for file in os.listdir(path) if file.endswith('.csv')]\n",
        "\n",
        "# Combine all CSV files into one DataFrame\n",
        "dataframes = [pd.read_csv(file) for file in csv_files]\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "\n",
        "# Ensure the dataset has the necessary columns\n",
        "assert 'Input Text' in combined_df.columns, \"'Input Text' column is missing!\"\n",
        "assert 'Target Text' in combined_df.columns, \"'Target Text' column is missing!\"\n",
        "\n",
        "# Split into input and target texts\n",
        "input_texts = combined_df['Input Text'].tolist()\n",
        "target_texts = combined_df['Target Text'].tolist()\n",
        "\n",
        "# Split into train, validation, and test sets (70/15/15 split)\n",
        "train_inputs, temp_inputs, train_targets, temp_targets = train_test_split(\n",
        "    input_texts, target_texts, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "val_inputs, test_inputs, val_targets, test_targets = train_test_split(\n",
        "    temp_inputs, temp_targets, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "# Combine inputs and targets for each set\n",
        "train_texts = [text for sublist in zip(train_inputs, train_targets) for text in sublist]\n",
        "val_texts = [text for sublist in zip(val_inputs, val_targets) for text in sublist]\n",
        "test_texts = [text for sublist in zip(test_inputs, test_targets) for text in sublist]\n",
        "\n",
        "print(\"Data processing completed!\")\n",
        "print(f\"Training set size: {len(train_texts)}\")\n",
        "print(f\"Validation set size: {len(val_texts)}\")\n",
        "print(f\"Test set size: {len(test_texts)}\")\n"
      ],
      "metadata": {
        "id": "1Xz_Z1-SwMs0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "44b87a38-deb1-435a-83c3-5fb06c2d6604"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/jsonali2003/linguogen-text-generation-dataset/versions/1\n",
            "Data processing completed!\n",
            "Training set size: 712\n",
            "Validation set size: 152\n",
            "Test set size: 154\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "train_dataloader = create_dataloader(train_texts, tokenizer, batch_size=32)\n",
        "val_dataloader = create_dataloader(val_texts, tokenizer, batch_size=32)\n",
        "test_dataloader = create_dataloader(test_texts, tokenizer, batch_size=32)\n",
        "\n",
        "# Initialize trainer\n",
        "# trainer = TitanTrainer(\n",
        "#     model=model,\n",
        "#     tokenizer=tokenizer,\n",
        "#     train_dataloader=train_dataloader,\n",
        "#     val_dataloader=val_dataloader,\n",
        "#     learning_rate=1e-4,\n",
        "#     use_wandb=True  # Set to True if using Weights & Biases\n",
        "# )\n",
        "\n",
        "trainer = TitanTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    learning_rate=2e-6,  # Try a smaller learning rate\n",
        "    use_wandb=True\n",
        ")\n",
        "\n",
        "# Train model\n",
        "\n"
      ],
      "metadata": {
        "id": "cTBwQ65vTp3E"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(num_epochs=10)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        },
        "id": "396--f24xLiy",
        "outputId": "6fcb44b2-99ea-4ca9-8f1d-02a2f3bfa5d8"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250120_172613-zd4d9kbw</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajveer-rathod1301/titan-transformer/runs/zd4d9kbw' target=\"_blank\">expert-sound-6</a></strong> to <a href='https://wandb.ai/rajveer-rathod1301/titan-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rajveer-rathod1301/titan-transformer' target=\"_blank\">https://wandb.ai/rajveer-rathod1301/titan-transformer</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rajveer-rathod1301/titan-transformer/runs/zd4d9kbw' target=\"_blank\">https://wandb.ai/rajveer-rathod1301/titan-transformer/runs/zd4d9kbw</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 23/23 [00:04<00:00,  5.10it/s, loss=1.6319]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 metrics: {'epoch': 0, 'loss': 1.6319234371185303, 'val_loss': 1.6296189207779734}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 23/23 [00:04<00:00,  5.47it/s, loss=1.6476]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 metrics: {'epoch': 1, 'loss': 1.6475766897201538, 'val_loss': 1.6304314575697247}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 23/23 [00:04<00:00,  5.50it/s, loss=1.5976]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 metrics: {'epoch': 2, 'loss': 1.5975821018218994, 'val_loss': 1.6298372180838334}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 23/23 [00:04<00:00,  5.41it/s, loss=1.6443]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 metrics: {'epoch': 3, 'loss': 1.644335150718689, 'val_loss': 1.6299264744708413}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 23/23 [00:04<00:00,  5.39it/s, loss=1.6581]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 metrics: {'epoch': 4, 'loss': 1.6580688953399658, 'val_loss': 1.6294582893973903}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 23/23 [00:04<00:00,  5.39it/s, loss=1.6458]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 metrics: {'epoch': 5, 'loss': 1.645768404006958, 'val_loss': 1.6299429316269725}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 23/23 [00:04<00:00,  5.54it/s, loss=1.6635]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 metrics: {'epoch': 6, 'loss': 1.6635148525238037, 'val_loss': 1.6300332797201056}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 23/23 [00:04<00:00,  5.35it/s, loss=1.6618]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 metrics: {'epoch': 7, 'loss': 1.6617921590805054, 'val_loss': 1.6298224925994873}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 23/23 [00:04<00:00,  5.38it/s, loss=1.6483]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 metrics: {'epoch': 8, 'loss': 1.6482689380645752, 'val_loss': 1.6299194913161428}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 23/23 [00:04<00:00,  5.33it/s, loss=1.6482]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 metrics: {'epoch': 9, 'loss': 1.648231029510498, 'val_loss': 1.6298907869740535}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train(num_epochs=20)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 711
        },
        "id": "usoriEJUxOgI",
        "outputId": "85a73a7d-0202-4402-9b5a-6277402304b4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 23/23 [00:04<00:00,  5.18it/s, loss=1.6268]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 metrics: {'epoch': 0, 'loss': 1.6268436908721924, 'val_loss': 1.6300630381232815}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 23/23 [00:04<00:00,  5.30it/s, loss=1.6294]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 metrics: {'epoch': 1, 'loss': 1.6293824911117554, 'val_loss': 1.6306439889104742}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 23/23 [00:04<00:00,  5.49it/s, loss=1.6311]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 metrics: {'epoch': 2, 'loss': 1.6310920715332031, 'val_loss': 1.629970161538375}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 23/23 [00:04<00:00,  5.28it/s, loss=1.7066]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 metrics: {'epoch': 3, 'loss': 1.7065709829330444, 'val_loss': 1.6303465805555646}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 23/23 [00:04<00:00,  5.32it/s, loss=1.6404]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 metrics: {'epoch': 4, 'loss': 1.6403987407684326, 'val_loss': 1.629947166693838}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 23/23 [00:04<00:00,  5.29it/s, loss=1.6268]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 metrics: {'epoch': 5, 'loss': 1.6268471479415894, 'val_loss': 1.6295787974407798}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 23/23 [00:04<00:00,  5.41it/s, loss=1.6359]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 metrics: {'epoch': 6, 'loss': 1.6358535289764404, 'val_loss': 1.630042132578398}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 23/23 [00:04<00:00,  5.35it/s, loss=1.6174]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 metrics: {'epoch': 7, 'loss': 1.617431879043579, 'val_loss': 1.629981938161348}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 23/23 [00:04<00:00,  5.29it/s, loss=1.6432]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 metrics: {'epoch': 8, 'loss': 1.6432247161865234, 'val_loss': 1.6298159172660427}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 23/23 [00:04<00:00,  5.38it/s, loss=1.6151]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 metrics: {'epoch': 9, 'loss': 1.6150799989700317, 'val_loss': 1.6301471057691073}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10: 100%|██████████| 23/23 [00:04<00:00,  5.42it/s, loss=1.6282]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10 metrics: {'epoch': 10, 'loss': 1.628175973892212, 'val_loss': 1.6299375232897306}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11: 100%|██████████| 23/23 [00:04<00:00,  5.29it/s, loss=1.6510]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11 metrics: {'epoch': 11, 'loss': 1.650999665260315, 'val_loss': 1.6298131880007292}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12: 100%|██████████| 23/23 [00:04<00:00,  5.32it/s, loss=1.6280]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12 metrics: {'epoch': 12, 'loss': 1.6280491352081299, 'val_loss': 1.6297607045424611}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13: 100%|██████████| 23/23 [00:04<00:00,  5.40it/s, loss=1.6311]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13 metrics: {'epoch': 13, 'loss': 1.6310899257659912, 'val_loss': 1.6294966622402793}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14: 100%|██████████| 23/23 [00:04<00:00,  5.35it/s, loss=1.6259]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14 metrics: {'epoch': 14, 'loss': 1.625939965248108, 'val_loss': 1.6299472294355695}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15: 100%|██████████| 23/23 [00:04<00:00,  5.42it/s, loss=1.6499]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15 metrics: {'epoch': 15, 'loss': 1.6499181985855103, 'val_loss': 1.6299481391906738}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16: 100%|██████████| 23/23 [00:04<00:00,  5.31it/s, loss=1.6628]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16 metrics: {'epoch': 16, 'loss': 1.6628252267837524, 'val_loss': 1.6303436003233258}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17: 100%|██████████| 23/23 [00:04<00:00,  5.37it/s, loss=1.6477]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17 metrics: {'epoch': 17, 'loss': 1.6477144956588745, 'val_loss': 1.6303890253368176}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18: 100%|██████████| 23/23 [00:04<00:00,  5.45it/s, loss=1.6179]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18 metrics: {'epoch': 18, 'loss': 1.6178721189498901, 'val_loss': 1.6301242991497642}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19: 100%|██████████| 23/23 [00:04<00:00,  5.30it/s, loss=1.6513]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19 metrics: {'epoch': 19, 'loss': 1.65126633644104, 'val_loss': 1.6295391383923983}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "test_results = trainer.test(test_dataloader)\n",
        "print(f\"Test Results: {test_results}\")\n"
      ],
      "metadata": {
        "id": "enzog-yG6scl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch server handler class"
      ],
      "metadata": {
        "id": "za5-kcElfKBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TorchServe handler class\n",
        "class TitanHandler:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # def initialize(self, context):\n",
        "    #     \"\"\"Initialize model and tokenizer.\"\"\"\n",
        "    #     properties = context.system_properties\n",
        "    #     model_dir = properties.get('model_dir')\n",
        "\n",
        "    #     # Load tokenizer\n",
        "    #     with open(os.path.join(model_dir, 'tokenizer_config.json'), 'r') as f:\n",
        "    #         tokenizer_config = json.load(f)\n",
        "    #     self.tokenizer = CustomTokenizer(**tokenizer_config)\n",
        "    #     self.tokenizer.load_vocab(os.path.join(model_dir, 'vocab.txt'))\n",
        "\n",
        "    #     # Load model\n",
        "    #     checkpoint = torch.load(\n",
        "    #         os.path.join(model_dir, 'model.pt'),\n",
        "    #         map_location=self.device\n",
        "    #     )\n",
        "\n",
        "    #     # Initialize model (you'll need to match the configuration used during training)\n",
        "    #     self.model = TitanModelWithCustomEmbedding(\n",
        "    #         embedding_layer=CustomEmbedding(\n",
        "    #             vocab_size=self.tokenizer.vocab_size_current,\n",
        "    #             embedding_dim=512  # Make sure this matches your training config\n",
        "    #         )\n",
        "    #     )\n",
        "    #     self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    #     self.model.to(self.device)\n",
        "    #     self.model.eval()\n",
        "    # def initialize(self, context):\n",
        "    #     \"\"\"Initialize model and tokenizer.\"\"\"\n",
        "    #     properties = context.system_properties\n",
        "    #     model_dir = properties.get('model_dir')\n",
        "\n",
        "    #     # Load tokenizer\n",
        "    #     with open(os.path.join(model_dir, 'tokenizer_config.json'), 'r') as f:\n",
        "    #         tokenizer_config = json.load(f)\n",
        "    #     self.tokenizer = CustomTokenizer(**tokenizer_config)\n",
        "    #     self.tokenizer.load_vocab(os.path.join(model_dir, 'vocab.txt'))\n",
        "\n",
        "    #     # Load model\n",
        "    #     checkpoint = torch.load(\n",
        "    #         os.path.join(model_dir, 'model.pt'),\n",
        "    #         map_location=self.device\n",
        "    #     )\n",
        "\n",
        "    #     # Initialize model with the correct vocabulary size from the loaded tokenizer\n",
        "    #     self.model = TitanModelWithCustomEmbedding(\n",
        "    #         embedding_layer=CustomEmbedding(\n",
        "    #             vocab_size=self.tokenizer.vocab_size_current,  # Use tokenizer's vocab size\n",
        "    #             embedding_dim=512  # Make sure this matches your training config\n",
        "    #         ),\n",
        "    #         max_seq_length=1024,  # Add this to match your training config\n",
        "    #         depth=12           # Add this to match your training config\n",
        "    #     )\n",
        "    #     self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    #     self.model.to(self.device)\n",
        "    #     self.model.eval()\n",
        "    def initialize(self, context):\n",
        "        \"\"\"Initialize model and tokenizer.\"\"\"\n",
        "        properties = context.system_properties\n",
        "        model_dir = properties.get('model_dir')\n",
        "\n",
        "        # Load tokenizer FIRST\n",
        "        with open(os.path.join(model_dir, 'tokenizer_config.json'), 'r') as f:\n",
        "            tokenizer_config = json.load(f)\n",
        "        self.tokenizer = CustomTokenizer(**tokenizer_config)\n",
        "        self.tokenizer.load_vocab(os.path.join(model_dir, 'vocab.txt'))\n",
        "\n",
        "        # Print vocabulary size for debugging\n",
        "        print(f\"Loaded tokenizer vocabulary size: {self.tokenizer.vocab_size_current}\")\n",
        "\n",
        "        # Load checkpoint to check saved model configuration\n",
        "        checkpoint = torch.load(\n",
        "            os.path.join(model_dir, 'model.pt'),\n",
        "            map_location=self.device\n",
        "        )\n",
        "\n",
        "        # Print model state dict shapes for debugging\n",
        "        print(\"\\nModel checkpoint shapes:\")\n",
        "        for name, param in checkpoint['model_state_dict'].items():\n",
        "            print(f\"{name}: {param.shape}\")\n",
        "\n",
        "        # Initialize model with the EXACT same vocabulary size as in checkpoint\n",
        "        vocab_size = checkpoint['model_state_dict']['embedding.embedding.weight'].shape[0]\n",
        "        print(f\"\\nInitializing model with vocabulary size: {vocab_size}\")\n",
        "\n",
        "        self.model = TitanModelWithCustomEmbedding(\n",
        "            embedding_layer=CustomEmbedding(\n",
        "                vocab_size=vocab_size,  # Use vocabulary size from checkpoint\n",
        "                embedding_dim=512\n",
        "            ),\n",
        "            max_seq_length=1024,\n",
        "            depth=12\n",
        "        )\n",
        "\n",
        "        # Load state dict\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        \"\"\"Preprocess input text.\"\"\"\n",
        "        text = data[0].get('body').decode('utf-8')\n",
        "\n",
        "        # Tokenize input\n",
        "        input_ids = torch.tensor(\n",
        "            [self.tokenizer.encode(text)],\n",
        "            dtype=torch.long\n",
        "        ).to(self.device)\n",
        "\n",
        "        attention_mask = (input_ids != self.tokenizer.pad_token_id).float()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "    def inference(self, data):\n",
        "        \"\"\"Run inference on processed input.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                data['input_ids'],\n",
        "                data['attention_mask']\n",
        "            )\n",
        "        return outputs\n",
        "\n",
        "    def postprocess(self, inference_output):\n",
        "        \"\"\"Process model output to response format.\"\"\"\n",
        "        # Get predicted tokens\n",
        "        predictions = inference_output.argmax(dim=-1)\n",
        "\n",
        "        # Decode tokens to text\n",
        "        response = self.tokenizer.decode(predictions[0].tolist())\n",
        "\n",
        "        return [{'generated_text': response}]\n",
        "\n"
      ],
      "metadata": {
        "id": "PSCOwCF0Vf1c"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First, create a mock context class\n",
        "import torch\n",
        "import os\n",
        "import json\n",
        "class MockContext:\n",
        "    def __init__(self, model_dir):\n",
        "        self.system_properties = {\n",
        "            'model_dir': model_dir  # Directory where model files are stored\n",
        "        }\n",
        "\n",
        "# Modified testing code\n",
        "def test_model_generation(model_dir):\n",
        "    \"\"\"\n",
        "    Test the model with various input prompts\n",
        "    \"\"\"\n",
        "    # Create mock context\n",
        "    context = MockContext(model_dir=model_dir)\n",
        "\n",
        "    # Initialize handler with mock context\n",
        "    handler = TitanHandler()\n",
        "    handler.initialize(context)\n",
        "\n",
        "    # Test cases\n",
        "    test_cases = [\n",
        "        # Simple completion tasks\n",
        "        \"The weather today is\",\n",
        "        \"The cat sat on the\",\n",
        "\n",
        "        # Question answering\n",
        "        \"What is the capital of France?\",\n",
        "        \"How does photosynthesis work?\",\n",
        "\n",
        "        # Environmental topics\n",
        "        \"Climate change is causing\",\n",
        "        \"Renewable energy sources include\",\n",
        "\n",
        "        # Animal-related\n",
        "        \"Lions are known for\",\n",
        "        \"Endangered species need protection because\",\n",
        "    ]\n",
        "\n",
        "    print(\"Starting Model Testing...\\n\")\n",
        "    print(\"-\" * 50)\n",
        "\n",
        "    for i, test_case in enumerate(test_cases, 1):\n",
        "        print(f\"\\nTest Case {i}:\")\n",
        "        print(f\"Input: {test_case}\")\n",
        "\n",
        "        try:\n",
        "            # Process through the handler pipeline\n",
        "            input_data = [{'body': test_case.encode('utf-8')}]\n",
        "            processed = handler.preprocess(input_data)\n",
        "            output = handler.inference(processed)\n",
        "            result = handler.postprocess(output)\n",
        "\n",
        "            print(f\"Generated: {result[0]['generated_text']}\")\n",
        "            print(\"-\" * 50)\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing test case: {str(e)}\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "# Usage example:\n",
        "model_dir = \"/content/model_store\"  # Replace with your model directory path\n",
        "test_model_generation(model_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEc0zY5s7ERu",
        "outputId": "de521c18-511c-46b7-8a19-f423a1a6c9b7"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded tokenizer vocabulary size: 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-9dd93b6c883c>:79: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Model checkpoint shapes:\n",
            "pos_embedding: torch.Size([1, 1024, 512])\n",
            "embedding.embedding.weight: torch.Size([6, 512])\n",
            "transformer.layers.0.norm1.weight: torch.Size([512])\n",
            "transformer.layers.0.norm1.bias: torch.Size([512])\n",
            "transformer.layers.0.attn.q_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.0.attn.q_proj.bias: torch.Size([512])\n",
            "transformer.layers.0.attn.k_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.0.attn.k_proj.bias: torch.Size([512])\n",
            "transformer.layers.0.attn.v_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.0.attn.v_proj.bias: torch.Size([512])\n",
            "transformer.layers.0.attn.q_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.0.attn.q_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.0.attn.q_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.0.attn.q_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.0.attn.k_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.0.attn.k_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.0.attn.k_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.0.attn.k_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.0.attn.v_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.0.attn.v_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.0.attn.v_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.0.attn.v_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.0.attn.out_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.0.attn.out_proj.bias: torch.Size([512])\n",
            "transformer.layers.0.norm2.weight: torch.Size([512])\n",
            "transformer.layers.0.norm2.bias: torch.Size([512])\n",
            "transformer.layers.0.mlp.0.weight: torch.Size([2048, 512])\n",
            "transformer.layers.0.mlp.0.bias: torch.Size([2048])\n",
            "transformer.layers.0.mlp.2.gate_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.0.mlp.2.gate_proj.bias: torch.Size([2048])\n",
            "transformer.layers.0.mlp.2.transform_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.0.mlp.2.transform_proj.bias: torch.Size([2048])\n",
            "transformer.layers.0.mlp.4.weight: torch.Size([512, 2048])\n",
            "transformer.layers.0.mlp.4.bias: torch.Size([512])\n",
            "transformer.layers.1.norm1.weight: torch.Size([512])\n",
            "transformer.layers.1.norm1.bias: torch.Size([512])\n",
            "transformer.layers.1.attn.q_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.1.attn.q_proj.bias: torch.Size([512])\n",
            "transformer.layers.1.attn.k_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.1.attn.k_proj.bias: torch.Size([512])\n",
            "transformer.layers.1.attn.v_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.1.attn.v_proj.bias: torch.Size([512])\n",
            "transformer.layers.1.attn.q_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.1.attn.q_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.1.attn.q_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.1.attn.q_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.1.attn.k_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.1.attn.k_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.1.attn.k_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.1.attn.k_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.1.attn.v_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.1.attn.v_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.1.attn.v_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.1.attn.v_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.1.attn.out_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.1.attn.out_proj.bias: torch.Size([512])\n",
            "transformer.layers.1.norm2.weight: torch.Size([512])\n",
            "transformer.layers.1.norm2.bias: torch.Size([512])\n",
            "transformer.layers.1.mlp.0.weight: torch.Size([2048, 512])\n",
            "transformer.layers.1.mlp.0.bias: torch.Size([2048])\n",
            "transformer.layers.1.mlp.2.gate_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.1.mlp.2.gate_proj.bias: torch.Size([2048])\n",
            "transformer.layers.1.mlp.2.transform_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.1.mlp.2.transform_proj.bias: torch.Size([2048])\n",
            "transformer.layers.1.mlp.4.weight: torch.Size([512, 2048])\n",
            "transformer.layers.1.mlp.4.bias: torch.Size([512])\n",
            "transformer.layers.2.norm1.weight: torch.Size([512])\n",
            "transformer.layers.2.norm1.bias: torch.Size([512])\n",
            "transformer.layers.2.attn.q_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.2.attn.q_proj.bias: torch.Size([512])\n",
            "transformer.layers.2.attn.k_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.2.attn.k_proj.bias: torch.Size([512])\n",
            "transformer.layers.2.attn.v_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.2.attn.v_proj.bias: torch.Size([512])\n",
            "transformer.layers.2.attn.q_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.2.attn.q_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.2.attn.q_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.2.attn.q_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.2.attn.k_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.2.attn.k_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.2.attn.k_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.2.attn.k_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.2.attn.v_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.2.attn.v_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.2.attn.v_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.2.attn.v_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.2.attn.out_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.2.attn.out_proj.bias: torch.Size([512])\n",
            "transformer.layers.2.norm2.weight: torch.Size([512])\n",
            "transformer.layers.2.norm2.bias: torch.Size([512])\n",
            "transformer.layers.2.mlp.0.weight: torch.Size([2048, 512])\n",
            "transformer.layers.2.mlp.0.bias: torch.Size([2048])\n",
            "transformer.layers.2.mlp.2.gate_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.2.mlp.2.gate_proj.bias: torch.Size([2048])\n",
            "transformer.layers.2.mlp.2.transform_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.2.mlp.2.transform_proj.bias: torch.Size([2048])\n",
            "transformer.layers.2.mlp.4.weight: torch.Size([512, 2048])\n",
            "transformer.layers.2.mlp.4.bias: torch.Size([512])\n",
            "transformer.layers.3.norm1.weight: torch.Size([512])\n",
            "transformer.layers.3.norm1.bias: torch.Size([512])\n",
            "transformer.layers.3.attn.q_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.3.attn.q_proj.bias: torch.Size([512])\n",
            "transformer.layers.3.attn.k_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.3.attn.k_proj.bias: torch.Size([512])\n",
            "transformer.layers.3.attn.v_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.3.attn.v_proj.bias: torch.Size([512])\n",
            "transformer.layers.3.attn.q_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.3.attn.q_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.3.attn.q_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.3.attn.q_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.3.attn.k_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.3.attn.k_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.3.attn.k_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.3.attn.k_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.3.attn.v_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.3.attn.v_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.3.attn.v_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.3.attn.v_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.3.attn.out_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.3.attn.out_proj.bias: torch.Size([512])\n",
            "transformer.layers.3.norm2.weight: torch.Size([512])\n",
            "transformer.layers.3.norm2.bias: torch.Size([512])\n",
            "transformer.layers.3.mlp.0.weight: torch.Size([2048, 512])\n",
            "transformer.layers.3.mlp.0.bias: torch.Size([2048])\n",
            "transformer.layers.3.mlp.2.gate_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.3.mlp.2.gate_proj.bias: torch.Size([2048])\n",
            "transformer.layers.3.mlp.2.transform_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.3.mlp.2.transform_proj.bias: torch.Size([2048])\n",
            "transformer.layers.3.mlp.4.weight: torch.Size([512, 2048])\n",
            "transformer.layers.3.mlp.4.bias: torch.Size([512])\n",
            "transformer.layers.4.norm1.weight: torch.Size([512])\n",
            "transformer.layers.4.norm1.bias: torch.Size([512])\n",
            "transformer.layers.4.attn.q_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.4.attn.q_proj.bias: torch.Size([512])\n",
            "transformer.layers.4.attn.k_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.4.attn.k_proj.bias: torch.Size([512])\n",
            "transformer.layers.4.attn.v_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.4.attn.v_proj.bias: torch.Size([512])\n",
            "transformer.layers.4.attn.q_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.4.attn.q_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.4.attn.q_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.4.attn.q_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.4.attn.k_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.4.attn.k_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.4.attn.k_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.4.attn.k_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.4.attn.v_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.4.attn.v_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.4.attn.v_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.4.attn.v_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.4.attn.out_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.4.attn.out_proj.bias: torch.Size([512])\n",
            "transformer.layers.4.norm2.weight: torch.Size([512])\n",
            "transformer.layers.4.norm2.bias: torch.Size([512])\n",
            "transformer.layers.4.mlp.0.weight: torch.Size([2048, 512])\n",
            "transformer.layers.4.mlp.0.bias: torch.Size([2048])\n",
            "transformer.layers.4.mlp.2.gate_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.4.mlp.2.gate_proj.bias: torch.Size([2048])\n",
            "transformer.layers.4.mlp.2.transform_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.4.mlp.2.transform_proj.bias: torch.Size([2048])\n",
            "transformer.layers.4.mlp.4.weight: torch.Size([512, 2048])\n",
            "transformer.layers.4.mlp.4.bias: torch.Size([512])\n",
            "transformer.layers.5.norm1.weight: torch.Size([512])\n",
            "transformer.layers.5.norm1.bias: torch.Size([512])\n",
            "transformer.layers.5.attn.q_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.5.attn.q_proj.bias: torch.Size([512])\n",
            "transformer.layers.5.attn.k_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.5.attn.k_proj.bias: torch.Size([512])\n",
            "transformer.layers.5.attn.v_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.5.attn.v_proj.bias: torch.Size([512])\n",
            "transformer.layers.5.attn.q_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.5.attn.q_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.5.attn.q_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.5.attn.q_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.5.attn.k_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.5.attn.k_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.5.attn.k_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.5.attn.k_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.5.attn.v_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.5.attn.v_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.5.attn.v_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.5.attn.v_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.5.attn.out_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.5.attn.out_proj.bias: torch.Size([512])\n",
            "transformer.layers.5.norm2.weight: torch.Size([512])\n",
            "transformer.layers.5.norm2.bias: torch.Size([512])\n",
            "transformer.layers.5.mlp.0.weight: torch.Size([2048, 512])\n",
            "transformer.layers.5.mlp.0.bias: torch.Size([2048])\n",
            "transformer.layers.5.mlp.2.gate_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.5.mlp.2.gate_proj.bias: torch.Size([2048])\n",
            "transformer.layers.5.mlp.2.transform_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.5.mlp.2.transform_proj.bias: torch.Size([2048])\n",
            "transformer.layers.5.mlp.4.weight: torch.Size([512, 2048])\n",
            "transformer.layers.5.mlp.4.bias: torch.Size([512])\n",
            "transformer.layers.6.norm1.weight: torch.Size([512])\n",
            "transformer.layers.6.norm1.bias: torch.Size([512])\n",
            "transformer.layers.6.attn.q_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.6.attn.q_proj.bias: torch.Size([512])\n",
            "transformer.layers.6.attn.k_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.6.attn.k_proj.bias: torch.Size([512])\n",
            "transformer.layers.6.attn.v_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.6.attn.v_proj.bias: torch.Size([512])\n",
            "transformer.layers.6.attn.q_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.6.attn.q_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.6.attn.q_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.6.attn.q_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.6.attn.k_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.6.attn.k_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.6.attn.k_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.6.attn.k_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.6.attn.v_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.6.attn.v_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.6.attn.v_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.6.attn.v_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.6.attn.out_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.6.attn.out_proj.bias: torch.Size([512])\n",
            "transformer.layers.6.norm2.weight: torch.Size([512])\n",
            "transformer.layers.6.norm2.bias: torch.Size([512])\n",
            "transformer.layers.6.mlp.0.weight: torch.Size([2048, 512])\n",
            "transformer.layers.6.mlp.0.bias: torch.Size([2048])\n",
            "transformer.layers.6.mlp.2.gate_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.6.mlp.2.gate_proj.bias: torch.Size([2048])\n",
            "transformer.layers.6.mlp.2.transform_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.6.mlp.2.transform_proj.bias: torch.Size([2048])\n",
            "transformer.layers.6.mlp.4.weight: torch.Size([512, 2048])\n",
            "transformer.layers.6.mlp.4.bias: torch.Size([512])\n",
            "transformer.layers.7.norm1.weight: torch.Size([512])\n",
            "transformer.layers.7.norm1.bias: torch.Size([512])\n",
            "transformer.layers.7.attn.q_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.7.attn.q_proj.bias: torch.Size([512])\n",
            "transformer.layers.7.attn.k_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.7.attn.k_proj.bias: torch.Size([512])\n",
            "transformer.layers.7.attn.v_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.7.attn.v_proj.bias: torch.Size([512])\n",
            "transformer.layers.7.attn.q_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.7.attn.q_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.7.attn.q_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.7.attn.q_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.7.attn.k_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.7.attn.k_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.7.attn.k_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.7.attn.k_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.7.attn.v_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.7.attn.v_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.7.attn.v_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.7.attn.v_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.7.attn.out_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.7.attn.out_proj.bias: torch.Size([512])\n",
            "transformer.layers.7.norm2.weight: torch.Size([512])\n",
            "transformer.layers.7.norm2.bias: torch.Size([512])\n",
            "transformer.layers.7.mlp.0.weight: torch.Size([2048, 512])\n",
            "transformer.layers.7.mlp.0.bias: torch.Size([2048])\n",
            "transformer.layers.7.mlp.2.gate_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.7.mlp.2.gate_proj.bias: torch.Size([2048])\n",
            "transformer.layers.7.mlp.2.transform_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.7.mlp.2.transform_proj.bias: torch.Size([2048])\n",
            "transformer.layers.7.mlp.4.weight: torch.Size([512, 2048])\n",
            "transformer.layers.7.mlp.4.bias: torch.Size([512])\n",
            "transformer.layers.8.norm1.weight: torch.Size([512])\n",
            "transformer.layers.8.norm1.bias: torch.Size([512])\n",
            "transformer.layers.8.attn.q_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.8.attn.q_proj.bias: torch.Size([512])\n",
            "transformer.layers.8.attn.k_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.8.attn.k_proj.bias: torch.Size([512])\n",
            "transformer.layers.8.attn.v_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.8.attn.v_proj.bias: torch.Size([512])\n",
            "transformer.layers.8.attn.q_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.8.attn.q_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.8.attn.q_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.8.attn.q_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.8.attn.k_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.8.attn.k_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.8.attn.k_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.8.attn.k_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.8.attn.v_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.8.attn.v_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.8.attn.v_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.8.attn.v_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.8.attn.out_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.8.attn.out_proj.bias: torch.Size([512])\n",
            "transformer.layers.8.norm2.weight: torch.Size([512])\n",
            "transformer.layers.8.norm2.bias: torch.Size([512])\n",
            "transformer.layers.8.mlp.0.weight: torch.Size([2048, 512])\n",
            "transformer.layers.8.mlp.0.bias: torch.Size([2048])\n",
            "transformer.layers.8.mlp.2.gate_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.8.mlp.2.gate_proj.bias: torch.Size([2048])\n",
            "transformer.layers.8.mlp.2.transform_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.8.mlp.2.transform_proj.bias: torch.Size([2048])\n",
            "transformer.layers.8.mlp.4.weight: torch.Size([512, 2048])\n",
            "transformer.layers.8.mlp.4.bias: torch.Size([512])\n",
            "transformer.layers.9.norm1.weight: torch.Size([512])\n",
            "transformer.layers.9.norm1.bias: torch.Size([512])\n",
            "transformer.layers.9.attn.q_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.9.attn.q_proj.bias: torch.Size([512])\n",
            "transformer.layers.9.attn.k_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.9.attn.k_proj.bias: torch.Size([512])\n",
            "transformer.layers.9.attn.v_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.9.attn.v_proj.bias: torch.Size([512])\n",
            "transformer.layers.9.attn.q_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.9.attn.q_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.9.attn.q_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.9.attn.q_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.9.attn.k_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.9.attn.k_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.9.attn.k_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.9.attn.k_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.9.attn.v_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.9.attn.v_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.9.attn.v_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.9.attn.v_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.9.attn.out_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.9.attn.out_proj.bias: torch.Size([512])\n",
            "transformer.layers.9.norm2.weight: torch.Size([512])\n",
            "transformer.layers.9.norm2.bias: torch.Size([512])\n",
            "transformer.layers.9.mlp.0.weight: torch.Size([2048, 512])\n",
            "transformer.layers.9.mlp.0.bias: torch.Size([2048])\n",
            "transformer.layers.9.mlp.2.gate_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.9.mlp.2.gate_proj.bias: torch.Size([2048])\n",
            "transformer.layers.9.mlp.2.transform_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.9.mlp.2.transform_proj.bias: torch.Size([2048])\n",
            "transformer.layers.9.mlp.4.weight: torch.Size([512, 2048])\n",
            "transformer.layers.9.mlp.4.bias: torch.Size([512])\n",
            "transformer.layers.10.norm1.weight: torch.Size([512])\n",
            "transformer.layers.10.norm1.bias: torch.Size([512])\n",
            "transformer.layers.10.attn.q_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.10.attn.q_proj.bias: torch.Size([512])\n",
            "transformer.layers.10.attn.k_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.10.attn.k_proj.bias: torch.Size([512])\n",
            "transformer.layers.10.attn.v_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.10.attn.v_proj.bias: torch.Size([512])\n",
            "transformer.layers.10.attn.q_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.10.attn.q_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.10.attn.q_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.10.attn.q_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.10.attn.k_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.10.attn.k_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.10.attn.k_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.10.attn.k_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.10.attn.v_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.10.attn.v_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.10.attn.v_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.10.attn.v_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.10.attn.out_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.10.attn.out_proj.bias: torch.Size([512])\n",
            "transformer.layers.10.norm2.weight: torch.Size([512])\n",
            "transformer.layers.10.norm2.bias: torch.Size([512])\n",
            "transformer.layers.10.mlp.0.weight: torch.Size([2048, 512])\n",
            "transformer.layers.10.mlp.0.bias: torch.Size([2048])\n",
            "transformer.layers.10.mlp.2.gate_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.10.mlp.2.gate_proj.bias: torch.Size([2048])\n",
            "transformer.layers.10.mlp.2.transform_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.10.mlp.2.transform_proj.bias: torch.Size([2048])\n",
            "transformer.layers.10.mlp.4.weight: torch.Size([512, 2048])\n",
            "transformer.layers.10.mlp.4.bias: torch.Size([512])\n",
            "transformer.layers.11.norm1.weight: torch.Size([512])\n",
            "transformer.layers.11.norm1.bias: torch.Size([512])\n",
            "transformer.layers.11.attn.q_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.11.attn.q_proj.bias: torch.Size([512])\n",
            "transformer.layers.11.attn.k_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.11.attn.k_proj.bias: torch.Size([512])\n",
            "transformer.layers.11.attn.v_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.11.attn.v_proj.bias: torch.Size([512])\n",
            "transformer.layers.11.attn.q_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.11.attn.q_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.11.attn.q_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.11.attn.q_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.11.attn.k_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.11.attn.k_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.11.attn.k_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.11.attn.k_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.11.attn.v_conv.depthwise.weight: torch.Size([512, 1, 3])\n",
            "transformer.layers.11.attn.v_conv.depthwise.bias: torch.Size([512])\n",
            "transformer.layers.11.attn.v_conv.pointwise.weight: torch.Size([512, 512, 1])\n",
            "transformer.layers.11.attn.v_conv.pointwise.bias: torch.Size([512])\n",
            "transformer.layers.11.attn.out_proj.weight: torch.Size([512, 512])\n",
            "transformer.layers.11.attn.out_proj.bias: torch.Size([512])\n",
            "transformer.layers.11.norm2.weight: torch.Size([512])\n",
            "transformer.layers.11.norm2.bias: torch.Size([512])\n",
            "transformer.layers.11.mlp.0.weight: torch.Size([2048, 512])\n",
            "transformer.layers.11.mlp.0.bias: torch.Size([2048])\n",
            "transformer.layers.11.mlp.2.gate_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.11.mlp.2.gate_proj.bias: torch.Size([2048])\n",
            "transformer.layers.11.mlp.2.transform_proj.weight: torch.Size([2048, 2048])\n",
            "transformer.layers.11.mlp.2.transform_proj.bias: torch.Size([2048])\n",
            "transformer.layers.11.mlp.4.weight: torch.Size([512, 2048])\n",
            "transformer.layers.11.mlp.4.bias: torch.Size([512])\n",
            "transformer.norm.weight: torch.Size([512])\n",
            "transformer.norm.bias: torch.Size([512])\n",
            "output_projection.weight: torch.Size([6, 512])\n",
            "output_projection.bias: torch.Size([6])\n",
            "\n",
            "Initializing model with vocabulary size: 6\n",
            "Starting Model Testing...\n",
            "\n",
            "--------------------------------------------------\n",
            "\n",
            "Test Case 1:\n",
            "Input: The weather today is\n",
            "Generated: <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "--------------------------------------------------\n",
            "\n",
            "Test Case 2:\n",
            "Input: The cat sat on the\n",
            "Generated: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "--------------------------------------------------\n",
            "\n",
            "Test Case 3:\n",
            "Input: What is the capital of France?\n",
            "Generated: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "--------------------------------------------------\n",
            "\n",
            "Test Case 4:\n",
            "Input: How does photosynthesis work?\n",
            "Generated: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "--------------------------------------------------\n",
            "\n",
            "Test Case 5:\n",
            "Input: Climate change is causing\n",
            "Generated: <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "--------------------------------------------------\n",
            "\n",
            "Test Case 6:\n",
            "Input: Renewable energy sources include\n",
            "Generated: <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "--------------------------------------------------\n",
            "\n",
            "Test Case 7:\n",
            "Input: Lions are known for\n",
            "Generated: <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "--------------------------------------------------\n",
            "\n",
            "Test Case 8:\n",
            "Input: Endangered species need protection because\n",
            "Generated: <UNK> <UNK> <UNK> <UNK> <UNK> <UNK>\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m4yIrLZC7EPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from typing import List, Dict\n",
        "import time\n",
        "\n",
        "class TitanModelTester:\n",
        "    def __init__(self, model_handler: TitanHandler):\n",
        "        self.handler = model_handler\n",
        "\n",
        "    def generate_text(self, prompt: str, max_length: int = 100) -> Dict:\n",
        "        \"\"\"\n",
        "        Generate text from a single prompt and measure performance.\n",
        "        \"\"\"\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Prepare input\n",
        "        data = [{'body': prompt.encode('utf-8')}]\n",
        "\n",
        "        # Run through handler pipeline\n",
        "        processed = self.handler.preprocess(data)\n",
        "        output = self.handler.inference(processed)\n",
        "        response = self.handler.postprocess(output)\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "        return {\n",
        "            'prompt': prompt,\n",
        "            'generated_text': response[0]['generated_text'],\n",
        "            'generation_time': end_time - start_time\n",
        "        }\n",
        "\n",
        "    def run_test_suite(self, temperature: float = 1.0) -> Dict[str, List[Dict]]:\n",
        "        \"\"\"\n",
        "        Run a comprehensive suite of text generation tests.\n",
        "        \"\"\"\n",
        "        test_cases = {\n",
        "            'completion': [\n",
        "                \"Complete this sentence: The quick brown fox\",\n",
        "                \"Write the next line: Roses are red,\",\n",
        "                \"Finish this thought: If I could travel anywhere,\"\n",
        "            ],\n",
        "            'creative': [\n",
        "                \"Write a short story about a magical library\",\n",
        "                \"Create a poem about the ocean\",\n",
        "                \"Describe a futuristic city\"\n",
        "            ],\n",
        "            'technical': [\n",
        "                \"Explain how photosynthesis works\",\n",
        "                \"Write a Python function that calculates fibonacci numbers\",\n",
        "                \"Describe the process of machine learning\"\n",
        "            ],\n",
        "            'dialogue': [\n",
        "                \"Write a conversation between two friends discussing their weekend\",\n",
        "                \"Create a scene with dialogue between a teacher and student\",\n",
        "                \"Write a job interview dialogue\"\n",
        "            ]\n",
        "        }\n",
        "\n",
        "        results = {}\n",
        "\n",
        "        for category, prompts in test_cases.items():\n",
        "            category_results = []\n",
        "            for prompt in prompts:\n",
        "                try:\n",
        "                    result = self.generate_text(prompt)\n",
        "                    result['status'] = 'success'\n",
        "                except Exception as e:\n",
        "                    result = {\n",
        "                        'prompt': prompt,\n",
        "                        'error': str(e),\n",
        "                        'status': 'failed'\n",
        "                    }\n",
        "                category_results.append(result)\n",
        "            results[category] = category_results\n",
        "\n",
        "        return results\n",
        "\n",
        "def format_test_results(results: Dict[str, List[Dict]]) -> str:\n",
        "    \"\"\"\n",
        "    Format test results into a readable report.\n",
        "    \"\"\"\n",
        "    report = []\n",
        "\n",
        "    for category, tests in results.items():\n",
        "        report.append(f\"\\n=== {category.upper()} TESTS ===\\n\")\n",
        "\n",
        "        for i, test in enumerate(tests, 1):\n",
        "            report.append(f\"Test {i}:\")\n",
        "            report.append(f\"Prompt: {test['prompt']}\")\n",
        "\n",
        "            if test['status'] == 'success':\n",
        "                report.append(f\"Generated Text: {test['generated_text']}\")\n",
        "                report.append(f\"Generation Time: {test['generation_time']:.2f} seconds\")\n",
        "            else:\n",
        "                report.append(f\"Error: {test['error']}\")\n",
        "\n",
        "            report.append(\"\")  # Empty line between tests\n",
        "\n",
        "    return \"\\n\".join(report)\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    # Initialize the handler\n",
        "    handler = TitanHandler()\n",
        "    handler.initialize(\"write a poerty\")  # You'll need to provide appropriate context\n",
        "\n",
        "    # Create tester instance\n",
        "    tester = TitanModelTester(handler)\n",
        "\n",
        "    # Run test suite\n",
        "    results = tester.run_test_suite()\n",
        "\n",
        "    # Print formatted results\n",
        "    print(format_test_results(results))"
      ],
      "metadata": {
        "id": "jf2Ng1N-Tl8A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "e3f624c1-8ba5-499c-ad68-b624c93c26fc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'str' object has no attribute 'system_properties'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-3025ad94963f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;31m# Initialize the handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0mhandler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTitanHandler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m     \u001b[0mhandler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"write a poerty\"\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# You'll need to provide appropriate context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m     \u001b[0;31m# Create tester instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-9dd93b6c883c>\u001b[0m in \u001b[0;36minitialize\u001b[0;34m(self, context)\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minitialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0;34m\"\"\"Initialize model and tokenizer.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m         \u001b[0mproperties\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem_properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m         \u001b[0mmodel_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproperties\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model_dir'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'system_properties'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZ6vzrQne1ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O01rIcO1aRC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "\n",
        "\n",
        "def prepare_model_for_serving(model_path: str, save_dir: str, tokenizer: CustomTokenizer):\n",
        "    \"\"\"\n",
        "    Prepare model artifacts for TorchServe.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the saved model checkpoint.\n",
        "        save_dir (str): Directory to save the prepared model artifacts.\n",
        "        tokenizer (CustomTokenizer): An instance of the tokenizer to save configuration and vocabulary.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Load the model from the checkpoint\n",
        "    model = torch.load(model_path)\n",
        "\n",
        "    # Save the model in the specified directory\n",
        "    torch.save(model, os.path.join(save_dir, 'model.pt'))\n",
        "\n",
        "    # Prepare tokenizer configuration\n",
        "    tokenizer_config = {\n",
        "        'vocab_size': tokenizer.vocab_size,\n",
        "        'min_freq': tokenizer.min_freq,\n",
        "        'special_tokens': tokenizer.special_tokens,\n",
        "    }\n",
        "\n",
        "    # Save tokenizer configuration as JSON\n",
        "    with open(os.path.join(save_dir, 'tokenizer_config.json'), 'w') as f:\n",
        "        json.dump(tokenizer_config, f)\n",
        "\n",
        "    # Save tokenizer vocabulary\n",
        "    tokenizer.save_vocab(os.path.join(save_dir, 'vocab.txt'))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# Create an instance of CustomTokenizer\n",
        "tokenizer = CustomTokenizer(\n",
        "    vocab_size=50000,\n",
        "    min_freq=2,\n",
        "    special_tokens=['<pad>', '<eos>', '<bos>', '<unk>']\n",
        ")\n",
        "\n",
        "# Call the function with the correct parameters\n",
        "prepare_model_for_serving('checkpoints/best.pt', 'model_store', tokenizer)\n"
      ],
      "metadata": {
        "id": "s_TysmkMa_kj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91e5b477-5ae0-4d91-8a83-90ad64357390"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-22-06d5ff2de07d>:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model = torch.load(model_path)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oie7oZF5jkqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "handler_code = \"\"\"\n",
        "# TorchServe handler class\n",
        "class TitanHandler:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # def initialize(self, context):\n",
        "    #\n",
        "    #     properties = context.system_properties\n",
        "    #     model_dir = properties.get('model_dir')\n",
        "\n",
        "    #     # Load tokenizer\n",
        "    #     with open(os.path.join(model_dir, 'tokenizer_config.json'), 'r') as f:\n",
        "    #         tokenizer_config = json.load(f)\n",
        "    #     self.tokenizer = CustomTokenizer(**tokenizer_config)\n",
        "    #     self.tokenizer.load_vocab(os.path.join(model_dir, 'vocab.txt'))\n",
        "\n",
        "    #     # Load model\n",
        "    #     checkpoint = torch.load(\n",
        "    #         os.path.join(model_dir, 'model.pt'),\n",
        "    #         map_location=self.device\n",
        "    #     )\n",
        "\n",
        "    #     # Initialize model (you'll need to match the configuration used during training)\n",
        "    #     self.model = TitanModelWithCustomEmbedding(\n",
        "    #         embedding_layer=CustomEmbedding(\n",
        "    #             vocab_size=self.tokenizer.vocab_size_current,\n",
        "    #             embedding_dim=512  # Make sure this matches your training config\n",
        "    #         )\n",
        "    #     )\n",
        "    #     self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    #     self.model.to(self.device)\n",
        "    #     self.model.eval()\n",
        "    # def initialize(self, context):\n",
        "    #\n",
        "    #     properties = context.system_properties\n",
        "    #     model_dir = properties.get('model_dir')\n",
        "\n",
        "    #     # Load tokenizer\n",
        "    #     with open(os.path.join(model_dir, 'tokenizer_config.json'), 'r') as f:\n",
        "    #         tokenizer_config = json.load(f)\n",
        "    #     self.tokenizer = CustomTokenizer(**tokenizer_config)\n",
        "    #     self.tokenizer.load_vocab(os.path.join(model_dir, 'vocab.txt'))\n",
        "\n",
        "    #     # Load model\n",
        "    #     checkpoint = torch.load(\n",
        "    #         os.path.join(model_dir, 'model.pt'),\n",
        "    #         map_location=self.device\n",
        "    #     )\n",
        "\n",
        "    #     # Initialize model with the correct vocabulary size from the loaded tokenizer\n",
        "    #     self.model = TitanModelWithCustomEmbedding(\n",
        "    #         embedding_layer=CustomEmbedding(\n",
        "    #             vocab_size=self.tokenizer.vocab_size_current,  # Use tokenizer's vocab size\n",
        "    #             embedding_dim=512  # Make sure this matches your training config\n",
        "    #         ),\n",
        "    #         max_seq_length=1024,  # Add this to match your training config\n",
        "    #         depth=12           # Add this to match your training config\n",
        "    #     )\n",
        "    #     self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "    #     self.model.to(self.device)\n",
        "    #     self.model.eval()\n",
        "    def initialize(self, context):\n",
        "\n",
        "        properties = context.system_properties\n",
        "        model_dir = properties.get('model_dir')\n",
        "\n",
        "        # Load tokenizer FIRST\n",
        "        with open(os.path.join(model_dir, 'tokenizer_config.json'), 'r') as f:\n",
        "            tokenizer_config = json.load(f)\n",
        "        self.tokenizer = CustomTokenizer(**tokenizer_config)\n",
        "        self.tokenizer.load_vocab(os.path.join(model_dir, 'vocab.txt'))\n",
        "\n",
        "        # Print vocabulary size for debugging\n",
        "        print(f\"Loaded tokenizer vocabulary size: {self.tokenizer.vocab_size_current}\")\n",
        "\n",
        "        # Load checkpoint to check saved model configuration\n",
        "        checkpoint = torch.load(\n",
        "            os.path.join(model_dir, 'model.pt'),\n",
        "            map_location=self.device\n",
        "        )\n",
        "\n",
        "        # Print model state dict shapes for debugging\n",
        "        print(\"\\nModel checkpoint shapes:\")\n",
        "        for name, param in checkpoint['model_state_dict'].items():\n",
        "            print(f\"{name}: {param.shape}\")\n",
        "\n",
        "        # Initialize model with the EXACT same vocabulary size as in checkpoint\n",
        "        vocab_size = checkpoint['model_state_dict']['embedding.embedding.weight'].shape[0]\n",
        "        print(f\"\\nInitializing model with vocabulary size: {vocab_size}\")\n",
        "\n",
        "        self.model = TitanModelWithCustomEmbedding(\n",
        "            embedding_layer=CustomEmbedding(\n",
        "                vocab_size=vocab_size,  # Use vocabulary size from checkpoint\n",
        "                embedding_dim=512\n",
        "            ),\n",
        "            max_seq_length=1024,\n",
        "            depth=12\n",
        "        )\n",
        "\n",
        "        # Load state dict\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess(self, data):\n",
        "\n",
        "        text = data[0].get('body').decode('utf-8')\n",
        "\n",
        "        # Tokenize input\n",
        "        input_ids = torch.tensor(\n",
        "            [self.tokenizer.encode(text)],\n",
        "            dtype=torch.long\n",
        "        ).to(self.device)\n",
        "\n",
        "        attention_mask = (input_ids != self.tokenizer.pad_token_id).float()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "    def inference(self, data):\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                data['input_ids'],\n",
        "                data['attention_mask']\n",
        "            )\n",
        "        return outputs\n",
        "\n",
        "    def postprocess(self, inference_output):\n",
        "\n",
        "        # Get predicted tokens\n",
        "        predictions = inference_output.argmax(dim=-1)\n",
        "\n",
        "        # Decode tokens to text\n",
        "        response = self.tokenizer.decode(predictions[0].tolist())\n",
        "\n",
        "        return [{'generated_text': response}]\n",
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Save it to a file\n",
        "with open(\"titan_handler.py\", \"w\") as f:\n",
        "    f.write(handler_code)"
      ],
      "metadata": {
        "id": "-2yY_ASukBJX"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchserve torch-model-archiver\n"
      ],
      "metadata": {
        "id": "c2IvOhFuiEK-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "52f3ee7c-483e-4576-fc80-a8ff050bd5c3"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchserve\n",
            "  Downloading torchserve-0.12.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting torch-model-archiver\n",
            "  Downloading torch_model_archiver-0.12.0-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from torchserve) (11.1.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from torchserve) (5.9.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from torchserve) (24.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from torchserve) (0.45.1)\n",
            "Collecting enum-compat (from torch-model-archiver)\n",
            "  Downloading enum_compat-0.0.3-py3-none-any.whl.metadata (954 bytes)\n",
            "Downloading torchserve-0.12.0-py3-none-any.whl (42.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 MB\u001b[0m \u001b[31m18.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torch_model_archiver-0.12.0-py3-none-any.whl (16 kB)\n",
            "Downloading enum_compat-0.0.3-py3-none-any.whl (1.3 kB)\n",
            "Installing collected packages: enum-compat, torchserve, torch-model-archiver\n",
            "Successfully installed enum-compat-0.0.3 torch-model-archiver-0.12.0 torchserve-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!torch-model-archiver --model-name titan \\\n",
        "                      --version 1.0 \\\n",
        "                      --model-file model_store/model.pt \\\n",
        "                      --handler titan_handler.py \\\n",
        "                      --extra-files \"model_store/tokenizer_config.json,model_store/vocab.txt\" \\\n",
        "                      --export-path model_store \\\n",
        "                      --force\n"
      ],
      "metadata": {
        "id": "ZJh9VOBXhvzt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba6569f1-4f53-4305-a44f-43cc2e469101"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING - Overwriting model_store/titan.mar ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8080\n"
      ],
      "metadata": {
        "id": "4TxdviC1mZpX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c980d3b2-9b65-4a0a-8433-11dd4a58a6ca"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMMAND PID USER   FD   TYPE DEVICE SIZE/OFF NODE NAME\n",
            "node      7 root   21u  IPv6  21340      0t0  TCP *:8080 (LISTEN)\n",
            "node      7 root   26u  IPv6 290112      0t0  TCP 049e73524b18:8080->172.28.0.1:49012 (ESTABLISHED)\n",
            "node      7 root   28u  IPv6 308839      0t0  TCP 049e73524b18:8080->172.28.0.1:47122 (ESTABLISHED)\n",
            "node      7 root   29u  IPv6 275399      0t0  TCP 049e73524b18:8080->172.28.0.1:34330 (ESTABLISHED)\n",
            "node      7 root   31u  IPv6  41861      0t0  TCP 049e73524b18:8080->172.28.0.1:43008 (ESTABLISHED)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a config.properties file\n",
        "%%writefile config.properties\n",
        "inference_address=http://127.0.0.1:8083\n",
        "management_address=http://127.0.0.1:8084\n",
        "metrics_address=http://127.0.0.1:8085\n",
        "model_store=model_store\n",
        "load_models=titan=titan.mar"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFI8F_swvjgz",
        "outputId": "d69683c4-6a98-484a-d348-0a38f27ee6ee"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing config.properties\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Dn_4C5eM0-FL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Start with minimal configuration\n",
        "!torchserve --start --model-store model_store --models titan=titan.mar --disable-token-auth"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICgb5O7C0ORB",
        "outputId": "47c514d9-f3f3-49f3-d1e2-c8a738f18010"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Removing orphan pid file.\n",
            "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
            "2025-01-20T18:07:55,019 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
            "2025-01-20T18:07:55,170 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program \"xpu-smi\": error=2, No such file or directory\n",
            "2025-01-20T18:07:55,172 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\n",
            "2025-01-20T18:07:55,242 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
            "2025-01-20T18:07:55,421 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml\n",
            "2025-01-20T18:07:55,545 [INFO ] main org.pytorch.serve.ModelServer - \n",
            "Torchserve version: 0.12.0\n",
            "TS Home: /usr/local/lib/python3.11/dist-packages\n",
            "Current directory: /content\n",
            "Temp directory: /tmp\n",
            "Metrics config path: /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml\n",
            "Number of GPUs: 1\n",
            "Number of CPUs: 2\n",
            "Max heap size: 3246 M\n",
            "Python executable: /usr/bin/python3\n",
            "Config file: logs/config/20250120175606986-shutdown.cfg\n",
            "Inference address: http://127.0.0.1:8083\n",
            "Management address: http://127.0.0.1:8084\n",
            "Metrics address: http://127.0.0.1:8085\n",
            "Model Store: /content/model_store\n",
            "Initial Models: titan=titan.mar\n",
            "Log dir: /content/logs\n",
            "Metrics dir: /content/logs\n",
            "Netty threads: 0\n",
            "Netty client threads: 0\n",
            "Default workers per model: 1\n",
            "Blacklist Regex: N/A\n",
            "Maximum Response Size: 6553500\n",
            "Maximum Request Size: 6553500\n",
            "Limit Maximum Image Pixels: true\n",
            "Prefer direct buffer: false\n",
            "Allowed Urls: [file://.*|http(s)?://.*]\n",
            "Custom python dependency for model allowed: false\n",
            "Enable metrics API: true\n",
            "Metrics mode: LOG\n",
            "Disable system metrics: false\n",
            "Workflow Store: /content/model_store\n",
            "CPP log config: N/A\n",
            "Model config: N/A\n",
            "System metrics command: default\n",
            "Model API enabled: false\n",
            "2025-01-20T18:07:55,574 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Started restoring models from snapshot {\n",
            "  \"name\": \"20250120175606986-shutdown.cfg\",\n",
            "  \"modelCount\": 1,\n",
            "  \"created\": 1737395766986,\n",
            "  \"models\": {\n",
            "    \"titan\": {\n",
            "      \"1.0\": {\n",
            "        \"defaultVersion\": true,\n",
            "        \"marName\": \"titan.mar\",\n",
            "        \"minWorkers\": 1,\n",
            "        \"maxWorkers\": 1,\n",
            "        \"batchSize\": 1,\n",
            "        \"maxBatchDelay\": 100,\n",
            "        \"responseTimeout\": 120,\n",
            "        \"startupTimeout\": 120,\n",
            "        \"runtimeType\": \"python\"\n",
            "      }\n",
            "    }\n",
            "  }\n",
            "}\n",
            "2025-01-20T18:07:55,600 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Validating snapshot 20250120175606986-shutdown.cfg\n",
            "2025-01-20T18:07:55,605 [INFO ] main org.pytorch.serve.snapshot.SnapshotManager - Snapshot 20250120175606986-shutdown.cfg validated successfully\n",
            "2025-01-20T18:08:29,088 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model titan\n",
            "2025-01-20T18:08:29,090 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model titan\n",
            "2025-01-20T18:08:29,091 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model titan\n",
            "2025-01-20T18:08:29,092 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model titan loaded.\n",
            "2025-01-20T18:08:29,093 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: titan, count: 1\n",
            "2025-01-20T18:08:29,107 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
            "2025-01-20T18:08:29,123 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml]\n",
            "2025-01-20T18:08:29,328 [INFO ] main org.pytorch.serve.ModelServer - Inference API bind to: http://127.0.0.1:8083\n",
            "2025-01-20T18:08:29,329 [INFO ] main org.pytorch.serve.ModelServer - Initialize Management server with: EpollServerSocketChannel.\n",
            "2025-01-20T18:08:29,336 [INFO ] main org.pytorch.serve.ModelServer - Management API bind to: http://127.0.0.1:8084\n",
            "2025-01-20T18:08:29,336 [INFO ] main org.pytorch.serve.ModelServer - Initialize Metrics server with: EpollServerSocketChannel.\n",
            "2025-01-20T18:08:29,338 [INFO ] main org.pytorch.serve.ModelServer - Metrics API bind to: http://127.0.0.1:8085\n",
            "Model server started.\n",
            "2025-01-20T18:08:30,156 [WARN ] pool-3-thread-1 org.pytorch.serve.metrics.MetricCollector - worker pid is not available yet.\n",
            "2025-01-20T18:08:30,219 [ERROR] Thread-1 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ts/metrics/metric_collector.py\", line 11, in <module>\n",
            "    from ts.metrics import system_metrics\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ts/__init__.py\", line 10, in <module>\n",
            "    from . import version\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ts/version.py\", line 5, in <module>\n",
            "    from pathlib import Path\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pathlib.py\", line 10, in <module>\n",
            "    from collections import Sequence\n",
            "ImportError: cannot import name 'Sequence' from 'collections' (/usr/lib/python3.11/collections/__init__.py)\n",
            "\n",
            "2025-01-20T18:08:31,851 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=12171\n",
            "2025-01-20T18:08:31,854 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:08:31,860 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml.\n",
            "2025-01-20T18:08:31,860 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - [PID]12171\n",
            "2025-01-20T18:08:31,860 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Torch worker started.\n",
            "2025-01-20T18:08:31,861 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Python runtime: 3.11.11\n",
            "2025-01-20T18:08:31,861 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change null -> WORKER_STARTED\n",
            "2025-01-20T18:08:31,868 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:08:31,877 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
            "2025-01-20T18:08:31,883 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1737396511883\n",
            "2025-01-20T18:08:31,887 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1737396511887\n",
            "2025-01-20T18:08:31,929 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - model_name: titan, batchSize: 1\n",
            "2025-01-20T18:08:31,934 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Backend worker process died.\n",
            "2025-01-20T18:08:31,935 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Traceback (most recent call last):\n",
            "2025-01-20T18:08:31,935 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 301, in <module>\n",
            "2025-01-20T18:08:31,935 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     worker.run_server()\n",
            "2025-01-20T18:08:31,935 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 268, in run_server\n",
            "2025-01-20T18:08:31,936 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n",
            "2025-01-20T18:08:31,936 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 196, in handle_connection\n",
            "2025-01-20T18:08:31,936 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n",
            "2025-01-20T18:08:31,936 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:31,937 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 133, in load_model\n",
            "2025-01-20T18:08:31,937 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service = model_loader.load(\n",
            "2025-01-20T18:08:31,937 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:31,942 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 125, in load\n",
            "2025-01-20T18:08:31,943 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     entry_point, initialize_fn = self._get_class_entry_point(module)\n",
            "2025-01-20T18:08:31,944 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:31,944 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 178, in _get_class_entry_point\n",
            "2025-01-20T18:08:31,946 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     raise ValueError(\n",
            "2025-01-20T18:08:31,947 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - ValueError: Expected only one class in custom service code or a function entry point [<class 'titan_handler.CustomEmbedding'>, <class 'titan_handler.CustomTokenizer'>, <class 'titan_handler.TextDataset'>, <class 'titan_handler.TitanHandler'>, <class 'titan_handler.TitanModelWithCustomEmbedding'>]\n",
            "2025-01-20T18:08:31,949 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\n",
            "2025-01-20T18:08:31,951 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED\n",
            "2025-01-20T18:08:31,951 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec\n",
            "java.lang.InterruptedException: null\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]\n",
            "\tat java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]\n",
            "\tat org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) [model-server.jar:?]\n",
            "\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:515) [?:?]\n",
            "\tat java.util.concurrent.FutureTask.run(FutureTask.java:264) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
            "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
            "2025-01-20T18:08:31,984 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: titan, error: Worker died.\n",
            "2025-01-20T18:08:31,984 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STARTED -> WORKER_STOPPED\n",
            "2025-01-20T18:08:31,985 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery start timestamp: 1737396511985\n",
            "2025-01-20T18:08:31,987 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\n",
            "2025-01-20T18:08:32,012 [INFO ] W-9000-titan_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stderr\n",
            "2025-01-20T18:08:32,015 [INFO ] W-9000-titan_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stdout\n",
            "2025-01-20T18:08:32,988 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml]\n",
            "2025-01-20T18:08:34,550 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=12197\n",
            "2025-01-20T18:08:34,554 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:08:34,559 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml.\n",
            "2025-01-20T18:08:34,560 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - [PID]12197\n",
            "2025-01-20T18:08:34,560 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STOPPED -> WORKER_STARTED\n",
            "2025-01-20T18:08:34,561 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:08:34,561 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Torch worker started.\n",
            "2025-01-20T18:08:34,562 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Python runtime: 3.11.11\n",
            "2025-01-20T18:08:34,565 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
            "2025-01-20T18:08:34,567 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1737396514567\n",
            "2025-01-20T18:08:34,568 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1737396514568\n",
            "2025-01-20T18:08:34,593 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - model_name: titan, batchSize: 1\n",
            "2025-01-20T18:08:34,594 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Backend worker process died.\n",
            "2025-01-20T18:08:34,595 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Traceback (most recent call last):\n",
            "2025-01-20T18:08:34,596 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 301, in <module>\n",
            "2025-01-20T18:08:34,596 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     worker.run_server()\n",
            "2025-01-20T18:08:34,596 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 268, in run_server\n",
            "2025-01-20T18:08:34,596 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n",
            "2025-01-20T18:08:34,596 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 196, in handle_connection\n",
            "2025-01-20T18:08:34,598 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n",
            "2025-01-20T18:08:34,598 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:34,598 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 133, in load_model\n",
            "2025-01-20T18:08:34,599 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service = model_loader.load(\n",
            "2025-01-20T18:08:34,599 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:34,599 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 125, in load\n",
            "2025-01-20T18:08:34,599 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     entry_point, initialize_fn = self._get_class_entry_point(module)\n",
            "2025-01-20T18:08:34,599 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:34,606 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 178, in _get_class_entry_point\n",
            "2025-01-20T18:08:34,606 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     raise ValueError(\n",
            "2025-01-20T18:08:34,607 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - ValueError: Expected only one class in custom service code or a function entry point [<class 'titan_handler.CustomEmbedding'>, <class 'titan_handler.CustomTokenizer'>, <class 'titan_handler.TextDataset'>, <class 'titan_handler.TitanHandler'>, <class 'titan_handler.TitanModelWithCustomEmbedding'>]\n",
            "2025-01-20T18:08:34,609 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\n",
            "2025-01-20T18:08:34,614 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED\n",
            "2025-01-20T18:08:34,614 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec\n",
            "java.lang.InterruptedException: null\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]\n",
            "\tat java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]\n",
            "\tat org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) [model-server.jar:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
            "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
            "2025-01-20T18:08:34,614 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: titan, error: Worker died.\n",
            "2025-01-20T18:08:34,615 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STARTED -> WORKER_STOPPED\n",
            "2025-01-20T18:08:34,615 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\n",
            "2025-01-20T18:08:34,615 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 1 seconds.\n",
            "2025-01-20T18:08:34,641 [INFO ] W-9000-titan_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stdout\n",
            "2025-01-20T18:08:34,641 [INFO ] W-9000-titan_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stderr\n",
            "2025-01-20T18:08:35,616 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml]\n",
            "2025-01-20T18:08:37,596 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=12217\n",
            "2025-01-20T18:08:37,607 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:08:37,621 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml.\n",
            "2025-01-20T18:08:37,622 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - [PID]12217\n",
            "2025-01-20T18:08:37,623 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Torch worker started.\n",
            "2025-01-20T18:08:37,623 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STOPPED -> WORKER_STARTED\n",
            "2025-01-20T18:08:37,625 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:08:37,628 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Python runtime: 3.11.11\n",
            "2025-01-20T18:08:37,633 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1737396517633\n",
            "2025-01-20T18:08:37,634 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1737396517633\n",
            "2025-01-20T18:08:37,639 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
            "2025-01-20T18:08:37,669 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - model_name: titan, batchSize: 1\n",
            "2025-01-20T18:08:37,673 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Backend worker process died.\n",
            "2025-01-20T18:08:37,673 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Traceback (most recent call last):\n",
            "2025-01-20T18:08:37,674 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 301, in <module>\n",
            "2025-01-20T18:08:37,675 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     worker.run_server()\n",
            "2025-01-20T18:08:37,676 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 268, in run_server\n",
            "2025-01-20T18:08:37,676 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n",
            "2025-01-20T18:08:37,680 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 196, in handle_connection\n",
            "2025-01-20T18:08:37,681 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n",
            "2025-01-20T18:08:37,682 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:37,683 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 133, in load_model\n",
            "2025-01-20T18:08:37,683 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service = model_loader.load(\n",
            "2025-01-20T18:08:37,684 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:37,688 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 125, in load\n",
            "2025-01-20T18:08:37,689 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     entry_point, initialize_fn = self._get_class_entry_point(module)\n",
            "2025-01-20T18:08:37,690 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:37,691 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 178, in _get_class_entry_point\n",
            "2025-01-20T18:08:37,692 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     raise ValueError(\n",
            "2025-01-20T18:08:37,692 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - ValueError: Expected only one class in custom service code or a function entry point [<class 'titan_handler.CustomEmbedding'>, <class 'titan_handler.CustomTokenizer'>, <class 'titan_handler.TextDataset'>, <class 'titan_handler.TitanHandler'>, <class 'titan_handler.TitanModelWithCustomEmbedding'>]\n",
            "2025-01-20T18:08:37,697 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\n",
            "2025-01-20T18:08:37,698 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED\n",
            "2025-01-20T18:08:37,698 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec\n",
            "java.lang.InterruptedException: null\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]\n",
            "\tat java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]\n",
            "\tat org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) [model-server.jar:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
            "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
            "2025-01-20T18:08:37,707 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: titan, error: Worker died.\n",
            "2025-01-20T18:08:37,707 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STARTED -> WORKER_STOPPED\n",
            "2025-01-20T18:08:37,707 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\n",
            "2025-01-20T18:08:37,708 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 2 seconds.\n",
            "2025-01-20T18:08:37,752 [INFO ] W-9000-titan_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stderr\n",
            "2025-01-20T18:08:37,753 [INFO ] W-9000-titan_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stdout\n",
            "2025-01-20T18:08:39,708 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml]\n",
            "2025-01-20T18:08:41,278 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=12239\n",
            "2025-01-20T18:08:41,279 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:08:41,286 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml.\n",
            "2025-01-20T18:08:41,286 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - [PID]12239\n",
            "2025-01-20T18:08:41,287 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STOPPED -> WORKER_STARTED\n",
            "2025-01-20T18:08:41,287 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:08:41,287 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Torch worker started.\n",
            "2025-01-20T18:08:41,288 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Python runtime: 3.11.11\n",
            "2025-01-20T18:08:41,291 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
            "2025-01-20T18:08:41,292 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1737396521292\n",
            "2025-01-20T18:08:41,292 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1737396521292\n",
            "2025-01-20T18:08:41,317 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - model_name: titan, batchSize: 1\n",
            "2025-01-20T18:08:41,320 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Backend worker process died.\n",
            "2025-01-20T18:08:41,320 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Traceback (most recent call last):\n",
            "2025-01-20T18:08:41,320 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 301, in <module>\n",
            "2025-01-20T18:08:41,320 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     worker.run_server()\n",
            "2025-01-20T18:08:41,320 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 268, in run_server\n",
            "2025-01-20T18:08:41,321 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n",
            "2025-01-20T18:08:41,321 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 196, in handle_connection\n",
            "2025-01-20T18:08:41,321 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n",
            "2025-01-20T18:08:41,321 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:41,321 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 133, in load_model\n",
            "2025-01-20T18:08:41,322 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service = model_loader.load(\n",
            "2025-01-20T18:08:41,322 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:41,322 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 125, in load\n",
            "2025-01-20T18:08:41,322 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     entry_point, initialize_fn = self._get_class_entry_point(module)\n",
            "2025-01-20T18:08:41,322 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:41,322 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 178, in _get_class_entry_point\n",
            "2025-01-20T18:08:41,329 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\n",
            "2025-01-20T18:08:41,331 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED\n",
            "2025-01-20T18:08:41,331 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec\n",
            "java.lang.InterruptedException: null\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]\n",
            "\tat java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]\n",
            "\tat org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) [model-server.jar:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
            "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
            "2025-01-20T18:08:41,332 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: titan, error: Worker died.\n",
            "2025-01-20T18:08:41,332 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STARTED -> WORKER_STOPPED\n",
            "2025-01-20T18:08:41,332 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\n",
            "2025-01-20T18:08:41,333 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 3 seconds.\n",
            "2025-01-20T18:08:41,331 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     raise ValueError(\n",
            "2025-01-20T18:08:41,334 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - ValueError: Expected only one class in custom service code or a function entry point [<class 'titan_handler.CustomEmbedding'>, <class 'titan_handler.CustomTokenizer'>, <class 'titan_handler.TextDataset'>, <class 'titan_handler.TitanHandler'>, <class 'titan_handler.TitanModelWithCustomEmbedding'>]\n",
            "2025-01-20T18:08:41,335 [INFO ] W-9000-titan_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stdout\n",
            "2025-01-20T18:08:41,356 [INFO ] W-9000-titan_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stderr\n",
            "2025-01-20T18:08:44,333 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml]\n",
            "2025-01-20T18:08:45,891 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=12267\n",
            "2025-01-20T18:08:45,895 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:08:45,899 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml.\n",
            "2025-01-20T18:08:45,900 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - [PID]12267\n",
            "2025-01-20T18:08:45,900 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STOPPED -> WORKER_STARTED\n",
            "2025-01-20T18:08:45,900 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:08:45,901 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Torch worker started.\n",
            "2025-01-20T18:08:45,901 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Python runtime: 3.11.11\n",
            "2025-01-20T18:08:45,902 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1737396525902\n",
            "2025-01-20T18:08:45,903 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
            "2025-01-20T18:08:45,903 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1737396525903\n",
            "2025-01-20T18:08:45,905 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - model_name: titan, batchSize: 1\n",
            "2025-01-20T18:08:45,907 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Backend worker process died.\n",
            "2025-01-20T18:08:45,907 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Traceback (most recent call last):\n",
            "2025-01-20T18:08:45,907 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 301, in <module>\n",
            "2025-01-20T18:08:45,907 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     worker.run_server()\n",
            "2025-01-20T18:08:45,908 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 268, in run_server\n",
            "2025-01-20T18:08:45,908 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n",
            "2025-01-20T18:08:45,908 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 196, in handle_connection\n",
            "2025-01-20T18:08:45,908 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n",
            "2025-01-20T18:08:45,908 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:45,908 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 133, in load_model\n",
            "2025-01-20T18:08:45,908 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service = model_loader.load(\n",
            "2025-01-20T18:08:45,909 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:45,909 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 125, in load\n",
            "2025-01-20T18:08:45,909 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     entry_point, initialize_fn = self._get_class_entry_point(module)\n",
            "2025-01-20T18:08:45,909 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\n",
            "2025-01-20T18:08:45,911 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED\n",
            "2025-01-20T18:08:45,911 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec\n",
            "java.lang.InterruptedException: null\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]\n",
            "\tat java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]\n",
            "\tat org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) [model-server.jar:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
            "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
            "2025-01-20T18:08:45,912 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: titan, error: Worker died.\n",
            "2025-01-20T18:08:45,912 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STARTED -> WORKER_STOPPED\n",
            "2025-01-20T18:08:45,912 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\n",
            "2025-01-20T18:08:45,912 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 5 seconds.\n",
            "2025-01-20T18:08:45,913 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:45,913 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 178, in _get_class_entry_point\n",
            "2025-01-20T18:08:45,913 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     raise ValueError(\n",
            "2025-01-20T18:08:45,913 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - ValueError: Expected only one class in custom service code or a function entry point [<class 'titan_handler.CustomEmbedding'>, <class 'titan_handler.CustomTokenizer'>, <class 'titan_handler.TextDataset'>, <class 'titan_handler.TitanHandler'>, <class 'titan_handler.TitanModelWithCustomEmbedding'>]\n",
            "2025-01-20T18:08:45,914 [INFO ] W-9000-titan_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stdout\n",
            "2025-01-20T18:08:45,960 [INFO ] W-9000-titan_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stderr\n",
            "2025-01-20T18:08:50,913 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml]\n",
            "2025-01-20T18:08:52,625 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=12298\n",
            "2025-01-20T18:08:52,627 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:08:52,635 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml.\n",
            "2025-01-20T18:08:52,635 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - [PID]12298\n",
            "2025-01-20T18:08:52,636 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STOPPED -> WORKER_STARTED\n",
            "2025-01-20T18:08:52,636 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:08:52,637 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Torch worker started.\n",
            "2025-01-20T18:08:52,637 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Python runtime: 3.11.11\n",
            "2025-01-20T18:08:52,639 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
            "2025-01-20T18:08:52,639 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1737396532639\n",
            "2025-01-20T18:08:52,640 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1737396532639\n",
            "2025-01-20T18:08:52,643 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - model_name: titan, batchSize: 1\n",
            "2025-01-20T18:08:52,645 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Backend worker process died.\n",
            "2025-01-20T18:08:52,646 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Traceback (most recent call last):\n",
            "2025-01-20T18:08:52,646 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\n",
            "2025-01-20T18:08:52,647 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED\n",
            "2025-01-20T18:08:52,647 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec\n",
            "java.lang.InterruptedException: null\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]\n",
            "\tat java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]\n",
            "\tat org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) [model-server.jar:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
            "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
            "2025-01-20T18:08:52,648 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: titan, error: Worker died.\n",
            "2025-01-20T18:08:52,649 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STARTED -> WORKER_STOPPED\n",
            "2025-01-20T18:08:52,650 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\n",
            "2025-01-20T18:08:52,650 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 8 seconds.\n",
            "2025-01-20T18:08:52,651 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 301, in <module>\n",
            "2025-01-20T18:08:52,651 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     worker.run_server()\n",
            "2025-01-20T18:08:52,652 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 268, in run_server\n",
            "2025-01-20T18:08:52,653 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n",
            "2025-01-20T18:08:52,653 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 196, in handle_connection\n",
            "2025-01-20T18:08:52,653 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n",
            "2025-01-20T18:08:52,653 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:52,653 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 133, in load_model\n",
            "2025-01-20T18:08:52,653 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service = model_loader.load(\n",
            "2025-01-20T18:08:52,654 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:52,654 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 125, in load\n",
            "2025-01-20T18:08:52,654 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     entry_point, initialize_fn = self._get_class_entry_point(module)\n",
            "2025-01-20T18:08:52,654 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:08:52,654 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 178, in _get_class_entry_point\n",
            "2025-01-20T18:08:52,655 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     raise ValueError(\n",
            "2025-01-20T18:08:52,655 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - ValueError: Expected only one class in custom service code or a function entry point [<class 'titan_handler.CustomEmbedding'>, <class 'titan_handler.CustomTokenizer'>, <class 'titan_handler.TextDataset'>, <class 'titan_handler.TitanHandler'>, <class 'titan_handler.TitanModelWithCustomEmbedding'>]\n",
            "2025-01-20T18:08:52,655 [INFO ] W-9000-titan_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stdout\n",
            "2025-01-20T18:08:52,677 [INFO ] W-9000-titan_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stderr\n",
            "2025-01-20T18:09:00,651 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml]\n",
            "2025-01-20T18:09:02,675 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=12345\n",
            "2025-01-20T18:09:02,677 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:09:02,696 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml.\n",
            "2025-01-20T18:09:02,696 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - [PID]12345\n",
            "2025-01-20T18:09:02,697 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Torch worker started.\n",
            "2025-01-20T18:09:02,697 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STOPPED -> WORKER_STARTED\n",
            "2025-01-20T18:09:02,698 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:09:02,700 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1737396542700\n",
            "2025-01-20T18:09:02,701 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Python runtime: 3.11.11\n",
            "2025-01-20T18:09:02,705 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1737396542705\n",
            "2025-01-20T18:09:02,706 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
            "2025-01-20T18:09:02,707 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - model_name: titan, batchSize: 1\n",
            "2025-01-20T18:09:02,709 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Backend worker process died.\n",
            "2025-01-20T18:09:02,709 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Traceback (most recent call last):\n",
            "2025-01-20T18:09:02,709 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 301, in <module>\n",
            "2025-01-20T18:09:02,709 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     worker.run_server()\n",
            "2025-01-20T18:09:02,710 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 268, in run_server\n",
            "2025-01-20T18:09:02,710 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n",
            "2025-01-20T18:09:02,710 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 196, in handle_connection\n",
            "2025-01-20T18:09:02,711 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n",
            "2025-01-20T18:09:02,711 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:09:02,711 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 133, in load_model\n",
            "2025-01-20T18:09:02,712 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service = model_loader.load(\n",
            "2025-01-20T18:09:02,712 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:09:02,712 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 125, in load\n",
            "2025-01-20T18:09:02,712 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     entry_point, initialize_fn = self._get_class_entry_point(module)\n",
            "2025-01-20T18:09:02,712 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:09:02,712 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 178, in _get_class_entry_point\n",
            "2025-01-20T18:09:02,712 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     raise ValueError(\n",
            "2025-01-20T18:09:02,712 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - ValueError: Expected only one class in custom service code or a function entry point [<class 'titan_handler.CustomEmbedding'>, <class 'titan_handler.CustomTokenizer'>, <class 'titan_handler.TextDataset'>, <class 'titan_handler.TitanHandler'>, <class 'titan_handler.TitanModelWithCustomEmbedding'>]\n",
            "2025-01-20T18:09:02,713 [INFO ] epollEventLoopGroup-5-3 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\n",
            "2025-01-20T18:09:02,713 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED\n",
            "2025-01-20T18:09:02,713 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec\n",
            "java.lang.InterruptedException: null\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]\n",
            "\tat java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]\n",
            "\tat org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) [model-server.jar:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
            "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
            "2025-01-20T18:09:02,714 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: titan, error: Worker died.\n",
            "2025-01-20T18:09:02,714 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STARTED -> WORKER_STOPPED\n",
            "2025-01-20T18:09:02,714 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\n",
            "2025-01-20T18:09:02,714 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 13 seconds.\n",
            "2025-01-20T18:09:02,759 [INFO ] W-9000-titan_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stderr\n",
            "2025-01-20T18:09:02,760 [INFO ] W-9000-titan_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stdout\n",
            "2025-01-20T18:09:15,715 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml]\n",
            "2025-01-20T18:09:17,832 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=12416\n",
            "2025-01-20T18:09:17,833 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:09:17,840 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml.\n",
            "2025-01-20T18:09:17,841 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - [PID]12416\n",
            "2025-01-20T18:09:17,841 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Torch worker started.\n",
            "2025-01-20T18:09:17,842 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Python runtime: 3.11.11\n",
            "2025-01-20T18:09:17,841 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STOPPED -> WORKER_STARTED\n",
            "2025-01-20T18:09:17,842 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:09:17,843 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
            "2025-01-20T18:09:17,843 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1737396557843\n",
            "2025-01-20T18:09:17,844 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1737396557844\n",
            "2025-01-20T18:09:17,845 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - model_name: titan, batchSize: 1\n",
            "2025-01-20T18:09:17,847 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Backend worker process died.\n",
            "2025-01-20T18:09:17,847 [INFO ] epollEventLoopGroup-5-4 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\n",
            "2025-01-20T18:09:17,848 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED\n",
            "2025-01-20T18:09:17,848 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec\n",
            "java.lang.InterruptedException: null\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]\n",
            "\tat java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]\n",
            "\tat org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) [model-server.jar:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
            "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
            "2025-01-20T18:09:17,848 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: titan, error: Worker died.\n",
            "2025-01-20T18:09:17,849 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STARTED -> WORKER_STOPPED\n",
            "2025-01-20T18:09:17,849 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\n",
            "2025-01-20T18:09:17,849 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 21 seconds.\n",
            "2025-01-20T18:09:17,853 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Traceback (most recent call last):\n",
            "2025-01-20T18:09:17,853 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 301, in <module>\n",
            "2025-01-20T18:09:17,853 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     worker.run_server()\n",
            "2025-01-20T18:09:17,854 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 268, in run_server\n",
            "2025-01-20T18:09:17,854 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n",
            "2025-01-20T18:09:17,854 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 196, in handle_connection\n",
            "2025-01-20T18:09:17,855 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n",
            "2025-01-20T18:09:17,855 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:09:17,856 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 133, in load_model\n",
            "2025-01-20T18:09:17,856 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service = model_loader.load(\n",
            "2025-01-20T18:09:17,856 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:09:17,856 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 125, in load\n",
            "2025-01-20T18:09:17,856 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     entry_point, initialize_fn = self._get_class_entry_point(module)\n",
            "2025-01-20T18:09:17,856 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:09:17,856 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 178, in _get_class_entry_point\n",
            "2025-01-20T18:09:17,856 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     raise ValueError(\n",
            "2025-01-20T18:09:17,856 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - ValueError: Expected only one class in custom service code or a function entry point [<class 'titan_handler.CustomEmbedding'>, <class 'titan_handler.CustomTokenizer'>, <class 'titan_handler.TextDataset'>, <class 'titan_handler.TitanHandler'>, <class 'titan_handler.TitanModelWithCustomEmbedding'>]\n",
            "2025-01-20T18:09:17,857 [INFO ] W-9000-titan_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stdout\n",
            "2025-01-20T18:09:17,884 [INFO ] W-9000-titan_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stderr\n",
            "2025-01-20T18:09:30,184 [ERROR] Thread-10 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ts/metrics/metric_collector.py\", line 11, in <module>\n",
            "    from ts.metrics import system_metrics\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ts/__init__.py\", line 10, in <module>\n",
            "    from . import version\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ts/version.py\", line 5, in <module>\n",
            "    from pathlib import Path\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pathlib.py\", line 10, in <module>\n",
            "    from collections import Sequence\n",
            "ImportError: cannot import name 'Sequence' from 'collections' (/usr/lib/python3.11/collections/__init__.py)\n",
            "\n",
            "2025-01-20T18:09:38,850 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml]\n",
            "2025-01-20T18:09:40,971 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=12521\n",
            "2025-01-20T18:09:40,972 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:09:40,988 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml.\n",
            "2025-01-20T18:09:40,988 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - [PID]12521\n",
            "2025-01-20T18:09:40,989 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Torch worker started.\n",
            "2025-01-20T18:09:40,989 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Python runtime: 3.11.11\n",
            "2025-01-20T18:09:40,989 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STOPPED -> WORKER_STARTED\n",
            "2025-01-20T18:09:40,989 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:09:40,990 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1737396580990\n",
            "2025-01-20T18:09:40,991 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
            "2025-01-20T18:09:40,991 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1737396580991\n",
            "2025-01-20T18:09:40,992 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - model_name: titan, batchSize: 1\n",
            "2025-01-20T18:09:40,994 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Backend worker process died.\n",
            "2025-01-20T18:09:40,995 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Traceback (most recent call last):\n",
            "2025-01-20T18:09:40,995 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 301, in <module>\n",
            "2025-01-20T18:09:40,995 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     worker.run_server()\n",
            "2025-01-20T18:09:40,995 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 268, in run_server\n",
            "2025-01-20T18:09:40,995 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n",
            "2025-01-20T18:09:40,995 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 196, in handle_connection\n",
            "2025-01-20T18:09:40,995 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n",
            "2025-01-20T18:09:40,995 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:09:40,995 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 133, in load_model\n",
            "2025-01-20T18:09:40,995 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service = model_loader.load(\n",
            "2025-01-20T18:09:40,995 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:09:40,996 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 125, in load\n",
            "2025-01-20T18:09:40,996 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     entry_point, initialize_fn = self._get_class_entry_point(module)\n",
            "2025-01-20T18:09:40,996 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:09:40,996 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 178, in _get_class_entry_point\n",
            "2025-01-20T18:09:40,996 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     raise ValueError(\n",
            "2025-01-20T18:09:40,996 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - ValueError: Expected only one class in custom service code or a function entry point [<class 'titan_handler.CustomEmbedding'>, <class 'titan_handler.CustomTokenizer'>, <class 'titan_handler.TextDataset'>, <class 'titan_handler.TitanHandler'>, <class 'titan_handler.TitanModelWithCustomEmbedding'>]\n",
            "2025-01-20T18:09:41,004 [INFO ] epollEventLoopGroup-5-1 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\n",
            "2025-01-20T18:09:41,004 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED\n",
            "2025-01-20T18:09:41,004 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec\n",
            "java.lang.InterruptedException: null\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]\n",
            "\tat java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]\n",
            "\tat org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) [model-server.jar:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
            "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
            "2025-01-20T18:09:41,005 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: titan, error: Worker died.\n",
            "2025-01-20T18:09:41,005 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STARTED -> WORKER_STOPPED\n",
            "2025-01-20T18:09:41,005 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\n",
            "2025-01-20T18:09:41,005 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 34 seconds.\n",
            "2025-01-20T18:09:41,046 [INFO ] W-9000-titan_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stderr\n",
            "2025-01-20T18:09:41,048 [INFO ] W-9000-titan_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stdout\n",
            "2025-01-20T18:10:15,006 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml]\n",
            "2025-01-20T18:10:16,670 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - s_name_part0=/tmp/.ts.sock, s_name_part1=9000, pid=12680\n",
            "2025-01-20T18:10:16,686 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Listening on port: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:10:16,697 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Successfully loaded /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml.\n",
            "2025-01-20T18:10:16,698 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - [PID]12680\n",
            "2025-01-20T18:10:16,698 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Torch worker started.\n",
            "2025-01-20T18:10:16,698 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Python runtime: 3.11.11\n",
            "2025-01-20T18:10:16,699 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STOPPED -> WORKER_STARTED\n",
            "2025-01-20T18:10:16,699 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Connecting to: /tmp/.ts.sock.9000\n",
            "2025-01-20T18:10:16,701 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Flushing req.cmd LOAD repeats 1 to backend at: 1737396616700\n",
            "2025-01-20T18:10:16,702 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Looping backend response at: 1737396616702\n",
            "2025-01-20T18:10:16,704 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Connection accepted: /tmp/.ts.sock.9000.\n",
            "2025-01-20T18:10:16,705 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - model_name: titan, batchSize: 1\n",
            "2025-01-20T18:10:16,707 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Backend worker process died.\n",
            "2025-01-20T18:10:16,708 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - Traceback (most recent call last):\n",
            "2025-01-20T18:10:16,709 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 301, in <module>\n",
            "2025-01-20T18:10:16,709 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     worker.run_server()\n",
            "2025-01-20T18:10:16,710 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 268, in run_server\n",
            "2025-01-20T18:10:16,711 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     self.handle_connection(cl_socket)\n",
            "2025-01-20T18:10:16,711 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 196, in handle_connection\n",
            "2025-01-20T18:10:16,712 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service, result, code = self.load_model(msg)\n",
            "2025-01-20T18:10:16,717 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                             ^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:10:16,718 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py\", line 133, in load_model\n",
            "2025-01-20T18:10:16,718 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     service = model_loader.load(\n",
            "2025-01-20T18:10:16,719 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -               ^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:10:16,719 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 125, in load\n",
            "2025-01-20T18:10:16,720 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     entry_point, initialize_fn = self._get_class_entry_point(module)\n",
            "2025-01-20T18:10:16,721 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "2025-01-20T18:10:16,721 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -   File \"/usr/local/lib/python3.11/dist-packages/ts/model_loader.py\", line 178, in _get_class_entry_point\n",
            "2025-01-20T18:10:16,722 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG -     raise ValueError(\n",
            "2025-01-20T18:10:16,722 [INFO ] W-9000-titan_1.0-stdout MODEL_LOG - ValueError: Expected only one class in custom service code or a function entry point [<class 'titan_handler.CustomEmbedding'>, <class 'titan_handler.CustomTokenizer'>, <class 'titan_handler.TextDataset'>, <class 'titan_handler.TitanHandler'>, <class 'titan_handler.TitanModelWithCustomEmbedding'>]\n",
            "2025-01-20T18:10:16,735 [INFO ] epollEventLoopGroup-5-2 org.pytorch.serve.wlm.WorkerThread - 9000 Worker disconnected. WORKER_STARTED\n",
            "2025-01-20T18:10:16,735 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - System state is : WORKER_STARTED\n",
            "2025-01-20T18:10:16,735 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Backend worker monitoring thread interrupted or backend worker process died., startupTimeout:120sec\n",
            "java.lang.InterruptedException: null\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.reportInterruptAfterWait(AbstractQueuedSynchronizer.java:2056) ~[?:?]\n",
            "\tat java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2133) ~[?:?]\n",
            "\tat java.util.concurrent.ArrayBlockingQueue.poll(ArrayBlockingQueue.java:432) ~[?:?]\n",
            "\tat org.pytorch.serve.wlm.WorkerThread.run(WorkerThread.java:234) [model-server.jar:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128) [?:?]\n",
            "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628) [?:?]\n",
            "\tat java.lang.Thread.run(Thread.java:829) [?:?]\n",
            "2025-01-20T18:10:16,736 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.BatchAggregator - Load model failed: titan, error: Worker died.\n",
            "2025-01-20T18:10:16,736 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - W-9000-titan_1.0 State change WORKER_STARTED -> WORKER_STOPPED\n",
            "2025-01-20T18:10:16,736 [WARN ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Auto recovery failed again\n",
            "2025-01-20T18:10:16,736 [INFO ] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerThread - Retry worker: 9000 in 55 seconds.\n",
            "2025-01-20T18:10:16,787 [INFO ] W-9000-titan_1.0-stderr org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stderr\n",
            "2025-01-20T18:10:16,789 [INFO ] W-9000-titan_1.0-stdout org.pytorch.serve.wlm.WorkerLifeCycle - Stopped Scanner - W-9000-titan_1.0-stdout\n",
            "2025-01-20T18:10:30,203 [ERROR] Thread-13 org.pytorch.serve.metrics.MetricCollector - Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ts/metrics/metric_collector.py\", line 11, in <module>\n",
            "    from ts.metrics import system_metrics\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ts/__init__.py\", line 10, in <module>\n",
            "    from . import version\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ts/version.py\", line 5, in <module>\n",
            "    from pathlib import Path\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pathlib.py\", line 10, in <module>\n",
            "    from collections import Sequence\n",
            "ImportError: cannot import name 'Sequence' from 'collections' (/usr/lib/python3.11/collections/__init__.py)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!torchserve --start --model-store model_store --models titan=titan.mar\n"
      ],
      "metadata": {
        "id": "uRw-_Y_pfO9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60bd74cc-4c06-4414-e1d8-b3980ab6e399"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: sun.reflect.Reflection.getCallerClass is not supported. This will impact performance.\n",
            "2025-01-20T17:19:27,325 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager -  Loading snapshot serializer plugin...\n",
            "2025-01-20T17:19:27,519 [DEBUG] main org.pytorch.serve.util.ConfigManager - xpu-smi not available or failed: Cannot run program \"xpu-smi\": error=2, No such file or directory\n",
            "2025-01-20T17:19:27,526 [WARN ] main org.pytorch.serve.util.ConfigManager - Your torchserve instance can access any URL to load models. When deploying to production, make sure to limit the set of allowed_urls in config.properties\n",
            "2025-01-20T17:19:27,606 [INFO ] main org.pytorch.serve.util.TokenAuthorization - \n",
            "######\n",
            "TorchServe now enforces token authorization by default.\n",
            "This requires the correct token to be provided when calling an API.\n",
            "Key file located at /content/key_file.json\n",
            "Check token authorization documenation for information: https://github.com/pytorch/serve/blob/master/docs/token_authorization_api.md \n",
            "######\n",
            "\n",
            "2025-01-20T17:19:27,606 [INFO ] main org.pytorch.serve.servingsdk.impl.PluginsManager - Initializing plugins manager...\n",
            "2025-01-20T17:19:27,897 [INFO ] main org.pytorch.serve.metrics.configuration.MetricConfiguration - Successfully loaded metrics configuration from /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml\n",
            "2025-01-20T17:19:28,207 [INFO ] main org.pytorch.serve.ModelServer - \n",
            "Torchserve version: 0.12.0\n",
            "TS Home: /usr/local/lib/python3.11/dist-packages\n",
            "Current directory: /content\n",
            "Temp directory: /tmp\n",
            "Metrics config path: /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml\n",
            "Number of GPUs: 1\n",
            "Number of CPUs: 2\n",
            "Max heap size: 3246 M\n",
            "Python executable: /usr/bin/python3\n",
            "Config file: N/A\n",
            "Inference address: http://127.0.0.1:8080\n",
            "Management address: http://127.0.0.1:8081\n",
            "Metrics address: http://127.0.0.1:8082\n",
            "Model Store: /content/model_store\n",
            "Initial Models: titan=titan.mar\n",
            "Log dir: /content/logs\n",
            "Metrics dir: /content/logs\n",
            "Netty threads: 0\n",
            "Netty client threads: 0\n",
            "Default workers per model: 1\n",
            "Blacklist Regex: N/A\n",
            "Maximum Response Size: 6553500\n",
            "Maximum Request Size: 6553500\n",
            "Limit Maximum Image Pixels: true\n",
            "Prefer direct buffer: false\n",
            "Allowed Urls: [file://.*|http(s)?://.*]\n",
            "Custom python dependency for model allowed: false\n",
            "Enable metrics API: true\n",
            "Metrics mode: LOG\n",
            "Disable system metrics: false\n",
            "Workflow Store: /content/model_store\n",
            "CPP log config: N/A\n",
            "Model config: N/A\n",
            "System metrics command: default\n",
            "Model API enabled: false\n",
            "2025-01-20T17:19:28,274 [INFO ] main org.pytorch.serve.ModelServer - Loading initial models: titan.mar\n",
            "2025-01-20T17:20:04,382 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Adding new version 1.0 for model titan\n",
            "2025-01-20T17:20:04,382 [DEBUG] main org.pytorch.serve.wlm.ModelVersionedRefs - Setting default version to 1.0 for model titan\n",
            "2025-01-20T17:20:04,383 [INFO ] main org.pytorch.serve.wlm.ModelManager - Model titan loaded.\n",
            "2025-01-20T17:20:04,383 [DEBUG] main org.pytorch.serve.wlm.ModelManager - updateModel: titan, count: 1\n",
            "2025-01-20T17:20:04,403 [INFO ] main org.pytorch.serve.ModelServer - Initialize Inference server with: EpollServerSocketChannel.\n",
            "2025-01-20T17:20:04,414 [DEBUG] W-9000-titan_1.0 org.pytorch.serve.wlm.WorkerLifeCycle - Worker cmdline: [/usr/bin/python3, /usr/local/lib/python3.11/dist-packages/ts/model_service_worker.py, --sock-type, unix, --sock-name, /tmp/.ts.sock.9000, --metrics-config, /usr/local/lib/python3.11/dist-packages/ts/configs/metrics.yaml]\n",
            "2025-01-20T17:20:06,562 [INFO ] main org.pytorch.serve.ModelServer - Torchserve stopped.\n",
            "java.io.IOException: Failed to bind to address: http://127.0.0.1:8080\n",
            "\tat org.pytorch.serve.ModelServer.initializeServer(ModelServer.java:354)\n",
            "\tat org.pytorch.serve.ModelServer.startRESTserver(ModelServer.java:415)\n",
            "\tat org.pytorch.serve.ModelServer.startAndWait(ModelServer.java:124)\n",
            "\tat org.pytorch.serve.ModelServer.main(ModelServer.java:105)\n",
            "Caused by: io.netty.channel.unix.Errors$NativeIoException: bind(..) failed: Address already in use\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.torchserve\n",
        "!nano ~/.torchserve/config.properties\n"
      ],
      "metadata": {
        "id": "FPOsGJa2nQZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## push model to huggingface"
      ],
      "metadata": {
        "id": "JfUeIGZonrEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()\n"
      ],
      "metadata": {
        "id": "Dwz7ywflk6x5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "b13156840a134523b7e75349c7f0a7c4",
            "5696dcc327904b278858d660ad3f8f7c",
            "f6766bbbdcb647e6a22041e628d8b2d0",
            "95d85b4bab814c30b3b89bf5636a9f8f",
            "f2d231bd699541949e5a073730f4d625",
            "dcc7b21db20e4a7c96d2c09e32befa36",
            "93c55ba22f4b44a9a743a12f9e42e11b",
            "c503930f1c9b420793d0534d3f0c7f1b",
            "39d90b14d7a54e1a9605363a7f4b47c6",
            "2be16a521e31493c8e8fe458eaa058ca",
            "721d9a62d50c4bffbb6daf78eb80c272",
            "33b7c997711042cc9c523dd876643538",
            "b8580ef455e24285a7a11821ebd51c97",
            "5fca24af31b24d868acd5f74a7b2c616",
            "7f888bf4839b4b1790b25bdb94c9a5e3",
            "59e5fd00d9d542b08ddc8b93fb100991",
            "60cca3e60fd74c2bb6e4db0e2fb2c5a2",
            "c557620bc9394123a90575ff981cbfa8",
            "eb8b3fbefe9247408522a3df181bd2f2",
            "d799cb39bb0446358210f05d934b7136"
          ]
        },
        "outputId": "f74d7b24-0657-4177-fa01-ad931a5042a8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b13156840a134523b7e75349c7f0a7c4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(\"titan-transformer\", private=False)  # Set `private=True` if you want the model to be private\n"
      ],
      "metadata": {
        "id": "B3nVInOdnuZN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "a98ef768-d8dc-4894-db1d-693cae983501"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "HfHubHTTPError",
          "evalue": "409 Client Error: Conflict for url: https://huggingface.co/api/repos/create (Request ID: Root=1-678e91d7-779f6a0a18d74ff153691531;f54ad6b9-0c1d-4f67-bdb6-c3a1b97e369f)\n\nYou already created this model repo",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 409 Client Error: Conflict for url: https://huggingface.co/api/repos/create",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-65713f5d9e18>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcreate_repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcreate_repo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"titan-transformer\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprivate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Set `private=True` if you want the model to be private\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mcreate_repo\u001b[0;34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[0m\n\u001b[1;32m   3523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3524\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3525\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3526\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3527\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mexist_ok\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m409\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;31m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHfHubHTTPError\u001b[0m: 409 Client Error: Conflict for url: https://huggingface.co/api/repos/create (Request ID: Root=1-678e91d7-779f6a0a18d74ff153691531;f54ad6b9-0c1d-4f67-bdb6-c3a1b97e369f)\n\nYou already created this model repo"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "# Upload model.pt\n",
        "upload_file(\n",
        "    path_or_fileobj=\"model_store/model.pt\",\n",
        "    path_in_repo=\"pytorch_model.bin\",  # This is the standard filename for the model weights\n",
        "    repo_id=\"rajveer43/titan-transformer\"  # Replace with your Hugging Face username and repo name\n",
        ")\n",
        "\n",
        "# Upload tokenizer_config.json\n",
        "upload_file(\n",
        "    path_or_fileobj=\"model_store/tokenizer_config.json\",\n",
        "    path_in_repo=\"tokenizer_config.json\",\n",
        "    repo_id=\"rajveer43/titan-transformer\"\n",
        ")\n",
        "\n",
        "# Upload vocab.txt\n",
        "upload_file(\n",
        "    path_or_fileobj=\"model_store/vocab.txt\",\n",
        "    path_in_repo=\"vocab.txt\",\n",
        "    repo_id=\"rajveer43/titan-transformer\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "EZO2WpCboY6Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "e307d4f896844ad789dd60c7d84e206e",
            "7113ac964e9b4f3d9aaa9377e777e6d8",
            "969cf83fff904a2abaa2aeef67a2941e",
            "d55e1fd354f740e9ae331340a30b593a",
            "de95fcc37c53468bb39785c94f54abf2",
            "fe196903723344f3a42800522f2b3af3",
            "78d5ea11905d42bcaaf257f307a75594",
            "9550f280af6d49b0b13cfba60b62c97e",
            "869c9dbe70614c3b84f9024dfd21c9b0",
            "d86f68c76bcb424fada8b788ebb94746",
            "b5c1a8f0bef3459a9876b478e7b2199b"
          ]
        },
        "outputId": "522b4b48-24e3-47fd-f594-90aaba72d17a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.pt:   0%|          | 0.00/1.78G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e307d4f896844ad789dd60c7d84e206e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/rajveer43/titan-transformer/commit/4a0f6d070b5b64f388daa0a06581f17e806bbc63', commit_message='Upload vocab.txt with huggingface_hub', commit_description='', oid='4a0f6d070b5b64f388daa0a06581f17e806bbc63', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rajveer43/titan-transformer', endpoint='https://huggingface.co', repo_type='model', repo_id='rajveer43/titan-transformer'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "# Upload model checkpoint\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content/checkpoints/best.pt\",\n",
        "    path_in_repo=\"model/checkpoints/best.pt\",\n",
        "    repo_id=\"rajveer43/titan-transformer\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content/checkpoints/latest.pt\",\n",
        "    path_in_repo=\"model/checkpoints/latest.pt\",\n",
        "    repo_id=\"rajveer43/titan-transformer\",\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Yu139sCEowj2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150,
          "referenced_widgets": [
            "5aa21eb9621c43f7b8ed6a88bca8f73e",
            "dd0f021734a34171b8e16d8feca7730e",
            "46700dbebc714d04bb0937c20e3d6b32",
            "7774423d40be4a45a70a8e65a3ba9938",
            "52e1e326c3de4afab83bb3de7f672a30",
            "e19b95a5f9d7465da13d41a5be9c87e4",
            "7dddfc478fdc4e74aaa8ab01ff904e84",
            "eb61d11ff02d4c9eb6de6b0de63a61bb",
            "536fd26457564d608ba41b81171da328",
            "c349571575fd44c7b338e314becbe0b7",
            "5ebdad84b0c440249ad5b108ba522ee6",
            "050fdc221f704e5cb12846d2ef4a5487",
            "baf32155ec8f4a30b26cc07a609bd25a",
            "6319f503d9a74baea291ec164e6fcebc",
            "cb71539f140048b0965f8cba47086123",
            "414a3f21b9864ad29963a13afe83b170",
            "1f4e7e158eac4ad6b8054cf6cd66cef1",
            "ace8961ca8fb4c4a99742e323cc6e936",
            "b8ab03626f2849be893b8dfd3d5cf4cb",
            "d941eee846354cf89da6bb3da4899b01",
            "44b534b49f2a45e1945bd1395121af60",
            "13f7ede4ab1342739d5b0e6d3cab88a1"
          ]
        },
        "outputId": "407de74b-6f6f-445a-8e2a-2ef3ea342818"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "best.pt:   0%|          | 0.00/1.78G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5aa21eb9621c43f7b8ed6a88bca8f73e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "latest.pt:   0%|          | 0.00/1.78G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "050fdc221f704e5cb12846d2ef4a5487"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CommitInfo(commit_url='https://huggingface.co/rajveer43/titan-transformer/commit/7d02efea87c49703d56bec1fafe885af81ab1f6c', commit_message='Upload model/checkpoints/latest.pt with huggingface_hub', commit_description='', oid='7d02efea87c49703d56bec1fafe885af81ab1f6c', pr_url=None, repo_url=RepoUrl('https://huggingface.co/rajveer43/titan-transformer', endpoint='https://huggingface.co', repo_type='model', repo_id='rajveer43/titan-transformer'), pr_revision=None, pr_num=None)"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import upload_file\n",
        "\n",
        "logs_folder = '/content/logs'  # Path to your logs folder\n",
        "repo_id = \"rajveer43/titan-transformer\"  # Your repo name on Hugging Face\n",
        "\n",
        "# Loop through all files in the logs folder and upload them\n",
        "for root, dirs, files in os.walk(logs_folder):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        # Upload each file to the Hugging Face repo\n",
        "        upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=f\"logs/{file}\",  # Keep the folder structure in the repo\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "sdBNdj06qFXv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c2e8ffdd-691e-4980-d851-03f1fc3100c8"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "No files have been modified since last commit. Skipping to prevent empty commit.\n",
            "WARNING:huggingface_hub.hf_api:No files have been modified since last commit. Skipping to prevent empty commit.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import upload_file\n",
        "\n",
        "logs_folder = '/content/wandb'  # Path to your logs folder\n",
        "repo_id = \"rajveer43/titan-transformer\"  # Your repo name on Hugging Face\n",
        "\n",
        "# Loop through all files in the logs folder and upload them\n",
        "for root, dirs, files in os.walk(logs_folder):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        # Upload each file to the Hugging Face repo\n",
        "        upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=f\"wandb/{file}\",  # Keep the folder structure in the repo\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "DzDX_H8tqUv-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "5e141b63-4964-4992-9cf2-fc10eb3d0195"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Provided path: '/content/wandb/debug-internal.log' is not a file on the local file system",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-2ecce9501fb2>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# Upload each file to the Hugging Face repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         upload_file(\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mpath_or_fileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mpath_in_repo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"wandb/{file}\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Keep the folder structure in the repo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_validators.py\u001b[0m in \u001b[0;36m_inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmoothly_deprecate_use_auth_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_token\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhas_token\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_inner_fn\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36m_inner\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;31m# Otherwise, call the function normally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0m_inner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_future_compatible\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/hf_api.py\u001b[0m in \u001b[0;36mupload_file\u001b[0;34m(self, path_or_fileobj, path_in_repo, repo_id, token, repo_type, revision, commit_message, commit_description, create_pr, parent_commit, run_as_future)\u001b[0m\n\u001b[1;32m   4432\u001b[0m             \u001b[0mcommit_message\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mcommit_message\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34mf\"Upload {path_in_repo} with huggingface_hub\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4433\u001b[0m         )\n\u001b[0;32m-> 4434\u001b[0;31m         operation = CommitOperationAdd(\n\u001b[0m\u001b[1;32m   4435\u001b[0m             \u001b[0mpath_or_fileobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_or_fileobj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4436\u001b[0m             \u001b[0mpath_in_repo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath_in_repo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/_commit_api.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_in_repo, path_or_fileobj)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/_commit_api.py\u001b[0m in \u001b[0;36m__post_init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m             \u001b[0mpath_or_fileobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_or_fileobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_or_fileobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Provided path: '{path_or_fileobj}' is not a file on the local file system\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath_or_fileobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBufferedIOBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m             \u001b[0;31m# ^^ Inspired from: https://stackoverflow.com/questions/44584829/how-to-determine-if-file-is-opened-in-binary-or-text-mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Provided path: '/content/wandb/debug-internal.log' is not a file on the local file system"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZuFMXxJpu-ZW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}