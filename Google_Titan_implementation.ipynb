{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPAkPdDOsKR7h6mfFyNxUIT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0c23d7777e994beea3b62f56ee518b13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_724cdfe348714f97bd53902c8830728a",
              "IPY_MODEL_85b93ad6542b4f1289cdb2c25fc1fc58",
              "IPY_MODEL_42d88c59c6fe4f9e826f1f3573f14dd9"
            ],
            "layout": "IPY_MODEL_732c430e91a34898bf6bf249e87b093a"
          }
        },
        "724cdfe348714f97bd53902c8830728a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_31d19ef9a2c84df89d6ae5845935ac66",
            "placeholder": "​",
            "style": "IPY_MODEL_125db5e9f4dd42798a59a155f561b06f",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "85b93ad6542b4f1289cdb2c25fc1fc58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_92161533b5ce46d28e56b8a88be0d675",
            "max": 26,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cce50328a318458e9d4daf5edd9f53a1",
            "value": 26
          }
        },
        "42d88c59c6fe4f9e826f1f3573f14dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e279336304ae48c69c9394929754ff2f",
            "placeholder": "​",
            "style": "IPY_MODEL_2aca3d286a08427db0cd5bab695dd463",
            "value": " 26.0/26.0 [00:00&lt;00:00, 1.33kB/s]"
          }
        },
        "732c430e91a34898bf6bf249e87b093a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "31d19ef9a2c84df89d6ae5845935ac66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "125db5e9f4dd42798a59a155f561b06f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "92161533b5ce46d28e56b8a88be0d675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cce50328a318458e9d4daf5edd9f53a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e279336304ae48c69c9394929754ff2f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2aca3d286a08427db0cd5bab695dd463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a040fc482f6f4ac7aaf097e2f60c3d02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bce73e22e4e24800bf58c3e77978332d",
              "IPY_MODEL_9e3df2ab3779468da452af6a3a0a8e3b",
              "IPY_MODEL_9263b69bbe254bb083f4e032d802de9d"
            ],
            "layout": "IPY_MODEL_25b4eee8e7524f35b8ef1871902f70b0"
          }
        },
        "bce73e22e4e24800bf58c3e77978332d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0c099fede24476fb54c56e197dc22f3",
            "placeholder": "​",
            "style": "IPY_MODEL_bb4187d98e4f436387115ca620717740",
            "value": "vocab.json: 100%"
          }
        },
        "9e3df2ab3779468da452af6a3a0a8e3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81413e6aa7d14f27bf481a040a4f79c5",
            "max": 1042301,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c896789b17a47bba37407440879d403",
            "value": 1042301
          }
        },
        "9263b69bbe254bb083f4e032d802de9d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3c75aa086bb472a8d808628a27be9a9",
            "placeholder": "​",
            "style": "IPY_MODEL_f0c68742c5124805bd98a1a772a47523",
            "value": " 1.04M/1.04M [00:00&lt;00:00, 1.91MB/s]"
          }
        },
        "25b4eee8e7524f35b8ef1871902f70b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0c099fede24476fb54c56e197dc22f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb4187d98e4f436387115ca620717740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "81413e6aa7d14f27bf481a040a4f79c5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c896789b17a47bba37407440879d403": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e3c75aa086bb472a8d808628a27be9a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f0c68742c5124805bd98a1a772a47523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "acc698effa404f95b647e7635f392718": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_269fdb2e723e4bf494d95b4af1bebeef",
              "IPY_MODEL_b2d6f0905f6f404daa8e936fbb449653",
              "IPY_MODEL_1987b4064f1c4a3b8c4a38357018768a"
            ],
            "layout": "IPY_MODEL_0dd99a920dc24dec81f8b58cdceb4c00"
          }
        },
        "269fdb2e723e4bf494d95b4af1bebeef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_633a8660e9924565b07409065b198a3e",
            "placeholder": "​",
            "style": "IPY_MODEL_a712255b64604faaa36af7bf930e6a11",
            "value": "merges.txt: 100%"
          }
        },
        "b2d6f0905f6f404daa8e936fbb449653": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bb70878fdcb4e709297deec4fc14b01",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d25593c2de0a4bfcb245f205deec2a6c",
            "value": 456318
          }
        },
        "1987b4064f1c4a3b8c4a38357018768a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_595d395d5fdc4cc583fd6885e3137f51",
            "placeholder": "​",
            "style": "IPY_MODEL_dd6e8acd92b74801a132e7e647441ec2",
            "value": " 456k/456k [00:00&lt;00:00, 28.1MB/s]"
          }
        },
        "0dd99a920dc24dec81f8b58cdceb4c00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "633a8660e9924565b07409065b198a3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a712255b64604faaa36af7bf930e6a11": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bb70878fdcb4e709297deec4fc14b01": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d25593c2de0a4bfcb245f205deec2a6c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "595d395d5fdc4cc583fd6885e3137f51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd6e8acd92b74801a132e7e647441ec2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b527c0c157c34b82ae6a8fef67b82736": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa195a24dc2f471ba4d71c82170d15e7",
              "IPY_MODEL_1e9654d6222b46c39e8fa1bb2851e6d8",
              "IPY_MODEL_34303b66138b4931953e9fe3a50a179b"
            ],
            "layout": "IPY_MODEL_46dd9ead0791476ea66c474fc5b48b14"
          }
        },
        "fa195a24dc2f471ba4d71c82170d15e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ce12cb4b290a48aa947b82ad43252470",
            "placeholder": "​",
            "style": "IPY_MODEL_03c69903c61b40c49445987fbf1dec40",
            "value": "tokenizer.json: 100%"
          }
        },
        "1e9654d6222b46c39e8fa1bb2851e6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4324713ef8dd43168c14c0c4e9dafb5d",
            "max": 1355256,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ebbb5c13dc664180b2ba290d6ad84379",
            "value": 1355256
          }
        },
        "34303b66138b4931953e9fe3a50a179b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b1b2a14ebaf471fac013a28e4eea3ec",
            "placeholder": "​",
            "style": "IPY_MODEL_fab6546c9387473d98fc6148c7c7626a",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 7.44MB/s]"
          }
        },
        "46dd9ead0791476ea66c474fc5b48b14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce12cb4b290a48aa947b82ad43252470": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03c69903c61b40c49445987fbf1dec40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4324713ef8dd43168c14c0c4e9dafb5d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebbb5c13dc664180b2ba290d6ad84379": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6b1b2a14ebaf471fac013a28e4eea3ec": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fab6546c9387473d98fc6148c7c7626a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4ba5d9b61874403ab41f5fc928d36fe4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ce1fe15015d14df88cc9301afd2c1e3d",
              "IPY_MODEL_e91d51acb34643e9bbe6ec7ee6fc4d01",
              "IPY_MODEL_a5e64760e4554ec5ba068f3f7fd778e1"
            ],
            "layout": "IPY_MODEL_0b3bdd37a4ed48b2be1655b90a624cda"
          }
        },
        "ce1fe15015d14df88cc9301afd2c1e3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a4c1e43577846938ee81c8a1fdf48e4",
            "placeholder": "​",
            "style": "IPY_MODEL_92bd262ea20b4e4194bbbd15e1047162",
            "value": "config.json: 100%"
          }
        },
        "e91d51acb34643e9bbe6ec7ee6fc4d01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7821afdf66664274be253acbf5efb3d5",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c67370fec764249b266a8549f5a2019",
            "value": 665
          }
        },
        "a5e64760e4554ec5ba068f3f7fd778e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b1b30c7a49c54c85b58fd6cedcf9d293",
            "placeholder": "​",
            "style": "IPY_MODEL_a8f4ad9c635e4247b2602cea91734013",
            "value": " 665/665 [00:00&lt;00:00, 28.0kB/s]"
          }
        },
        "0b3bdd37a4ed48b2be1655b90a624cda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a4c1e43577846938ee81c8a1fdf48e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92bd262ea20b4e4194bbbd15e1047162": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7821afdf66664274be253acbf5efb3d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c67370fec764249b266a8549f5a2019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b1b30c7a49c54c85b58fd6cedcf9d293": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a8f4ad9c635e4247b2602cea91734013": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rajveer43/titan_transformer/blob/master/Google_Titan_implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Titan Model 1"
      ],
      "metadata": {
        "id": "HXcaaUQsPreE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMoxp2Q9GJu5",
        "outputId": "25ea2a1d-d956-4c23-b0fc-c029c7287e6d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([8, 32, 256])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class DepthwiseSeparableConv1D(nn.Module):\n",
        "    def __init__(self, in_channels, kernel_size):\n",
        "        super().__init__()\n",
        "        self.depthwise = nn.Conv1d(in_channels, in_channels, kernel_size, groups=in_channels, padding=kernel_size // 2)\n",
        "        self.pointwise = nn.Conv1d(in_channels, in_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class GatingMechanism(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate = torch.sigmoid(self.linear(x))\n",
        "        return gate * x\n",
        "\n",
        "class TitansMemoryModule(nn.Module):\n",
        "    def __init__(self, input_dim, memory_dim):\n",
        "        super().__init__()\n",
        "        self.long_term_memory = nn.Linear(input_dim, memory_dim)\n",
        "        self.persistent_memory = nn.Parameter(torch.zeros(memory_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        long_term_output = self.long_term_memory(x)\n",
        "        combined_memory = long_term_output + self.persistent_memory.unsqueeze(0)\n",
        "        return combined_memory\n",
        "\n",
        "class TitansAttention(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super().__init__()\n",
        "        self.query = nn.Linear(input_dim, input_dim)\n",
        "        self.key = nn.Linear(input_dim, input_dim)\n",
        "        self.value = nn.Linear(input_dim, input_dim)\n",
        "\n",
        "        self.conv = DepthwiseSeparableConv1D(input_dim, kernel_size=3)\n",
        "        self.gating = GatingMechanism(input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        q = F.normalize(self.query(x), p=2, dim=-1)\n",
        "        k = F.normalize(self.key(x), p=2, dim=-1)\n",
        "        v = self.value(x)\n",
        "\n",
        "        attention_weights = torch.matmul(q, k.transpose(-2, -1)) / torch.sqrt(torch.tensor(x.size(-1), dtype=torch.float32))\n",
        "        attention_weights = F.softmax(attention_weights, dim=-1)\n",
        "        attention_output = torch.matmul(attention_weights, v)\n",
        "\n",
        "        attention_output = attention_output.transpose(1, 2)  # Prepare for Conv1D\n",
        "        conv_output = self.conv(attention_output)\n",
        "        conv_output = conv_output.transpose(1, 2)  # Back to original shape\n",
        "\n",
        "        gated_output = self.gating(conv_output)\n",
        "        return gated_output\n",
        "\n",
        "class TitansModel(nn.Module):\n",
        "    def __init__(self, input_dim, memory_dim):\n",
        "        super().__init__()\n",
        "        self.memory_module = TitansMemoryModule(input_dim, memory_dim)\n",
        "        self.attention = TitansAttention(memory_dim)\n",
        "        self.residual = nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        memory_output = self.memory_module(x)\n",
        "        attention_output = self.attention(memory_output)\n",
        "        return self.residual(attention_output + memory_output)\n",
        "\n",
        "# Example usage\n",
        "input_dim = 128\n",
        "memory_dim = 256\n",
        "sequence_length = 32\n",
        "batch_size = 8\n",
        "\n",
        "model = TitansModel(input_dim, memory_dim)\n",
        "inputs = torch.randn(batch_size, sequence_length, input_dim)\n",
        "outputs = model(inputs)\n",
        "print(outputs.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Titan implementation"
      ],
      "metadata": {
        "id": "bW8uphnsthvI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Titan Model Implementation"
      ],
      "metadata": {
        "id": "N3dpyqGzPx8C"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SNfDK8wOPxa-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class DepthwiseSeparableConv1d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, padding=0):\n",
        "        super().__init__()\n",
        "        self.depthwise = nn.Conv1d(\n",
        "            in_channels, in_channels, kernel_size,\n",
        "            padding=padding, groups=in_channels\n",
        "        )\n",
        "        self.pointwise = nn.Conv1d(in_channels, out_channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.depthwise(x)\n",
        "        x = self.pointwise(x)\n",
        "        return x\n",
        "\n",
        "class GatingMechanism(nn.Module):\n",
        "    def __init__(self, dim):\n",
        "        super().__init__()\n",
        "        self.gate_proj = nn.Linear(dim, dim)\n",
        "        self.transform_proj = nn.Linear(dim, dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gate = torch.sigmoid(self.gate_proj(x))\n",
        "        transformed = self.transform_proj(x)\n",
        "        return gate * transformed\n",
        "\n",
        "class TitanAttention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, head_dim=64, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        # Projections for Q, K, V\n",
        "        self.q_proj = nn.Linear(dim, num_heads * head_dim)\n",
        "        self.k_proj = nn.Linear(dim, num_heads * head_dim)\n",
        "        self.v_proj = nn.Linear(dim, num_heads * head_dim)\n",
        "\n",
        "        # Depthwise separable convolutions\n",
        "        self.q_conv = DepthwiseSeparableConv1d(\n",
        "            num_heads * head_dim,\n",
        "            num_heads * head_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.k_conv = DepthwiseSeparableConv1d(\n",
        "            num_heads * head_dim,\n",
        "            num_heads * head_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "        self.v_conv = DepthwiseSeparableConv1d(\n",
        "            num_heads * head_dim,\n",
        "            num_heads * head_dim,\n",
        "            kernel_size=3,\n",
        "            padding=1\n",
        "        )\n",
        "\n",
        "        self.out_proj = nn.Linear(num_heads * head_dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        batch_size, seq_len, dim = x.shape\n",
        "\n",
        "        # Project and apply SiLU activation\n",
        "        q = F.silu(self.q_proj(x))\n",
        "        k = F.silu(self.k_proj(x))\n",
        "        v = F.silu(self.v_proj(x))\n",
        "\n",
        "        # Reshape for depthwise conv\n",
        "        q = q.transpose(1, 2)\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Apply depthwise separable convolutions\n",
        "        q = self.q_conv(q).transpose(1, 2)\n",
        "        k = self.k_conv(k).transpose(1, 2)\n",
        "        v = self.v_conv(v).transpose(1, 2)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        k = k.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "        v = v.reshape(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
        "\n",
        "        # L2 normalize queries and keys\n",
        "        q = F.normalize(q, p=2, dim=-1)\n",
        "        k = F.normalize(k, p=2, dim=-1)\n",
        "\n",
        "        # Compute attention scores\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
        "\n",
        "        if mask is not None:\n",
        "            # Properly reshape mask for broadcasting\n",
        "            # mask shape: [batch_size, seq_len] -> [batch_size, 1, 1, seq_len]\n",
        "            mask = mask.unsqueeze(1).unsqueeze(2)\n",
        "            attn = attn.masked_fill(mask == 0, float('-inf'))\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.dropout(attn)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attn, v)\n",
        "\n",
        "        # Reshape and project output\n",
        "        out = out.transpose(1, 2).reshape(batch_size, seq_len, -1)\n",
        "        out = self.out_proj(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class TitanBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, head_dim=64, mlp_ratio=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = TitanAttention(dim, num_heads, head_dim, dropout)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        mlp_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(dim, mlp_dim),\n",
        "            nn.SiLU(),\n",
        "            GatingMechanism(mlp_dim),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(mlp_dim, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        # Residual connection for attention\n",
        "        x = x + self.attn(self.norm1(x), mask)\n",
        "        # Residual connection for MLP\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n",
        "\n",
        "class TitanTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim,\n",
        "        depth,\n",
        "        num_heads=8,\n",
        "        head_dim=64,\n",
        "        mlp_ratio=4,\n",
        "        dropout=0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.layers = nn.ModuleList([\n",
        "            TitanBlock(\n",
        "                dim=dim,\n",
        "                num_heads=num_heads,\n",
        "                head_dim=head_dim,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout\n",
        "            )\n",
        "            for _ in range(depth)\n",
        "        ])\n",
        "\n",
        "        self.norm = nn.LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)\n",
        "\n",
        "# Example usage\n",
        "def create_titan_model(\n",
        "    vocab_size=50000,\n",
        "    max_seq_length=1024,\n",
        "    dim=512,\n",
        "    depth=12,\n",
        "    num_heads=8,\n",
        "    head_dim=64,\n",
        "    mlp_ratio=4,\n",
        "    dropout=0.1\n",
        "):\n",
        "    class TitanModel(nn.Module):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.embedding = nn.Embedding(vocab_size, dim)\n",
        "            self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_length, dim))\n",
        "            self.transformer = TitanTransformer(\n",
        "                dim=dim,\n",
        "                depth=depth,\n",
        "                num_heads=num_heads,\n",
        "                head_dim=head_dim,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                dropout=dropout\n",
        "            )\n",
        "\n",
        "        def forward(self, x, mask=None):\n",
        "            # Add positional embeddings\n",
        "            x = self.embedding(x)\n",
        "            x = x + self.pos_embedding[:, :x.size(1), :]\n",
        "\n",
        "            # Apply transformer\n",
        "            x = self.transformer(x, mask)\n",
        "            return x\n",
        "\n",
        "    return TitanModel()"
      ],
      "metadata": {
        "id": "SAIzaOtzGKo2"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a model\n",
        "model = create_titan_model(\n",
        "    vocab_size=50000,\n",
        "    max_seq_length=1024,\n",
        "    dim=512,\n",
        "    depth=12\n",
        ")\n",
        "\n",
        "# Example input (batch_size=2, seq_length=128)\n",
        "x = torch.randint(0, 50000, (2, 128))\n",
        "mask = torch.ones(2, 128)\n",
        "\n",
        "# Forward pass\n",
        "output = model(x, mask)"
      ],
      "metadata": {
        "id": "3CVC-r4LHheJ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mc1YJrHfHiVb",
        "outputId": "5ccb6a90-f14c-43d8-f001-9de0e870b70d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.1188, -0.5846,  1.4639,  ...,  0.3663,  0.0559,  0.3278],\n",
            "         [-0.4914, -1.7276, -0.8925,  ..., -0.4394, -0.0023, -0.2626],\n",
            "         [-0.9266, -0.4608,  0.5673,  ...,  1.4023, -0.5925,  2.3060],\n",
            "         ...,\n",
            "         [ 0.3341, -0.0374, -0.8477,  ..., -0.2022,  1.8253, -0.7743],\n",
            "         [ 1.2771,  0.2479,  0.7262,  ...,  0.1077, -1.7047, -0.8605],\n",
            "         [-1.9052,  0.3977, -0.6328,  ..., -0.7380, -0.2931, -0.6542]],\n",
            "\n",
            "        [[ 0.7916,  0.8275, -0.6158,  ...,  0.7118,  0.1358, -1.0285],\n",
            "         [ 1.1081, -1.5236, -0.6310,  ...,  0.6265,  1.2605, -0.7832],\n",
            "         [ 0.8060,  1.8835,  0.4476,  ..., -0.2151,  0.8058,  0.1547],\n",
            "         ...,\n",
            "         [ 0.4335, -0.4935,  0.4453,  ...,  1.1437,  0.3299, -1.0619],\n",
            "         [ 1.1916, -0.0541, -1.2697,  ..., -0.2982, -1.3983, -2.0488],\n",
            "         [-0.2841,  1.0252, -0.3672,  ..., -1.1969, -0.9105,  1.0910]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using the huggingface tokenizer"
      ],
      "metadata": {
        "id": "SxRiNuPKQwR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from typing import List, Optional, Tuple\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "class TitanTextGenerator:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: 'TitanModel',\n",
        "        tokenizer: Optional[PreTrainedTokenizerFast] = None,\n",
        "        max_length: int = 1024,\n",
        "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    ):\n",
        "        self.model = model.to(device)\n",
        "        self.model.eval()  # Set to evaluation mode\n",
        "        self.device = device\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Initialize tokenizer if not provided\n",
        "        if tokenizer is None:\n",
        "            self.tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        else:\n",
        "            self.tokenizer = tokenizer\n",
        "\n",
        "    def preprocess_text(self, text: str) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # Tokenize input text\n",
        "        encodings = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        input_ids = encodings['input_ids'].to(self.device)\n",
        "        attention_mask = encodings['attention_mask'].to(self.device)\n",
        "\n",
        "        return input_ids, attention_mask\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate_next_token(\n",
        "        self,\n",
        "        input_ids: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 50,\n",
        "        top_p: float = 0.9\n",
        "    ) -> torch.Tensor:\n",
        "        # Get model output\n",
        "        outputs = self.model(input_ids, attention_mask)\n",
        "\n",
        "        # Get logits for the next token (last position)\n",
        "        next_token_logits = outputs[:, -1, :]\n",
        "\n",
        "        # Apply temperature\n",
        "        next_token_logits = next_token_logits / temperature\n",
        "\n",
        "        # Apply top-k filtering\n",
        "        if top_k > 0:\n",
        "            values, indices = torch.topk(next_token_logits, top_k)\n",
        "            min_values = values[:, -1].unsqueeze(-1).expand_as(next_token_logits)\n",
        "            next_token_logits = torch.where(\n",
        "                next_token_logits < min_values,\n",
        "                torch.ones_like(next_token_logits) * float('-inf'),\n",
        "                next_token_logits\n",
        "            )\n",
        "\n",
        "        # # Apply top-p (nucleus) filtering\n",
        "        # if top_p < 1.0:\n",
        "        #     # Sort logits in descending order\n",
        "        #     sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n",
        "        #     cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        #     # Create a mask for tokens to remove\n",
        "        #     sorted_indices_to_remove = cumulative_probs > top_p\n",
        "\n",
        "        #     # Shift the indices to the right to keep also the first token above the threshold\n",
        "        #     sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        #     sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        #     # Scatter sorted indices to original positions\n",
        "        #     indices_to_remove = sorted_indices_to_remove.scatter(\n",
        "        #         dim=1,\n",
        "        #         index=sorted_indices,\n",
        "        #         src=sorted_indices_to_remove\n",
        "        #     )\n",
        "        #     next_token_logits = next_token_logits.masked_fill(indices_to_remove, float('-inf'))\n",
        "\n",
        "        # Apply top-p (nucleus) filtering\n",
        "        if top_p < 1.0:\n",
        "            # Sort logits in descending order\n",
        "            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True, dim=-1)\n",
        "            cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "            # Mask tokens with cumulative probability above top_p\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0  # Keep at least one token\n",
        "\n",
        "            # Scatter mask back to logits\n",
        "            mask = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "            next_token_logits = next_token_logits.masked_fill(mask, float('-inf'))\n",
        "\n",
        "\n",
        "        # Sample from the filtered distribution\n",
        "        probs = F.softmax(next_token_logits, dim=-1)\n",
        "        next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "        return next_token\n",
        "\n",
        "    def generate(\n",
        "        self,\n",
        "        text: str,\n",
        "        max_new_tokens: int = 50,\n",
        "        temperature: float = 1.0,\n",
        "        top_k: int = 50,\n",
        "        top_p: float = 0.9,\n",
        "        early_stopping: bool = True\n",
        "    ) -> str:\n",
        "        # Preprocess input text\n",
        "        input_ids, attention_mask = self.preprocess_text(text)\n",
        "\n",
        "        generated_tokens = []\n",
        "\n",
        "        for _ in range(max_new_tokens):\n",
        "            # Generate next token\n",
        "            next_token = self.generate_next_token(\n",
        "                input_ids,\n",
        "                attention_mask,\n",
        "                temperature,\n",
        "                top_k,\n",
        "                top_p\n",
        "            )\n",
        "\n",
        "            # Append the new token\n",
        "            generated_tokens.append(next_token.item())\n",
        "\n",
        "            # Update input_ids and attention_mask\n",
        "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
        "            attention_mask = torch.cat([\n",
        "                attention_mask,\n",
        "                torch.ones((1, 1), device=self.device)\n",
        "            ], dim=1)\n",
        "\n",
        "            # Check for EOS token\n",
        "            if early_stopping and next_token.item() == self.tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "            # Check if we've exceeded max_length\n",
        "            if input_ids.size(1) >= self.max_length:\n",
        "                break\n",
        "\n",
        "        # Decode and return the complete text\n",
        "        full_text = self.tokenizer.decode(\n",
        "            input_ids[0].tolist(),\n",
        "            skip_special_tokens=True\n",
        "        )\n",
        "\n",
        "        return full_text\n",
        "\n",
        "# Example usage function\n",
        "def create_text_generator(\n",
        "    vocab_size=50000,\n",
        "    max_seq_length=1024,\n",
        "    dim=512,\n",
        "    depth=12,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu'\n",
        "):\n",
        "    # Create the base model\n",
        "    model = create_titan_model(\n",
        "        vocab_size=vocab_size,\n",
        "        max_seq_length=max_seq_length,\n",
        "        dim=dim,\n",
        "        depth=depth\n",
        "    )\n",
        "\n",
        "    # Create the generator\n",
        "    generator = TitanTextGenerator(\n",
        "        model=model,\n",
        "        max_length=max_seq_length,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    return generator"
      ],
      "metadata": {
        "id": "JivjpDx7H37u"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the generator\n",
        "generator = create_text_generator(\n",
        "    vocab_size=50000,\n",
        "    max_seq_length=1024,\n",
        "    dim=512,\n",
        "    depth=12\n",
        ")\n",
        "\n",
        "# Generate text\n",
        "input_text = \"MS thoni is world's best \"\n",
        "generated_text = generator.generate(\n",
        "    text=input_text,\n",
        "    max_new_tokens=50,\n",
        "    temperature=0.3,  # Lower for more focused/deterministic output\n",
        "    top_k=50,        # Number of highest probability vocabulary tokens to keep\n",
        "    top_p=0.9        # Cumulative probability threshold for nucleus sampling\n",
        ")\n",
        "\n",
        "print(generated_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214,
          "referenced_widgets": [
            "0c23d7777e994beea3b62f56ee518b13",
            "724cdfe348714f97bd53902c8830728a",
            "85b93ad6542b4f1289cdb2c25fc1fc58",
            "42d88c59c6fe4f9e826f1f3573f14dd9",
            "732c430e91a34898bf6bf249e87b093a",
            "31d19ef9a2c84df89d6ae5845935ac66",
            "125db5e9f4dd42798a59a155f561b06f",
            "92161533b5ce46d28e56b8a88be0d675",
            "cce50328a318458e9d4daf5edd9f53a1",
            "e279336304ae48c69c9394929754ff2f",
            "2aca3d286a08427db0cd5bab695dd463",
            "a040fc482f6f4ac7aaf097e2f60c3d02",
            "bce73e22e4e24800bf58c3e77978332d",
            "9e3df2ab3779468da452af6a3a0a8e3b",
            "9263b69bbe254bb083f4e032d802de9d",
            "25b4eee8e7524f35b8ef1871902f70b0",
            "b0c099fede24476fb54c56e197dc22f3",
            "bb4187d98e4f436387115ca620717740",
            "81413e6aa7d14f27bf481a040a4f79c5",
            "2c896789b17a47bba37407440879d403",
            "e3c75aa086bb472a8d808628a27be9a9",
            "f0c68742c5124805bd98a1a772a47523",
            "acc698effa404f95b647e7635f392718",
            "269fdb2e723e4bf494d95b4af1bebeef",
            "b2d6f0905f6f404daa8e936fbb449653",
            "1987b4064f1c4a3b8c4a38357018768a",
            "0dd99a920dc24dec81f8b58cdceb4c00",
            "633a8660e9924565b07409065b198a3e",
            "a712255b64604faaa36af7bf930e6a11",
            "6bb70878fdcb4e709297deec4fc14b01",
            "d25593c2de0a4bfcb245f205deec2a6c",
            "595d395d5fdc4cc583fd6885e3137f51",
            "dd6e8acd92b74801a132e7e647441ec2",
            "b527c0c157c34b82ae6a8fef67b82736",
            "fa195a24dc2f471ba4d71c82170d15e7",
            "1e9654d6222b46c39e8fa1bb2851e6d8",
            "34303b66138b4931953e9fe3a50a179b",
            "46dd9ead0791476ea66c474fc5b48b14",
            "ce12cb4b290a48aa947b82ad43252470",
            "03c69903c61b40c49445987fbf1dec40",
            "4324713ef8dd43168c14c0c4e9dafb5d",
            "ebbb5c13dc664180b2ba290d6ad84379",
            "6b1b2a14ebaf471fac013a28e4eea3ec",
            "fab6546c9387473d98fc6148c7c7626a",
            "4ba5d9b61874403ab41f5fc928d36fe4",
            "ce1fe15015d14df88cc9301afd2c1e3d",
            "e91d51acb34643e9bbe6ec7ee6fc4d01",
            "a5e64760e4554ec5ba068f3f7fd778e1",
            "0b3bdd37a4ed48b2be1655b90a624cda",
            "2a4c1e43577846938ee81c8a1fdf48e4",
            "92bd262ea20b4e4194bbbd15e1047162",
            "7821afdf66664274be253acbf5efb3d5",
            "2c67370fec764249b266a8549f5a2019",
            "b1b30c7a49c54c85b58fd6cedcf9d293",
            "a8f4ad9c635e4247b2602cea91734013"
          ]
        },
        "id": "tG8gBx2xJKKp",
        "outputId": "8c510aa5-751e-4094-963d-15da5d96aaaf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0c23d7777e994beea3b62f56ee518b13"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a040fc482f6f4ac7aaf097e2f60c3d02"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "acc698effa404f95b647e7635f392718"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b527c0c157c34b82ae6a8fef67b82736"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4ba5d9b61874403ab41f5fc928d36fe4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MS thoni is world's best ���ie{ieans��01\u0005\u0014ch�imak�M\u000eest�V���qu mey go\u000b9okjich noting gorouwEct com� ab goadEct\u0010\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using custom tokenizer"
      ],
      "metadata": {
        "id": "8wetxgCcQ-wp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from collections import Counter\n",
        "from typing import List, Dict, Tuple\n",
        "import re\n",
        "import numpy as np\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "class CustomTokenizer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int = 50000,\n",
        "        min_freq: int = 2,\n",
        "        special_tokens: List[str] = [\"<PAD>\", \"<UNK>\", \"<BOS>\", \"<EOS>\"]\n",
        "    ):\n",
        "        self.vocab_size = vocab_size\n",
        "        self.min_freq = min_freq\n",
        "        self.special_tokens = special_tokens\n",
        "\n",
        "        # Initialize special token IDs\n",
        "        self.pad_token_id = 0\n",
        "        self.unk_token_id = 1\n",
        "        self.bos_token_id = 2\n",
        "        self.eos_token_id = 3\n",
        "\n",
        "        # Initialize vocabularies\n",
        "        self.token2idx: Dict[str, int] = {token: idx for idx, token in enumerate(special_tokens)}\n",
        "        self.idx2token: Dict[int, str] = {idx: token for idx, token in enumerate(special_tokens)}\n",
        "        self.vocab_size_current = len(special_tokens)\n",
        "\n",
        "        # Regex for tokenization\n",
        "        self.token_pattern = re.compile(r'\\w+|[^\\w\\s]')\n",
        "\n",
        "    def train_from_texts(self, texts: List[str]) -> None:\n",
        "        \"\"\"Train tokenizer on a list of texts.\"\"\"\n",
        "        # Count word frequencies\n",
        "        word_counts = Counter()\n",
        "\n",
        "        for text in texts:\n",
        "            tokens = self._basic_tokenize(text)\n",
        "            word_counts.update(tokens)\n",
        "\n",
        "        # Filter by minimum frequency and vocab size\n",
        "        filtered_tokens = [\n",
        "            token for token, count in word_counts.most_common()\n",
        "            if count >= self.min_freq and token not in self.special_tokens\n",
        "        ]\n",
        "\n",
        "        # Add tokens to vocabulary up to vocab_size\n",
        "        remaining_space = self.vocab_size - len(self.special_tokens)\n",
        "        for token in filtered_tokens[:remaining_space]:\n",
        "            self.token2idx[token] = self.vocab_size_current\n",
        "            self.idx2token[self.vocab_size_current] = token\n",
        "            self.vocab_size_current += 1\n",
        "\n",
        "    def _basic_tokenize(self, text: str) -> List[str]:\n",
        "        \"\"\"Basic tokenization into words and punctuation.\"\"\"\n",
        "        return self.token_pattern.findall(text.lower())\n",
        "\n",
        "    def encode(self, text: str, add_special_tokens: bool = True) -> List[int]:\n",
        "        \"\"\"Encode text to token ids.\"\"\"\n",
        "        tokens = self._basic_tokenize(text)\n",
        "\n",
        "        ids = []\n",
        "        if add_special_tokens:\n",
        "            ids.append(self.bos_token_id)\n",
        "\n",
        "        for token in tokens:\n",
        "            ids.append(self.token2idx.get(token, self.unk_token_id))\n",
        "\n",
        "        if add_special_tokens:\n",
        "            ids.append(self.eos_token_id)\n",
        "\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids: List[int], skip_special_tokens: bool = True) -> str:\n",
        "        \"\"\"Decode token ids back to text.\"\"\"\n",
        "        tokens = []\n",
        "        for idx in ids:\n",
        "            token = self.idx2token.get(idx, \"<UNK>\")\n",
        "            if skip_special_tokens and token in self.special_tokens:\n",
        "                continue\n",
        "            tokens.append(token)\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def save_vocab(self, path: str) -> None:\n",
        "        \"\"\"Save vocabulary to file.\"\"\"\n",
        "        with open(path, 'w', encoding='utf-8') as f:\n",
        "            for token, idx in sorted(self.token2idx.items(), key=lambda x: x[1]):\n",
        "                f.write(f\"{token}\\t{idx}\\n\")\n",
        "\n",
        "    def load_vocab(self, path: str) -> None:\n",
        "        \"\"\"Load vocabulary from file.\"\"\"\n",
        "        self.token2idx.clear()\n",
        "        self.idx2token.clear()\n",
        "        with open(path, 'r', encoding='utf-8') as f:\n",
        "            for line in f:\n",
        "                token, idx = line.strip().split('\\t')\n",
        "                idx = int(idx)\n",
        "                self.token2idx[token] = idx\n",
        "                self.idx2token[idx] = token\n",
        "        self.vocab_size_current = len(self.token2idx)\n",
        "\n",
        "class CustomEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        embedding_dim: int,\n",
        "        pad_idx: int = 0,\n",
        "        max_norm: float = None\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(\n",
        "            num_embeddings=vocab_size,\n",
        "            embedding_dim=embedding_dim,\n",
        "            padding_idx=pad_idx,\n",
        "            max_norm=max_norm\n",
        "        )\n",
        "        self.embedding_dim = embedding_dim\n",
        "\n",
        "        # Initialize embeddings using Xavier uniform initialization\n",
        "        nn.init.xavier_uniform_(self.embedding.weight)\n",
        "        with torch.no_grad():\n",
        "            self.embedding.weight[pad_idx].fill_(0)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.embedding(x)\n",
        "\n",
        "class TextDataset(torch.utils.data.Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        texts: List[str],\n",
        "        tokenizer: CustomTokenizer,\n",
        "        max_length: int = 1024\n",
        "    ):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "        # Tokenize all texts\n",
        "        self.encoded_texts = [\n",
        "            self.tokenizer.encode(text)[:max_length] for text in texts\n",
        "        ]\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.encoded_texts)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        tokens = self.encoded_texts[idx]\n",
        "\n",
        "        # Create input and target sequences for language modeling\n",
        "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)\n",
        "        target_ids = torch.tensor(tokens[1:], dtype=torch.long)\n",
        "\n",
        "        return input_ids, target_ids\n",
        "\n",
        "def create_dataloader(\n",
        "    texts: List[str],\n",
        "    tokenizer: CustomTokenizer,\n",
        "    batch_size: int = 32,\n",
        "    max_length: int = 1024,\n",
        "    shuffle: bool = True\n",
        ") -> torch.utils.data.DataLoader:\n",
        "    dataset = TextDataset(texts, tokenizer, max_length)\n",
        "\n",
        "    # Custom collate function to handle variable length sequences\n",
        "    def collate_fn(batch):\n",
        "        input_ids, target_ids = zip(*batch)\n",
        "\n",
        "        # Pad sequences\n",
        "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "        target_ids = pad_sequence(target_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n",
        "\n",
        "        # Create attention mask\n",
        "        attention_mask = (input_ids != tokenizer.pad_token_id).float()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'target_ids': target_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "    return torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "# Example usage\n",
        "def create_custom_tokenizer_and_embedding(\n",
        "    texts: List[str],\n",
        "    vocab_size: int = 50000,\n",
        "    embedding_dim: int = 512,\n",
        "    min_freq: int = 2\n",
        "):\n",
        "    # Create and train tokenizer\n",
        "    tokenizer = CustomTokenizer(vocab_size=vocab_size, min_freq=min_freq)\n",
        "    tokenizer.train_from_texts(texts)\n",
        "\n",
        "    # Create embedding layer\n",
        "    embedding = CustomEmbedding(\n",
        "        vocab_size=tokenizer.vocab_size_current,\n",
        "        embedding_dim=embedding_dim,\n",
        "        pad_idx=tokenizer.pad_token_id\n",
        "    )\n",
        "\n",
        "    return tokenizer, embedding"
      ],
      "metadata": {
        "id": "fnVuROJGJK4-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TitanModelWithCustomEmbedding(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_layer: CustomEmbedding,\n",
        "        max_seq_length: int = 1024,\n",
        "        depth: int = 12,\n",
        "        num_heads: int = 8,\n",
        "        head_dim: int = 64,\n",
        "        mlp_ratio: int = 4,\n",
        "        dropout: float = 0.1\n",
        "    ):\n",
        "        super().__init__()\n",
        "        dim = embedding_layer.embedding_dim\n",
        "\n",
        "        self.embedding = embedding_layer\n",
        "        self.pos_embedding = nn.Parameter(torch.zeros(1, max_seq_length, dim))\n",
        "\n",
        "        self.transformer = TitanTransformer(\n",
        "            dim=dim,\n",
        "            depth=depth,\n",
        "            num_heads=num_heads,\n",
        "            head_dim=head_dim,\n",
        "            mlp_ratio=mlp_ratio,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "        # Add final projection for token prediction\n",
        "        self.output_projection = nn.Linear(dim, embedding_layer.embedding.num_embeddings)\n",
        "\n",
        "    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None):\n",
        "        # Get embeddings\n",
        "        x = self.embedding(input_ids)\n",
        "\n",
        "        # Add positional embeddings\n",
        "        x = x + self.pos_embedding[:, :x.size(1), :]\n",
        "\n",
        "        # Apply transformer\n",
        "        x = self.transformer(x, attention_mask)\n",
        "\n",
        "        # Project to vocabulary\n",
        "        logits = self.output_projection(x)\n",
        "\n",
        "        return logits\n",
        "\n",
        "# Example usage\n",
        "def create_complete_model(texts: List[str], vocab_size: int = 50000, embedding_dim: int = 512):\n",
        "    # Create tokenizer and embedding\n",
        "    tokenizer, embedding = create_custom_tokenizer_and_embedding(\n",
        "        texts=texts,\n",
        "        vocab_size=vocab_size,\n",
        "        embedding_dim=embedding_dim\n",
        "    )\n",
        "\n",
        "    # Create model\n",
        "    model = TitanModelWithCustomEmbedding(\n",
        "        embedding_layer=embedding,\n",
        "        max_seq_length=1024,\n",
        "        depth=12\n",
        "    )\n",
        "\n",
        "    return model, tokenizer"
      ],
      "metadata": {
        "id": "HSlZ2uyXRNIS"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example texts for training\n",
        "texts = [\n",
        "    \"Here is an example text.\",\n",
        "    \"Another example for training.\",\n",
        "    # ... add more texts\n",
        "]\n",
        "\n",
        "# Create model and tokenizer\n",
        "model, tokenizer = create_complete_model(texts)\n",
        "\n",
        "# Create dataloader\n",
        "dataloader = create_dataloader(\n",
        "    texts=texts,\n",
        "    tokenizer=tokenizer,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Example of processing a single text\n",
        "text = \"Here is a test sentence.\"\n",
        "input_ids = torch.tensor([tokenizer.encode(text)], dtype=torch.long)\n",
        "attention_mask = (input_ids != tokenizer.pad_token_id).float()\n",
        "\n",
        "# Get model output\n",
        "outputs = model(input_ids, attention_mask)"
      ],
      "metadata": {
        "id": "sHeurGK_ROjd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uQtOxi4lRUjV",
        "outputId": "86c2f318-904c-44a2-bfcd-038687669ffe"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 0.4767, -0.9255,  0.5904,  0.1348,  0.1423, -0.6384],\n",
            "         [ 0.1688, -0.9605,  0.4719,  0.2942,  0.2333, -0.4241],\n",
            "         [ 0.4716, -1.1663,  0.5285,  0.3165,  0.3163, -0.6672],\n",
            "         [ 0.3238, -1.3739,  0.7421,  0.3640,  0.4876, -0.4620],\n",
            "         [ 0.3853, -1.0436,  0.5586,  0.3868,  0.3675, -0.3782],\n",
            "         [ 0.3612, -1.2545,  0.5282,  0.1778,  0.4147, -0.6663],\n",
            "         [ 0.1366, -1.3913,  0.6650,  0.2408,  0.2576, -0.5010],\n",
            "         [ 0.4934, -0.9900,  0.6197,  0.4881,  0.4472, -0.5710]]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Bji4Wts9TTIT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating traning pipeline"
      ],
      "metadata": {
        "id": "KnxJAqoCTUMi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.nn import functional as F\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "import wandb\n",
        "import os\n",
        "from typing import Dict, Any\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "class TitanTrainer:\n",
        "    def __init__(\n",
        "        self,\n",
        "        model: nn.Module,\n",
        "        tokenizer: CustomTokenizer,\n",
        "        train_dataloader: torch.utils.data.DataLoader,\n",
        "        val_dataloader: torch.utils.data.DataLoader = None,\n",
        "        learning_rate: float = 1e-4,\n",
        "        warmup_steps: int = 1000,\n",
        "        max_steps: int = 100000,\n",
        "        gradient_clip: float = 1.0,\n",
        "        device: str = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
        "        checkpoint_dir: str = 'checkpoints',\n",
        "        use_wandb: bool = False\n",
        "    ):\n",
        "        self.model = model.to(device)\n",
        "        self.tokenizer = tokenizer\n",
        "        self.train_dataloader = train_dataloader\n",
        "        self.val_dataloader = val_dataloader\n",
        "        self.device = device\n",
        "        self.checkpoint_dir = checkpoint_dir\n",
        "        self.use_wandb = use_wandb\n",
        "\n",
        "        # Create checkpoint directory\n",
        "        os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "        # Initialize optimizer\n",
        "        self.optimizer = optim.AdamW(\n",
        "            self.model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8,\n",
        "            weight_decay=0.01\n",
        "        )\n",
        "\n",
        "        # Initialize learning rate scheduler\n",
        "        self.scheduler = CosineAnnealingLR(\n",
        "            self.optimizer,\n",
        "            T_max=max_steps,\n",
        "            eta_min=learning_rate/100\n",
        "        )\n",
        "\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.gradient_clip = gradient_clip\n",
        "\n",
        "        # Initialize training state\n",
        "        self.current_step = 0\n",
        "        self.best_val_loss = float('inf')\n",
        "\n",
        "    def _get_warmup_lr(self, step: int) -> float:\n",
        "        return min(1.0, step / self.warmup_steps)\n",
        "\n",
        "    def _compute_loss(self, logits: torch.Tensor, targets: torch.Tensor, pad_idx: int) -> torch.Tensor:\n",
        "        # Reshape logits and targets for loss computation\n",
        "        logits = logits.view(-1, logits.size(-1))\n",
        "        targets = targets.view(-1)\n",
        "\n",
        "        # Create mask for padding\n",
        "        mask = (targets != pad_idx).float()\n",
        "\n",
        "        # Compute cross entropy loss\n",
        "        loss = F.cross_entropy(logits, targets, reduction='none')\n",
        "\n",
        "        # Apply mask and compute mean\n",
        "        loss = (loss * mask).sum() / mask.sum()\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def train_step(self, batch: Dict[str, torch.Tensor]) -> Dict[str, float]:\n",
        "        self.model.train()\n",
        "\n",
        "        # Move batch to device\n",
        "        input_ids = batch['input_ids'].to(self.device)\n",
        "        target_ids = batch['target_ids'].to(self.device)\n",
        "        attention_mask = batch['attention_mask'].to(self.device)\n",
        "\n",
        "        # Forward pass\n",
        "        logits = self.model(input_ids, attention_mask)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = self._compute_loss(logits, target_ids, self.tokenizer.pad_token_id)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.gradient_clip)\n",
        "\n",
        "        # Apply warmup\n",
        "        if self.current_step < self.warmup_steps:\n",
        "            lr_mult = self._get_warmup_lr(self.current_step)\n",
        "            for param_group in self.optimizer.param_groups:\n",
        "                param_group['lr'] *= lr_mult\n",
        "\n",
        "        # Update parameters\n",
        "        self.optimizer.step()\n",
        "        self.scheduler.step()\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        return {'loss': loss.item()}\n",
        "\n",
        "    # @torch.no_grad()\n",
        "    # def evaluate(self) -> Dict[str, float]:\n",
        "    #     if not self.val_dataloader:\n",
        "    #         return {}\n",
        "\n",
        "    #     self.model.eval()\n",
        "    #     total_loss = 0\n",
        "    #     total_tokens = 0\n",
        "\n",
        "    #     for batch in self.val_dataloader:\n",
        "    #         input_ids = batch['input_ids'].to(self.device)\n",
        "    #         target_ids = batch['target_ids'].to(self.device)\n",
        "    #         attention_mask = batch['attention_mask'].to(self.device)\n",
        "\n",
        "    #         logits = self.model(input_ids, attention_mask)\n",
        "    #         loss = self._compute_loss(logits, target_ids, self.tokenizer.pad_token_id)\n",
        "\n",
        "    #         total_loss += loss.item() * input_ids.size(0)\n",
        "    #         total_tokens += input_ids.size(0)\n",
        "\n",
        "    #     return {'val_loss': total_loss / total_tokens}\n",
        "    @torch.no_grad()\n",
        "    def evaluate(self, dataloader: torch.utils.data.DataLoader) -> Dict[str, float]:\n",
        "        \"\"\"\n",
        "        Evaluates the model on the provided dataloader.\n",
        "\n",
        "        Args:\n",
        "            dataloader: The dataloader to use for evaluation.\n",
        "\n",
        "        Returns:\n",
        "            A dictionary containing the evaluation metrics.\n",
        "        \"\"\"\n",
        "\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        total_tokens = 0\n",
        "\n",
        "        for batch in dataloader:  # Use the provided dataloader\n",
        "            input_ids = batch['input_ids'].to(self.device)\n",
        "            target_ids = batch['target_ids'].to(self.device)\n",
        "            attention_mask = batch['attention_mask'].to(self.device)\n",
        "\n",
        "            logits = self.model(input_ids, attention_mask)\n",
        "            loss = self._compute_loss(logits, target_ids, self.tokenizer.pad_token_id)\n",
        "\n",
        "            total_loss += loss.item() * input_ids.size(0)\n",
        "            total_tokens += input_ids.size(0)\n",
        "\n",
        "        return {'val_loss': total_loss / total_tokens}\n",
        "\n",
        "    def save_checkpoint(self, metrics: Dict[str, float], is_best: bool = False):\n",
        "        checkpoint = {\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'scheduler_state_dict': self.scheduler.state_dict(),\n",
        "            'metrics': metrics,\n",
        "            'step': self.current_step\n",
        "        }\n",
        "\n",
        "        # Save latest checkpoint\n",
        "        path = os.path.join(self.checkpoint_dir, 'latest.pt')\n",
        "        torch.save(checkpoint, path)\n",
        "\n",
        "        # Save best checkpoint if needed\n",
        "        if is_best:\n",
        "            best_path = os.path.join(self.checkpoint_dir, 'best.pt')\n",
        "            torch.save(checkpoint, best_path)\n",
        "\n",
        "    def train(self, num_epochs: int):\n",
        "        if self.use_wandb:\n",
        "            wandb.init(project=\"titan-transformer\")\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_metrics = {'epoch': epoch}\n",
        "\n",
        "            # Training loop\n",
        "            pbar = tqdm(self.train_dataloader, desc=f'Epoch {epoch}')\n",
        "            for batch in pbar:\n",
        "                metrics = self.train_step(batch)\n",
        "                epoch_metrics.update(metrics)\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.set_postfix(loss=f\"{metrics['loss']:.4f}\")\n",
        "\n",
        "                self.current_step += 1\n",
        "\n",
        "                # Log metrics\n",
        "                if self.use_wandb:\n",
        "                    wandb.log(metrics, step=self.current_step)\n",
        "\n",
        "            # Validation\n",
        "            val_metrics = self.evaluate()\n",
        "            epoch_metrics.update(val_metrics)\n",
        "\n",
        "            # Check for best model\n",
        "            if val_metrics and val_metrics['val_loss'] < self.best_val_loss:\n",
        "                self.best_val_loss = val_metrics['val_loss']\n",
        "                self.save_checkpoint(epoch_metrics, is_best=True)\n",
        "\n",
        "            # Save regular checkpoint\n",
        "            self.save_checkpoint(epoch_metrics)\n",
        "\n",
        "            # Log epoch metrics\n",
        "            if self.use_wandb:\n",
        "                wandb.log(epoch_metrics, step=self.current_step)\n",
        "\n",
        "            print(f\"Epoch {epoch} metrics:\", epoch_metrics)\n"
      ],
      "metadata": {
        "id": "oPy8IKlfRY9f"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_texts = [\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "    \"Artificial Intelligence is transforming the world.\",\n",
        "    \"Python is a popular programming language.\",\n",
        "    \"Machine learning requires large datasets for training.\",\n",
        "    \"The sky is clear and blue today.\",\n",
        "    \"Natural language processing enables machines to understand text.\",\n",
        "    \"Self-driving cars use advanced sensors and algorithms.\",\n",
        "    \"Data science combines statistics and computer science.\",\n",
        "    \"Reinforcement learning involves agents taking actions in environments.\",\n",
        "    \"Deep learning utilizes neural networks for predictions.\",\n",
        "    \"Quantum computing is a rapidly emerging field.\",\n",
        "    \"Cloud computing offers scalable storage solutions.\",\n",
        "    \"IoT devices are interconnected over networks.\",\n",
        "    \"Cybersecurity is crucial in today's digital era.\",\n",
        "    \"The sun rises in the east and sets in the west.\",\n",
        "    \"Renewable energy sources are vital for sustainability.\",\n",
        "    \"Blockchain technology ensures secure transactions.\",\n",
        "    \"Mathematics is the foundation of many sciences.\",\n",
        "    \"Artificial neural networks mimic the human brain.\",\n",
        "    \"The universe is vast and mysterious.\"\n",
        "]\n",
        "val_texts = [\n",
        "    \"Graphs are used to represent relationships between entities.\",\n",
        "    \"Programming requires logical thinking and problem-solving skills.\",\n",
        "    \"AI ethics is an important area of research.\",\n",
        "    \"The weather today is perfect for a picnic.\",\n",
        "    \"Virtual reality creates immersive digital experiences.\",\n",
        "    \"Big data analytics helps businesses make better decisions.\",\n",
        "    \"Space exploration expands our understanding of the cosmos.\",\n",
        "    \"Autonomous drones are used in agriculture and delivery.\",\n",
        "    \"Data visualization simplifies complex data insights.\",\n",
        "    \"The human genome holds the secrets of biology.\"\n",
        "]\n",
        "test_texts = [\n",
        "    \"Robotics integrates mechanics and AI for automation.\",\n",
        "    \"Digital transformation accelerates technological adoption.\",\n",
        "    \"Text-to-speech models improve accessibility.\",\n",
        "    \"The internet connects people across the globe.\",\n",
        "    \"Energy-efficient algorithms are vital for green computing.\"\n",
        "]\n"
      ],
      "metadata": {
        "id": "BjePUERCWvSa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Datapreprocessing\n"
      ],
      "metadata": {
        "id": "W3zuLjOOwJd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "# Read the CSV files\n",
        "animals_df = pd.read_csv('/content/Animals.csv')\n",
        "climate_df = pd.read_csv('/content/Climate.csv')\n",
        "environment_df = pd.read_csv('/content/Environment.csv')\n",
        "\n",
        "# Combine all datasets\n",
        "combined_df = pd.concat([animals_df, climate_df, environment_df], ignore_index=True)\n",
        "\n",
        "# Split into input and target texts\n",
        "input_texts = combined_df['Input Text'].tolist()\n",
        "target_texts = combined_df['Target Text'].tolist()\n",
        "\n",
        "# Split into train, validation and test sets (70/15/15 split)\n",
        "train_inputs, temp_inputs, train_targets, temp_targets = train_test_split(\n",
        "    input_texts, target_texts, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "val_inputs, test_inputs, val_targets, test_targets = train_test_split(\n",
        "    temp_inputs, temp_targets, test_size=0.5, random_state=42\n",
        ")\n",
        "\n",
        "\n",
        "train_texts = [text for sublist in zip(train_inputs, train_targets) for text in sublist]\n",
        "val_texts = [text for sublist in zip(val_inputs, val_targets) for text in sublist]\n",
        "test_texts = [text for sublist in zip(test_inputs, test_targets) for text in sublist]\n"
      ],
      "metadata": {
        "id": "1Xz_Z1-SwMs0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dataloaders\n",
        "train_dataloader = create_dataloader(train_texts, tokenizer, batch_size=32)\n",
        "val_dataloader = create_dataloader(val_texts, tokenizer, batch_size=32)\n",
        "test_dataloader = create_dataloader(test_texts, tokenizer, batch_size=32)\n",
        "\n",
        "# Initialize trainer\n",
        "trainer = TitanTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    learning_rate=1e-4,\n",
        "    use_wandb=True  # Set to True if using Weights & Biases\n",
        ")\n",
        "\n",
        "# Train model\n",
        "trainer.train(num_epochs=10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 853
        },
        "id": "cTBwQ65vTp3E",
        "outputId": "dc9b2543-3b5a-48d1-e2cc-266901d3c987"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.2"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250119_134939-92q2lxyq</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/rajveer-rathod1301/titan-transformer/runs/92q2lxyq' target=\"_blank\">vague-sun-2</a></strong> to <a href='https://wandb.ai/rajveer-rathod1301/titan-transformer' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/rajveer-rathod1301/titan-transformer' target=\"_blank\">https://wandb.ai/rajveer-rathod1301/titan-transformer</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/rajveer-rathod1301/titan-transformer/runs/92q2lxyq' target=\"_blank\">https://wandb.ai/rajveer-rathod1301/titan-transformer/runs/92q2lxyq</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 7/7 [00:53<00:00,  7.71s/it, loss=2.7150]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 metrics: {'epoch': 0, 'loss': 2.7150418758392334, 'val_loss': 2.7037910764867608}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1: 100%|██████████| 7/7 [00:47<00:00,  6.83s/it, loss=2.7115]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 metrics: {'epoch': 1, 'loss': 2.711519718170166, 'val_loss': 2.7025785012678667}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2: 100%|██████████| 7/7 [00:46<00:00,  6.70s/it, loss=2.7379]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 metrics: {'epoch': 2, 'loss': 2.7379109859466553, 'val_loss': 2.703842661597512}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3: 100%|██████████| 7/7 [00:48<00:00,  6.89s/it, loss=2.7444]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 metrics: {'epoch': 3, 'loss': 2.7443697452545166, 'val_loss': 2.703831109133634}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4: 100%|██████████| 7/7 [00:50<00:00,  7.16s/it, loss=2.7618]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 metrics: {'epoch': 4, 'loss': 2.7617969512939453, 'val_loss': 2.7039150324734775}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5: 100%|██████████| 7/7 [00:48<00:00,  6.88s/it, loss=2.7702]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 metrics: {'epoch': 5, 'loss': 2.770242691040039, 'val_loss': 2.7044940211556177}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6: 100%|██████████| 7/7 [00:50<00:00,  7.26s/it, loss=2.7400]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 metrics: {'epoch': 6, 'loss': 2.740037441253662, 'val_loss': 2.701648408716375}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7: 100%|██████████| 7/7 [00:46<00:00,  6.62s/it, loss=2.7284]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 metrics: {'epoch': 7, 'loss': 2.7283754348754883, 'val_loss': 2.703801241787997}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8: 100%|██████████| 7/7 [00:46<00:00,  6.58s/it, loss=2.7208]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 metrics: {'epoch': 8, 'loss': 2.7208120822906494, 'val_loss': 2.70264855298129}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9: 100%|██████████| 7/7 [00:46<00:00,  6.71s/it, loss=2.7022]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 metrics: {'epoch': 9, 'loss': 2.7022132873535156, 'val_loss': 2.7039245908910576}\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "TitanTrainer.evaluate() takes 1 positional argument but 2 were given",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-420f8204212f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtest_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Test Results: {test_results}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: TitanTrainer.evaluate() takes 1 positional argument but 2 were given"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = TitanTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataloader=train_dataloader,\n",
        "    val_dataloader=val_dataloader,\n",
        "    learning_rate=1e-4,\n",
        "    use_wandb=True  # Set to True if using Weights & Biases\n",
        ")\n",
        "\n",
        "\n",
        "test_results = trainer.evaluate(test_dataloader)\n",
        "print(f\"Test Results: {test_results}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "enzog-yG6scl",
        "outputId": "922f60b2-eaa3-44a9-859f-2349354d910c"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Results: {'val_loss': 2.725238261015519}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## torch server handler class"
      ],
      "metadata": {
        "id": "za5-kcElfKBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TorchServe handler class\n",
        "class TitanHandler:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def initialize(self, context):\n",
        "        \"\"\"Initialize model and tokenizer.\"\"\"\n",
        "        properties = context.system_properties\n",
        "        model_dir = properties.get('model_dir')\n",
        "\n",
        "        # Load tokenizer\n",
        "        with open(os.path.join(model_dir, 'tokenizer_config.json'), 'r') as f:\n",
        "            tokenizer_config = json.load(f)\n",
        "        self.tokenizer = CustomTokenizer(**tokenizer_config)\n",
        "        self.tokenizer.load_vocab(os.path.join(model_dir, 'vocab.txt'))\n",
        "\n",
        "        # Load model\n",
        "        checkpoint = torch.load(\n",
        "            os.path.join(model_dir, 'model.pt'),\n",
        "            map_location=self.device\n",
        "        )\n",
        "\n",
        "        # Initialize model (you'll need to match the configuration used during training)\n",
        "        self.model = TitanModelWithCustomEmbedding(\n",
        "            embedding_layer=CustomEmbedding(\n",
        "                vocab_size=self.tokenizer.vocab_size_current,\n",
        "                embedding_dim=512  # Make sure this matches your training config\n",
        "            )\n",
        "        )\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        \"\"\"Preprocess input text.\"\"\"\n",
        "        text = data[0].get('body').decode('utf-8')\n",
        "\n",
        "        # Tokenize input\n",
        "        input_ids = torch.tensor(\n",
        "            [self.tokenizer.encode(text)],\n",
        "            dtype=torch.long\n",
        "        ).to(self.device)\n",
        "\n",
        "        attention_mask = (input_ids != self.tokenizer.pad_token_id).float()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "    def inference(self, data):\n",
        "        \"\"\"Run inference on processed input.\"\"\"\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                data['input_ids'],\n",
        "                data['attention_mask']\n",
        "            )\n",
        "        return outputs\n",
        "\n",
        "    def postprocess(self, inference_output):\n",
        "        \"\"\"Process model output to response format.\"\"\"\n",
        "        # Get predicted tokens\n",
        "        predictions = inference_output.argmax(dim=-1)\n",
        "\n",
        "        # Decode tokens to text\n",
        "        response = self.tokenizer.decode(predictions[0].tolist())\n",
        "\n",
        "        return [{'generated_text': response}]\n",
        "\n"
      ],
      "metadata": {
        "id": "PSCOwCF0Vf1c"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jf2Ng1N-Tl8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZ6vzrQne1ne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O01rIcO1aRC7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import torch\n",
        "\n",
        "\n",
        "def prepare_model_for_serving(model_path: str, save_dir: str, tokenizer: CustomTokenizer):\n",
        "    \"\"\"\n",
        "    Prepare model artifacts for TorchServe.\n",
        "\n",
        "    Args:\n",
        "        model_path (str): Path to the saved model checkpoint.\n",
        "        save_dir (str): Directory to save the prepared model artifacts.\n",
        "        tokenizer (CustomTokenizer): An instance of the tokenizer to save configuration and vocabulary.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Load the model from the checkpoint\n",
        "    model = torch.load(model_path)\n",
        "\n",
        "    # Save the model in the specified directory\n",
        "    torch.save(model, os.path.join(save_dir, 'model.pt'))\n",
        "\n",
        "    # Prepare tokenizer configuration\n",
        "    tokenizer_config = {\n",
        "        'vocab_size': tokenizer.vocab_size,\n",
        "        'min_freq': tokenizer.min_freq,\n",
        "        'special_tokens': tokenizer.special_tokens,\n",
        "    }\n",
        "\n",
        "    # Save tokenizer configuration as JSON\n",
        "    with open(os.path.join(save_dir, 'tokenizer_config.json'), 'w') as f:\n",
        "        json.dump(tokenizer_config, f)\n",
        "\n",
        "    # Save tokenizer vocabulary\n",
        "    tokenizer.save_vocab(os.path.join(save_dir, 'vocab.txt'))\n",
        "\n",
        "\n",
        "# Example usage\n",
        "# Create an instance of CustomTokenizer\n",
        "tokenizer = CustomTokenizer(\n",
        "    vocab_size=50000,\n",
        "    min_freq=2,\n",
        "    special_tokens=['<pad>', '<eos>', '<bos>', '<unk>']\n",
        ")\n",
        "\n",
        "# Call the function with the correct parameters\n",
        "prepare_model_for_serving('checkpoints/best.pt', 'model_store', tokenizer)\n"
      ],
      "metadata": {
        "id": "s_TysmkMa_kj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oie7oZF5jkqP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchserve torch-model-archiver\n"
      ],
      "metadata": {
        "id": "c2IvOhFuiEK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "handler_code = \"\"\"\n",
        "# TorchServe handler class\n",
        "class TitanHandler:\n",
        "    def __init__(self):\n",
        "        self.model = None\n",
        "        self.tokenizer = None\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def initialize(self, context):\n",
        "        properties = context.system_properties\n",
        "        model_dir = properties.get('model_dir')\n",
        "\n",
        "        # Load tokenizer\n",
        "        with open(os.path.join(model_dir, 'tokenizer_config.json'), 'r') as f:\n",
        "            tokenizer_config = json.load(f)\n",
        "        self.tokenizer = CustomTokenizer(**tokenizer_config)\n",
        "        self.tokenizer.load_vocab(os.path.join(model_dir, 'vocab.txt'))\n",
        "\n",
        "        # Load model\n",
        "        checkpoint = torch.load(\n",
        "            os.path.join(model_dir, 'model.pt'),\n",
        "            map_location=self.device\n",
        "        )\n",
        "\n",
        "        # Initialize model (you'll need to match the configuration used during training)\n",
        "        self.model = TitanModelWithCustomEmbedding(\n",
        "            embedding_layer=CustomEmbedding(\n",
        "                vocab_size=self.tokenizer.vocab_size_current,\n",
        "                embedding_dim=512  # Make sure this matches your training config\n",
        "            )\n",
        "        )\n",
        "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def preprocess(self, data):\n",
        "        text = data[0].get('body').decode('utf-8')\n",
        "\n",
        "        # Tokenize input\n",
        "        input_ids = torch.tensor(\n",
        "            [self.tokenizer.encode(text)],\n",
        "            dtype=torch.long\n",
        "        ).to(self.device)\n",
        "\n",
        "        attention_mask = (input_ids != self.tokenizer.pad_token_id).float()\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask\n",
        "        }\n",
        "\n",
        "    def inference(self, data):\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(\n",
        "                data['input_ids'],\n",
        "                data['attention_mask']\n",
        "            )\n",
        "        return outputs\n",
        "\n",
        "    def postprocess(self, inference_output):\n",
        "        # Get predicted tokens\n",
        "        predictions = inference_output.argmax(dim=-1)\n",
        "\n",
        "        # Decode tokens to text\n",
        "        response = self.tokenizer.decode(predictions[0].tolist())\n",
        "\n",
        "        return [{'generated_text': response}]\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Save it to a file\n",
        "with open(\"titan_handler.py\", \"w\") as f:\n",
        "    f.write(handler_code)"
      ],
      "metadata": {
        "id": "-2yY_ASukBJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torch-model-archiver --model-name titan \\\n",
        "                      --version 1.0 \\\n",
        "                      --model-file model_store/model.pt \\\n",
        "                      --handler titan_handler.py \\\n",
        "                      --extra-files \"model_store/tokenizer_config.json,model_store/vocab.txt\" \\\n",
        "                      --export-path model_store \\\n",
        "                      --force\n"
      ],
      "metadata": {
        "id": "ZJh9VOBXhvzt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!lsof -i :8080\n"
      ],
      "metadata": {
        "id": "4TxdviC1mZpX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!torchserve --start --model-store model_store --models titan=titan.mar --inference-address http://127.0.0.1:8083\n"
      ],
      "metadata": {
        "id": "uRw-_Y_pfO9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.torchserve\n",
        "!nano ~/.torchserve/config.properties\n"
      ],
      "metadata": {
        "id": "FPOsGJa2nQZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## push model to huggingface"
      ],
      "metadata": {
        "id": "JfUeIGZonrEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()\n"
      ],
      "metadata": {
        "id": "Dwz7ywflk6x5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import create_repo\n",
        "\n",
        "create_repo(\"titan-transformer\", private=False)  # Set `private=True` if you want the model to be private\n"
      ],
      "metadata": {
        "id": "B3nVInOdnuZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "# Upload model.pt\n",
        "upload_file(\n",
        "    path_or_fileobj=\"model_store/model.pt\",\n",
        "    path_in_repo=\"pytorch_model.bin\",  # This is the standard filename for the model weights\n",
        "    repo_id=\"rajveer43/titan-transformer\"  # Replace with your Hugging Face username and repo name\n",
        ")\n",
        "\n",
        "# Upload tokenizer_config.json\n",
        "upload_file(\n",
        "    path_or_fileobj=\"model_store/tokenizer_config.json\",\n",
        "    path_in_repo=\"tokenizer_config.json\",\n",
        "    repo_id=\"rajveer43/titan-transformer\"\n",
        ")\n",
        "\n",
        "# Upload vocab.txt\n",
        "upload_file(\n",
        "    path_or_fileobj=\"model_store/vocab.txt\",\n",
        "    path_in_repo=\"vocab.txt\",\n",
        "    repo_id=\"rajveer43/titan-transformer\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "EZO2WpCboY6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import upload_file\n",
        "\n",
        "# Upload model checkpoint\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content/checkpoints/best.pt\",\n",
        "    path_in_repo=\"model/checkpoints/best.pt\",\n",
        "    repo_id=\"rajveer43/titan-transformer\",\n",
        "    repo_type=\"model\"\n",
        ")\n",
        "upload_file(\n",
        "    path_or_fileobj=\"/content/checkpoints/latest.pt\",\n",
        "    path_in_repo=\"model/checkpoints/latest.pt\",\n",
        "    repo_id=\"rajveer43/titan-transformer\",\n",
        "    repo_type=\"model\"\n",
        ")\n"
      ],
      "metadata": {
        "id": "Yu139sCEowj2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import upload_file\n",
        "\n",
        "logs_folder = '/content/logs'  # Path to your logs folder\n",
        "repo_id = \"rajveer43/titan-transformer\"  # Your repo name on Hugging Face\n",
        "\n",
        "# Loop through all files in the logs folder and upload them\n",
        "for root, dirs, files in os.walk(logs_folder):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        # Upload each file to the Hugging Face repo\n",
        "        upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=f\"logs/{file}\",  # Keep the folder structure in the repo\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "sdBNdj06qFXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from huggingface_hub import upload_file\n",
        "\n",
        "logs_folder = '/content/wandb'  # Path to your logs folder\n",
        "repo_id = \"rajveer43/titan-transformer\"  # Your repo name on Hugging Face\n",
        "\n",
        "# Loop through all files in the logs folder and upload them\n",
        "for root, dirs, files in os.walk(logs_folder):\n",
        "    for file in files:\n",
        "        file_path = os.path.join(root, file)\n",
        "        # Upload each file to the Hugging Face repo\n",
        "        upload_file(\n",
        "            path_or_fileobj=file_path,\n",
        "            path_in_repo=f\"wandb/{file}\",  # Keep the folder structure in the repo\n",
        "            repo_id=repo_id,\n",
        "            repo_type=\"model\"\n",
        "        )\n"
      ],
      "metadata": {
        "id": "DzDX_H8tqUv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZuFMXxJpu-ZW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}